{
  "hash": "697d0694f2b99191b3f31ab7f778e30e",
  "result": {
    "markdown": "---\ntitle: Dimensional Modelling with dbt and different flavors of PostgreSQL\nauthor: \"Francisco Mussari\"  \ndate: 2023-05-15  \nimage: \"\"  \ncategories: [dbt, Data-Engineer-Camp, Datawarehousing, Kimball]  \nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    \n---\n\n## Overview\n\nThis exploration of dbt (Data Build Tool) is almost a recreation of the awsome tutorial [Building a Kimball dimensional model with dbt](https://docs.getdbt.com/blog/kimball-dimensional-model) but with some twists.  \n  \nI encourage you to read that document for a discussion of what a dimensional model is, its benefits, and the differences with other modeling techniques such as Third Normal Form (3NF) and One Big Table (OBT).\n  \n\nWhat I did different was the following:\n\n1. Worked with the AdventureWorks data already loaded into PostgreSQL, and not seeded from .csv files. For that I managed to create a backup that you can restore to postgres. You can download it [here](./Adventureworks.backup).\n\n2. Experimented with three flavors of Postgres:\n    - Local PostgreSQL.\n    - Supabase PostgreSQL as-a-service (free).\n    - AWS Aurora Serverless for PostgreSQL.\n\n3. Created custom schema names [Deploy to custom schemas & override dbt defaults](https://youtu.be/CbhobcWZ3Hk).\n\n4. Created two date dimension tables, one with calogical [`dbt-date`](https://github.com/calogica/dbt-date#dbt-date)  extension, which is a wraper around [`dbt_utils.date_spine`](https://github.com/dbt-labs/dbt-utils#date_spine-source), and the other with `date_spine` itself. I managed to create the calendar dynamically ranging from the first day in the fact table to the last one.\n\n## Setting up the project\n\nIs not in the scope of this document to explain how to get started with dbt. I had never used dbt and it was straight forward with these two tutorials:\n\n1. [Building a Kimball dimensional model with dbt](https://docs.getdbt.com/blog/kimball-dimensional-model)\n2. [Intro to Data Build Tool (dbt) // Create your first project!](https://youtu.be/5rNquRnNb4E)\n\nYou can explore the whole experiment I did in this repository: [fmussari/dbt-dimensional-modeling-experiment](https://github.com/fmussari/dbt-dimensional-modeling-experiment/tree/master/dbt_dimensional_demo)\n\n\n## Defining the databases in `profiles.yml`\n\ndbt is a transformation tool, so it only works with one database at a time. That database is the source and the target. In this case we have the normalized AdventureWorks database and we are creating the transformations to create dimensions and fact tables. I found interesting that multiple databases can be defined in a project, so you can test, for example, in your local database, and then create the models into the cloud database.  \n  \nFor storing the credentials I used conda, based on [setting environment variables](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables).  \n   \nThe [`profiles.yml`](https://github.com/fmussari/dbt-dimensional-modeling-experiment/blob/master/dbt_dimensional_demo/profiles.yml) ended looking like this:\n\n\n```{yml}\ndbt_dimensional_demo:\n\n  outputs:\n\n    postgresAdventure:\n      type: postgres\n      threads: 4\n      host: localhost\n      port: 5432\n      user: postgres\n      pass: '10042076'\n      database: Adventureworks\n      schema: target  # this is override in each model's sql\n\n    # https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#setting-environment-variables\n    # C:\\ProgramData\\Miniconda3\\envs\\powerbi\\etc\\conda\\activate.d\n    # C:\\ProgramData\\Miniconda3\\envs\\powerbi\\etc\\conda\\deactivate.d\n\n    supabaseAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('SUPABASE_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('SUPABASE_USER') }}\"\n      pass: \"{{ env_var('SUPABASE_PASS') }}\"\n      database: Adventureworks\n      schema: t\n    \n    auroraAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('AURORA_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('AURORA_USER') }}\"\n      pass: \"{{ env_var('AURORA_PASS') }}\"\n      database: Adventureworks\n      schema: t\n\n  target: supabaseAdventure\n```\n\n\nAs you can see your experiment can run in any database specified by changing the `target`.\n\n",
    "supporting": [
      "dimensional_model_with_dbt_postgres_files"
    ],
    "filters": [],
    "includes": {}
  }
}