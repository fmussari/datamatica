[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Francisco Mussari, blogging from Caracas about data, ai, cloud computing, engineering and social sciences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datamatica",
    "section": "",
    "text": "computer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\ngradio\n\n\nhugging face\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "",
    "text": "Part 1. Create the service and upload the pictures"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\nWe are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 will cover:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "References",
    "text": "References\n\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.1. Create a Custom Vision Service",
    "text": "Part 1.1. Create a Custom Vision Service\nI’m not going to get into the details of creating the service. And the reason is that there is a detailed tutorial covering not just that, but also the code for uploading and training a simple model. I encourage you to try it first:\nDetect Objects in Images with Custom Vision\n\nFor this tutorial I created a Custom Vision with the following settings:\n\nCustom Vision service:\n\nResource: ai102cvision\nResource Kind: Custom Vision Training\n\nProject:\n\nName: Telecom Equipment Detection\nDescription: Detect different types of antennas\nResource: ai102cvision [F0]\nProject Types: Object Detection\nDomains: General"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.2. Upload the images",
    "text": "Part 1.2. Upload the images\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nTrainingEndpoint=YOUR_TRAINING_ENDPOINT\nTrainingKey=YOUR_TRAINING_KEY\nProjectID=YOUR_PROJECT_ID\n\n\n\n\n\n\nNote\n\n\n\nIn order to protect my credentials, I’m going to store .env file in a creds folder that isn’t being pushed to github.\n\n\n\nDOTENV_PATH = './.env'\n\n\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nFunctions\n\n# Borrowed from fastai library\ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\n    #except PIL.UnidentifiedImageError:\n\nThe SDK / API allows to upload images in batches but I didn’t find a way to match the local image name with the id generated by the service. Then I opted to create a function that uploads the pictures one by one.\n\nImageFileCreateEntry\nCustomVisionTrainingClient.create_images_from_files()\n\n\ndef Upload_Images_1by1(pictures: list[Path]) -> list('dict'):\n    \"\"\"Upload the pictures from a list of paths,\n    one by one to keep track of the relation between\n    local image name and Azure image id.\n    And to track the ones that python fails opening\n    \"\"\"\n    print(\"Uploading images...\")\n\n    processed_ids = []\n    processed_status = []\n    picture_names = []\n\n    for pic_path in pictures:\n\n        if verify_image(pic_path):\n            with open(pic_path, mode=\"rb\") as image_data:\n                image_entry = ImageFileCreateEntry(\n                    name=pic_path.name, contents=image_data.read()\n                )\n            \n            # Upload the list of (1) images as a batch\n            upload_result = training_client.create_images_from_files(\n                custom_vision_project.id, \n                # Creates an ImageFileCreateBatch from a list of 1 ImageFileCreateEntry\n                ImageFileCreateBatch(images=[image_entry])\n            )\n            # Check for failure\n            if not upload_result.is_batch_successful:\n                pic_status = upload_result.images[0].status\n                pic_id = None\n            else:\n                pic_status = upload_result.images[0].status\n                pic_id = upload_result.images[0].image.id\n        else:\n            pic_status = \"ReadError\" # Equivalente to SDK `ErrorSource`\n            pic_id = None\n        \n        processed_status.append(pic_status)\n        processed_ids.append(pic_id)\n        picture_names.append(pic_path.name)\n        print(pic_path.name, \"//\", pic_id, \"//\", pic_status)\n    \n    return {\"image_name\": picture_names, \n            \"image_id\": processed_ids, \n            \"image_status\": processed_status}\n\n\n\nExplore pictures\n\npics_folder = Path('./train_images')\n\npictures = sorted(list(pics_folder.iterdir()))\n\nprint(f\"There are {len(pictures)} pictures\")\n\nThere are 203 pictures\n\n\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\nfor i, ax in enumerate(axes.flat):\n    im = Image.open( pictures[i*10] )\n    ax = show_img(im, ax=ax)\n\n\n\n\nAs you can see the pictures are very varied. Different cameras, lighting conditions, focus, resolutions and sizes…\n\n\nUpload the pictures to Custom Vision Service\n\nuploaded_images_df = pd.DataFrame(columns=[\"image_name\", \"image_id\", \"image_status\"])\n\n\nupload_batch = Upload_Images_1by1(pictures)\n\n\nuploaded_images_df = pd.DataFrame(upload_batch)\nuploaded_images_df\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      198\n      torre cerro el pavon 075.jpg\n      b6dd061a-a68d-4d91-a39f-711968445571\n      OK\n    \n    \n      199\n      torre cerro el pavon 080.jpg\n      d12264cf-3d7b-469c-9445-da8dce8dabef\n      OK\n    \n    \n      200\n      torre cerro el pavon 085.jpg\n      c6d587fe-5f3a-46ea-bc04-7ff54f10b4ae\n      OK\n    \n    \n      201\n      torre cerro el pavon 086.jpg\n      ea34cbad-8d50-4b5f-aed0-91d7fe40a754\n      OK\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n203 rows × 3 columns\n\n\n\n\nprint(f\"{sum(uploaded_images_df.image_status != 'OK')} \n      images failed when uploading\")\n\n0 images failed uploading\n\n\nSave a csv:\n\nuploaded_images_df.to_csv('20221012_203_Images_Uploaded.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.3. Explore Data from Custom Vision Service",
    "text": "Part 1.3. Explore Data from Custom Vision Service\n\nGet id’s of uploaded images\n\nCustomVisionTrainingClient.get_images()\n\n\ntrain_images = training_client.get_images(\n    project_id=custom_vision_project.id,\n    take=250,\n    skip=0\n)\n\n\nprint(f\"There are {len(train_images)} training images in the service.\")\nprint(f\"Each image has a type of {type(train_images[0])}.\")\n\nThere are 203 training images in the service.\nEach image has a type of <class 'azure.cognitiveservices.vision.customvision.training.models._models_py3.Image'>.\n\n\nSome properties of the image class:\n\nimage = train_images[0]\nprint(f\"image.id: {image.id}\")\nprint(f\"image.width: {image.width}\")\nprint(f\"image.height: {image.height}\")\n\nimage.id: 6e274dfc-411a-4bf3-9151-51b96f662248\nimage.width: 1188\nimage.height: 900\n\n\n\nimage.original_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/o-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=ru8DNhvBrpA46oZtmzNP7CRHSkwGugumb3R%2F3IzJaUE%3D'\n\n\n\nimage.resized_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/i-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=U5UQ6tjjdLF5gZHFR6wrrWk8B0w9at4cIUeYyxylx2E%3D'\n\n\nOf course there are no tags yet:\n\nprint(f\"image.tags: {image.tags}\")\n\nimage.tags: None\n\n\n\n\nThe images are resized when uploaded\nLet’s see the same image locally:\n\nuploaded_images_df[uploaded_images_df.image_id==image.id]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n\n\n\n\nlocal_image = uploaded_images_df[\n    uploaded_images_df.image_id=='6e274dfc-411a-4bf3-9151-51b96f662248'\n].image_name.item()\nlocal_image\n\n'torre cerro el pavon 087.jpg'\n\n\n\nim = Image.open(pics_folder / local_image)\nim.size\n\n(2576, 1952)\n\n\nThe local image has a size of (2576, 1952) and was resized to (1188, 900) by the service\n\n\nKeep track of original size vs. size in the service\nTo get the real width and height we need to consider EXIF metadata. That’s because local images are sometimes rotated by the viewer with some app viewer.\n\nSize of local images\n\n# The image has some EXIF meta data including information about orientation (rotation)\n# https://stackoverflow.com/a/63950647\n    \nlocal_w = []\nlocal_h = []\n\nfor image in uploaded_images_df.image_name:\n    im = Image.open(pics_folder / image)\n    im = ImageOps.exif_transpose(im)\n\n    local_w.append(im.size[0])\n    local_h.append(im.size[1])\n\n\nlocal_w[:5], local_h[:5]\n\n([640, 1620, 1620, 2160, 2160], [480, 2160, 2160, 1620, 1620])\n\n\n\nuploaded_images_df['ori_w'] = local_w\nuploaded_images_df['ori_h'] = local_h\nuploaded_images_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n    \n  \n\n\n\n\n\n\nSize of images in the service\n\nservice_ids = [im.id for im in train_images]\nservice_w = [im.width for im in train_images]\nservice_h = [im.height for im in train_images]\n\n\nservice_w = {id: w for id, w in zip(service_ids, service_w)}\nservice_h = {id: h for id, h in zip(service_ids, service_h)}\n\nuploaded_images_df['train_w'] = uploaded_images_df['image_id'].map(service_w)\nuploaded_images_df['train_h'] = uploaded_images_df['image_id'].map(service_h)\n\n\n\n\nChecking consistency in the ratios\n\nori_ratio = uploaded_images_df.ori_w / uploaded_images_df.ori_h\ntrain_ratio = uploaded_images_df.train_w / uploaded_images_df.train_h\nall(abs(ori_ratio - i_ratio) > .3)\n\nFalse\n\n\nImages that has an inconsistent ratio:\n\nuploaded_images_df[abs(ori_ratio - train_ratio) > 0.1]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      480\n      640\n      640\n      480\n    \n  \n\n\n\n\n\nim = Image.open( pics_folder / 'TORRE EL TIGRITO 01.jpg' )\nshow_img(im);\n\n\n\n\n\nim.size\n\n(640, 480)\n\n\nImageOps.exif_transpose failed for this image.\nBut if you don’t use it, more images would be inconsistent.\nIf seems that exif_transpose keep track of manually rotated images.\n\nim = ImageOps.exif_transpose(im)\nim.size\n\n(480, 640)\n\n\n\nfilter = uploaded_images_df.image_id == '2563fffe-d621-4799-8e81-6ad57049cdaa'\nuploaded_images_df.loc[filter, ['ori_w', 'ori_h']] = (640, 480)\nuploaded_images_df[filter]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\n\nExporting csv with size data\n\nuploaded_images_df.sample(10)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      155\n      P1100700.JPG\n      b10efb57-70a4-48d6-a846-121ded4546f8\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      7\n      CUMACA 11.jpg\n      2c55467b-5de5-4329-91d2-a2fafdedd080\n      OK\n      2592\n      1944\n      1200\n      900\n    \n    \n      49\n      IMG_1170.JPG\n      ce2177ae-d03e-4a61-9dfb-4229542572fe\n      OK\n      480\n      640\n      480\n      640\n    \n    \n      141\n      MVC-024S.JPG\n      9ba84daa-e00c-4975-a07b-3ae23ef8f884\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      136\n      Imagen008.jpg\n      c861b4de-127a-4dc0-84ea-9cb96fb380f2\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n      2576\n      1952\n      1188\n      900\n    \n    \n      145\n      P1100611.JPG\n      1148d437-fc44-4c51-af4a-4751e242b3b7\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      147\n      P1100613.JPG\n      c9dab11e-0663-42f8-8c93-4e2351b15d4c\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      171\n      PICT0386.JPG\n      0e51a561-b938-48f6-8bc6-3c3bf4c72c44\n      OK\n      2560\n      1920\n      1200\n      900\n    \n    \n      142\n      MVC-025S.JPG\n      a8c2a746-2a65-4872-b7b5-0bd5edf965c9\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\nuploaded_images_df.to_csv('20221015_203_Images_Uploaded_WxH.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Conslusions",
    "text": "Conslusions\n\nIt was straightforward to upload images to the service.\nBig images got resized, but their ratios were kept.\nexif_transpose needs to be used to get the real width and height of the image, which may be different to the original size. For example when the image is rotated manually when looking at it. But somehow it failed with one of the images."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "",
    "text": "Part 2. Label images with Smart Labeler"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\n-> We are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "References",
    "text": "References\n\nCustom Vision Documentation: Label images faster with Smart Labeler\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.1. Labeling the Images",
    "text": "Part 2.1. Labeling the Images\nSmart Labeler is a simple tool for labeling images. It can be used for classification and object detection problems. When working in this problem I missed the ability to zoom-in when labeling some small objects, but as I said, this is a straightforward tool.\nFor speeding up bigger projects it might be usefull that you can first label some pictures, then train and get suggestions for the untagged images, but I didn’t use it. By default the labeler tries to give suggestions even without that first training.\nThe process is simple and you can the use the annotation to train models outside the service (as we are going to try after this series, hopefully, using fastai).\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region, ImageRegionCreateEntry\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nDOTENV_PATH = './.env'\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nCreating Labels\nSince I already did the manual tagging, we can use those tags in this new project.\nFirst we need to create the labels/tags in the service: - CustomVisionTrainingClient.create_tag()\n\ntags = ['Grid', 'Panel', 'Radome', 'RRU', 'Shroud', 'Solid']\ndesc = ['Grid Antenna', 'Panel Cel. Antenna', 'Radome Antenna', \n        'RRU Equipment', 'Shroud Antenna', 'Solid Antenna']\n\n\nservice_tags = []\nfor i, tag in enumerate(tags):\n    service_tags.append(\n        training_client.create_tag(\n            project_id=project_id, name=tag,\n            description=desc[i]\n        )\n    )\nservice_tags\n\n[<azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>]\n\n\nNow we can see this in the service:\n\n\nCustomVisionTrainingClient.get_tags()\n\n\nservice_tags = training_client.get_tags(project_id=project_id)\n\n\nservice_tag_ids = {tag.name: tag.id for tag in service_tags}\nservice_tag_ids\n\n{'RRU': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n 'Shroud': '4e413c15-141a-419b-a958-1485008b2904',\n 'Solid': '3f13d9b0-7b4d-4679-8fb8-7855cea0a118',\n 'Radome': 'a1020654-79c5-4d8a-867c-93dfb2a4a81d',\n 'Grid': 'e016b6a4-49e6-4897-a0c7-d8fc64d032f1',\n 'Panel': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e'}\n\n\n\n\nUpload Regions from json file\nAs I pointed before, you can create all the regions with Smart Labeler. Since I did that already in a previos project, I updated the region’s image ids and tags to the ones in this project and save them as a json.\n\nCustomVisionTrainingClient.create_image_regions()\n\nWe can see from the documentation that “There is a limit of 64 entries in a batch.”\n\nwith open(\"20221016_CreateImageRegions_Body.json\") as json_file:\n    regions_dict = json.load(json_file)\n\nprint(f'We have a total of {len(regions_dict[\"regions\"]):_} regions.')\nprint()\nprint('The first two regions:')\nregions_dict['regions'][:2]\n\nWe have a total of 1_279 regions.\n\nThe first two regions:\n\n\n[{'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n  'left': 0.6395582,\n  'top': 0.0,\n  'width': 0.10740108,\n  'height': 0.14776269},\n {'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e',\n  'left': 0.772766,\n  'top': 0.16059849,\n  'width': 0.22664931,\n  'height': 0.40633526}]\n\n\n\n# Create batches of 60 regions\n\nregions = regions_dict['regions']\n\nfor i in range(int(1_279 / 60)+1):\n    \n    batch_regions = []\n    print(f'Creating Regions {i*60+1:>{5}_} to {min((i+1)*60, 1_279):>{5}_}')\n    \n    for region in regions[i*60: (i+1)*60]:\n        batch_regions.append(\n            ImageRegionCreateEntry(\n                image_id=region['imageId'],\n                tag_id=region['tagId'],\n                left=region['left'], top=region['top'],\n                width=region['width'], height=region['height']\n        ))\n\n    training_client.create_image_regions(\n        project_id=project_id, \n        regions=batch_regions\n    )\n\nCreating Regions     1 to    60\nCreating Regions    61 to   120\nCreating Regions   121 to   180\nCreating Regions   181 to   240\nCreating Regions   241 to   300\nCreating Regions   301 to   360\nCreating Regions   361 to   420\nCreating Regions   421 to   480\nCreating Regions   481 to   540\nCreating Regions   541 to   600\nCreating Regions   601 to   660\nCreating Regions   661 to   720\nCreating Regions   721 to   780\nCreating Regions   781 to   840\nCreating Regions   841 to   900\nCreating Regions   901 to   960\nCreating Regions   961 to 1_020\nCreating Regions 1_021 to 1_080\nCreating Regions 1_081 to 1_140\nCreating Regions 1_141 to 1_200\nCreating Regions 1_201 to 1_260\nCreating Regions 1_261 to 1_279\n\n\nExample image, capture from the service:\n\n\n\nVerifying the number of created Regions\n\nCustomVisionTrainingClient.get_images()\n\n\nall_tagged_images = training_client.get_images(\n    project_id=project_id,\n    tagging_status=\"Tagged\", \n    take=250   # Max 256\n)\ni = 0\nfor im in all_tagged_images: i += len(im.regions)\nprint(f\"Number of created Regions: {i:_}\")\n\nNumber of created Regions: 1_279\n\n\n\n\nDraw some regions\n\nimages_df = pd.read_csv('20221015_203_Images_Uploaded_WxH.csv')\nimages_df.index = images_df.image_id\nimages_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n    \n      image_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n      1200\n      900\n    \n    \n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n      1200\n      900\n    \n  \n\n\n\n\nCreate a dictionary to easily access all regions from an image id:\n\nimg2ann = dict()\n\nfor image in all_tagged_images:\n    img2ann[image.id] = tuple([list(), list()])\n    image_w = image.width; image_h = image.height\n    ori_w = images_df.loc[image.id].ori_w\n    ori_h = images_df.loc[image.id].ori_h\n    for region in image.regions:\n        img2ann[image.id][1].append(region.tag_name)\n        img2ann[image.id][0].append([\n            region.left*ori_w, \n            region.top*ori_h, \n            region.width*ori_w, \n            region.height*ori_h\n        ])\n\n\npics_folder = Path('./train_images')\n\n\n# https://youtu.be/Z0ssNAbe81M?t=4636\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()\n    ])\n\ndef draw_rect(ax, b):\n    patch = ax.add_patch(\n        patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=1)\n    )\n    draw_outline(patch, 4)\n\ndef draw_text(ax, xy, txt, sz=14):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_regions(index=0):\n    im = Image.open( pics_folder / images_df.iloc[index].image_name )\n    ax = show_img(im, figsize=(8,8))\n\n    reg, lab = img2ann[images_df.iloc[index].image_id]\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(index=0)\n\n[[329.19859199999996, 205.3586496, 53.42696959999999, 114.2365248], [249.3986112, 264.75866399999995, 112.4269696, 85.23652799999999]]\n\n\n\n\n\nA dragon-fly was cought in that picture!\n\ndraw_regions(index=100)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.2. Train and Test a Model",
    "text": "Part 2.2. Train and Test a Model\n\nTrain the model\n\nCustomVisionTrainingClient.train_project()\n\n\ntrain_iteration = training_client.train_project(\n    project_id=project_id,\n    training_type='Regular'\n)\n\n\ntrain_iteration.as_dict()\n\n{'id': 'd0006e20-33dd-4806-9fe9-cfc3fca82552',\n 'name': 'Iteration 1',\n 'status': 'Training',\n 'created': '2022-10-12T13:34:38.120Z',\n 'last_modified': '2022-10-22T14:56:30.406Z',\n 'project_id': 'f6cb4ba7-5bbe-46a4-8836-69654dc86f3a',\n 'exportable': False,\n 'training_type': 'Regular',\n 'reserved_budget_in_hours': 0,\n 'training_time_in_minutes': 0}\n\n\n\nCustomVisionTrainingClient.get_iteration_performance()\n\n\nperformance = training_client.get_iteration_performance(\n    project_id=project_id,\n    iteration_id=train_iteration.id\n).as_dict()\n\n   \nfor tag in performance['per_tag_performance']:\n    print('/'*20)\n    print('tag:', tag['name'])\n    print('image count:', training_client.get_tag(\n        project_id=project_id, tag_id=service_tags[tag['name']]\n    ).image_count)\n    print('recall:', tag['recall'])\n    print('average_precision:', tag['average_precision'])\n\n////////////////////\ntag: Shroud\nimage count: 140\nrecall: 0.35789475\naverage_precision: 0.7280897\n////////////////////\ntag: Panel\nimage count: 68\nrecall: 0.11392405\naverage_precision: 0.3710658\n////////////////////\ntag: Solid\nimage count: 88\nrecall: 0.21428572\naverage_precision: 0.4641156\n////////////////////\ntag: Grid\nimage count: 80\nrecall: 0.10526316\naverage_precision: 0.3784035\n////////////////////\ntag: Radome\nimage count: 20\nrecall: 0.0\naverage_precision: 0.051538005\n////////////////////\ntag: RRU\nimage count: 32\nrecall: 0.13043478\naverage_precision: 0.48053658\n\n\nSome things that I would take into account now that negatively impact the model performance: - I choose many images with small boxes. - Some tags are not represented equally, so we ended an unbalanced distribution. - And of course lets remember we only did a quick train.\nThis is a very good thread on some tips and tricks to improve object detection:\n\n\n😨 Training an Object Detection Model is a very challenging task and involves tweaking so many knobsHere is an exhaustive 🎁 tips & tricks list 🎁 that you could use to boost your model performance 🧵 pic.twitter.com/sOvEUhCCwg\n\n— AI Fast Track (@ai_fast_track) October 20, 2022\n\n\n\n\nTest the model (Quick Test)\nQuick test allows to test the model without publishing a prediction API.\n\n# Load image and get height, width and channels\nimage_file = Path(\"./test_images/las-palmas-at-60-(20).jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\nCustomVisionTrainingClient.quick_test_image()\n\n\n# Detect objects in the test image\nprint('Detecting objects in', image_file)\n\nwith open(image_file, mode=\"rb\") as image_data:\n    results = training_client.quick_test_image(\n        project_id=project_id, \n        image_data=image_data,\n        iteration_id=train_iteration.id\n    )\n\nDetecting objects in test_images/las-palmas-at-60-(20).jpg\n\n\n\ndef get_reg_lab(results):\n    reg = []; lab = []\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            left = prediction.bounding_box.left * w\n            top = prediction.bounding_box.top * h\n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n            reg.append([left, top, width, height])\n            lab.append(prediction.tag_name)\n    return reg, lab\n\n\ndef draw_regions(image):\n    \n    ax = show_img(image, figsize=(8,8))\n    reg, lab = get_reg_lab(results)\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(image)\n\n\n\n\nAs you can see, it didn’t detect some antennas. But taking into account we did a regular training and the limitations mentioned in the training data, it is impressive that it got some right in such a complex problem as object detection.\n\nimage_file = Path(\"./test_images/DSC09399.jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\ndraw_regions(image)\n\n\n\n\nNot a good job in this one. But this result is with a “regular” training (quick).\nYou can see in the Gradio demo Telecom Object Detection with Azure Custom Vision that the model trained for 1 hour (free tier limit) does a better job with this picture."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Conslusions",
    "text": "Conslusions\n\nObject detection is a complex problem. The fact that the service does a reasonable good job with unbalanced training photos and with such a limited training time talks about the great margin for improvement."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "",
    "text": "Part 3. Deploy Gradio Web App"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Introduction",
    "text": "Introduction\nAfter we train the model in Azure for 1 hour (free tier) and publishing it, when end up with a Prediction URL.\n\nWe are going to use that Prediction endpoint to do the inference.\nHere is the App already published for you to try:\nTelecom-Object-Detection\nAnd here the repository:\nhttps://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 covered:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "References",
    "text": "References\n\nCreate A 🤗 Space From A Notebook\nBuild & Share Delightful Machine Learning Apps"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Part 3.1. Publishing a Gradio App",
    "text": "Part 3.1. Publishing a Gradio App\nGradio is a great tool to demo machine learning models. The model is already deployed in Azure, so our Gradio App is going to be our front end to connect to that prediction endpoint. What I mean is that the model itself is not going to be deployed in Hugging Face Spaces, which is the normal workflow.\nIf you are new to Gradio, I encourage you to start from the Quickstart.\nThe Gradio demo was created from a Jupyter Notebook with a great tool from fast.ai which is nbdev. You can start here: Create A 🤗 Space From A Notebook\nIn both tutorials you will find the instructions to setup a Gradio enabled space in Hugging Face.\nThis code is based and adapted from: - https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py - https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py\n\nInstall and import libraries\n#|export\n\nimport gradio as gr\nimport numpy as np\nimport os\nimport io\n\nimport requests, validators\n\nfrom pathlib import Path\n\n\nAzure Functions\n#| export\n\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nfrom msrest.authentication import ApiKeyCredentials\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom dotenv import load_dotenv\n\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nPredictionEndpoint=YOUR_PREDICTION_ENDPOINT\nPredictionKey=YOUR_PREDICTION_KEY\nProjectID=YOUR_PROJECT_ID\nModelName=YOUR_PUBLISHED_MODEL\nWe need to create these environment variables in the Hugging Face Spaces repository under Settings -> Repo Secrets:\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n#| export\n\ndef fig2img(fig):\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n    return img\n    \ndef custom_vision_detect_objects(image_file: Path):\n    dpi = 100\n\n    # Get Configuration Settings\n    load_dotenv()\n    prediction_endpoint = os.getenv('PredictionEndpoint')\n    prediction_key = os.getenv('PredictionKey')\n    project_id = os.getenv('ProjectID')\n    model_name = os.getenv('ModelName')\n\n    # Authenticate a client for the training API\n    credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n    prediction_client = CustomVisionPredictionClient(\n        endpoint=prediction_endpoint, credentials=credentials)\n\n    # Load image and get height, width and channels\n    #image_file = 'produce.jpg'\n    print('Detecting objects in', image_file)\n    image = Image.open(image_file)\n    h, w, ch = np.array(image).shape\n\n    # Detect objects in the test image\n    with open(image_file, mode=\"rb\") as image_data:\n        results = prediction_client.detect_image(project_id, model_name, image_data)\n    \n    # Create a figure for the results\n    fig = plt.figure(figsize=(w/dpi, h/dpi))\n    plt.axis('off')\n\n    # Display the image with boxes around each detected object\n    draw = ImageDraw.Draw(image)\n    lineWidth = int(w/800)\n    color = 'cyan'\n\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            # Box coordinates and dimensions are proportional - convert to absolutes\n            left = prediction.bounding_box.left * w \n            top = prediction.bounding_box.top * h \n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n\n            # Draw the box\n            points = ((left,top), (left+width,top), \n                      (left+width,top+height), (left,top+height), \n                      (left,top))\n            draw.line(points, fill=color, width=lineWidth)\n\n            # Add the tag name and probability\n            plt.annotate(\n                prediction.tag_name + \": {0:.0f}%\".format(prediction.probability * 100),\n                (left, top-1.372*h/dpi), \n                backgroundcolor=color,\n                fontsize=max(w/dpi, h/dpi), \n                fontfamily='monospace'\n            )\n\n    plt.imshow(image)\n    plt.tight_layout(pad=0)\n    \n    return fig2img(fig)\n\n    outputfile = 'output.jpg'\n    fig.savefig(outputfile)\n    print('Resulabsts saved in ', outputfile)\n\n\nGradio\n#| export\n\ntitle = \"\"\"<h1 id=\"title\">Telecom Object Detection with Azure Custom Vision</h1>\"\"\"\n\ncss = \"\"\"\nh1#title {\n  text-align: center;\n}\n\"\"\"\nExample images and url to be used in the App\n#| export\n\nurls = [\"https://www.dropbox.com/s/y5bk8om5ucu46d3/747.jpg?dl=1\"]\nimgs = [path.as_posix() for path in sorted(Path('images').rglob('*.jpg'))]\nimg_samples = [[path.as_posix()] for path in sorted(Path('images').rglob('*.jpg'))]\nFunctions for the Gradio App\n#| export\n\ndef set_example_url(example: list) -> dict:\n    print(gr.Textbox.update(value=example[0]))\n    return gr.Textbox.update(value=example[0])\n\ndef set_example_image(example: list) -> dict:\n    return gr.Image.update(value=example[0])\n\ndef detect_objects(url_input:str, image_input:Image):\n    print(f\"{url_input=}\")\n    if validators.url(url_input):\n        image = Image.open(requests.get(url_input, stream=True).raw)\n    elif image_input:\n        image = image_input\n        \n    print(image)\n    print(image.size)\n    w, h = image.size\n    \n    if max(w, h) > 1_200:\n        factor = 1_200 / max(w, h)\n        factor = 1\n        size = (int(w*factor), int(h*factor))\n        image = image.resize(size, resample=Image.Resampling.BILINEAR)\n    \n    resized_image_path = \"input_object_detection.jpg\"\n    image.save(resized_image_path)\n    \n    return custom_vision_detect_objects(resized_image_path)\n#| export\n\nwith gr.Blocks(css=css) as demo:\n    \n    gr.Markdown(title)\n    \n    with gr.Tabs():\n        with gr.TabItem(\"Image Upload\"):\n            with gr.Row():\n                image_input = gr.Image(type='pil')\n                image_output = gr.Image(shape=(650,650))\n                \n            with gr.Row(): \n                example_images = gr.Dataset(components=[image_input], samples=img_samples)\n            \n            image_button = gr.Button(\"Detect\")\n        \n        with gr.TabItem(\"Image URL\"):\n            with gr.Row():\n                url_input = gr.Textbox(lines=2, label='Enter valid image URL here..')\n                img_output_from_url = gr.Image(shape=(650,650))\n                \n            with gr.Row():\n                example_url = gr.Dataset(components=[url_input], samples=[[str(url)] for url in urls])\n            url_button = gr.Button(\"Detect\")\n            \n    url_button.click(detect_objects, inputs=[url_input,image_input], outputs=img_output_from_url)\n    image_button.click(detect_objects, inputs=[url_input,image_input], outputs=image_output)\n    \n    example_url.click(fn=set_example_url, inputs=[example_url], outputs=[url_input])\n    example_images.click(fn=set_example_image, inputs=[example_images], outputs=[image_input])\n\ndemo.launch()\nTo publish the script app.py from the notebook:\n\nfrom nbdev.export import nb_export\nnb_export('ObjectDetectionWithAzureCustomVision_Part_3', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Conslusions",
    "text": "Conslusions\n\nGradio is great for publishing our demo Apps."
  }
]