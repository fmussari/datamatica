[
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "",
    "text": "Part 3. Deploy Gradio Web App"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Introduction",
    "text": "Introduction\nAfter we train the model in Azure for 1 hour (free tier) and publishing it, when end up with a Prediction URL.\n\nWe are going to use that Prediction endpoint to do the inference.\nHere is the App already published for you to try:\nTelecom-Object-Detection\nAnd here the repository:\nhttps://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 covered:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "References",
    "text": "References\n\nCreate A 🤗 Space From A Notebook\nBuild & Share Delightful Machine Learning Apps"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Part 3.1. Publishing a Gradio App",
    "text": "Part 3.1. Publishing a Gradio App\nGradio is a great tool to demo machine learning models. The model is already deployed in Azure, so our Gradio App is going to be our front end to connect to that prediction endpoint. What I mean is that the model itself is not going to be deployed in Hugging Face Spaces, which is the normal workflow.\nIf you are new to Gradio, I encourage you to start from the Quickstart.\nThe Gradio demo was created from a Jupyter Notebook with a great tool from fast.ai which is nbdev. You can start learning the basics here: Create A 🤗 Space From A Notebook\nIn both tutorials you will find the instructions to setup a Gradio enabled space in Hugging Face.\nThis code is based and adapted from:\n- https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py\n- https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py\n\nInstall and import libraries\n#|export\n\nimport gradio as gr\nimport numpy as np\nimport os\nimport io\n\nimport requests, validators\n\nfrom pathlib import Path\n\n\nAzure Functions\n#| export\n\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nfrom msrest.authentication import ApiKeyCredentials\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom dotenv import load_dotenv\n\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nPredictionEndpoint=YOUR_PREDICTION_ENDPOINT\nPredictionKey=YOUR_PREDICTION_KEY\nProjectID=YOUR_PROJECT_ID\nModelName=YOUR_PUBLISHED_MODEL\nWe need to create these environment variables in the Hugging Face Spaces repository under Settings -&gt; Repo Secrets:\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n#| export\n\ndef fig2img(fig):\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n    return img\n    \ndef custom_vision_detect_objects(image_file: Path):\n    dpi = 100\n\n    # Get Configuration Settings\n    load_dotenv()\n    prediction_endpoint = os.getenv('PredictionEndpoint')\n    prediction_key = os.getenv('PredictionKey')\n    project_id = os.getenv('ProjectID')\n    model_name = os.getenv('ModelName')\n\n    # Authenticate a client for the training API\n    credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n    prediction_client = CustomVisionPredictionClient(\n        endpoint=prediction_endpoint, credentials=credentials)\n\n    # Load image and get height, width and channels\n    #image_file = 'produce.jpg'\n    print('Detecting objects in', image_file)\n    image = Image.open(image_file)\n    h, w, ch = np.array(image).shape\n\n    # Detect objects in the test image\n    with open(image_file, mode=\"rb\") as image_data:\n        results = prediction_client.detect_image(project_id, model_name, image_data)\n    \n    # Create a figure for the results\n    fig = plt.figure(figsize=(w/dpi, h/dpi))\n    plt.axis('off')\n\n    # Display the image with boxes around each detected object\n    draw = ImageDraw.Draw(image)\n    lineWidth = int(w/800)\n    color = 'cyan'\n\n    for prediction in results.predictions:\n        # Only show objects with a &gt; 50% probability\n        if (prediction.probability*100) &gt; 50:\n            # Box coordinates and dimensions are proportional - convert to absolutes\n            left = prediction.bounding_box.left * w \n            top = prediction.bounding_box.top * h \n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n\n            # Draw the box\n            points = ((left,top), (left+width,top), \n                      (left+width,top+height), (left,top+height), \n                      (left,top))\n            draw.line(points, fill=color, width=lineWidth)\n\n            # Add the tag name and probability\n            plt.annotate(\n                prediction.tag_name + \": {0:.0f}%\".format(prediction.probability * 100),\n                (left, top-1.372*h/dpi), \n                backgroundcolor=color,\n                fontsize=max(w/dpi, h/dpi), \n                fontfamily='monospace'\n            )\n\n    plt.imshow(image)\n    plt.tight_layout(pad=0)\n    \n    return fig2img(fig)\n\n    outputfile = 'output.jpg'\n    fig.savefig(outputfile)\n    print('Resulabsts saved in ', outputfile)\n\n\nGradio\n#| export\n\ntitle = \"\"\"&lt;h1 id=\"title\"&gt;Telecom Object Detection with Azure Custom Vision&lt;/h1&gt;\"\"\"\n\ncss = \"\"\"\nh1#title {\n  text-align: center;\n}\n\"\"\"\nExample images and url to be used in the App\n#| export\n\nurls = [\"https://www.dropbox.com/s/y5bk8om5ucu46d3/747.jpg?dl=1\"]\nimgs = [path.as_posix() for path in sorted(Path('images').rglob('*.jpg'))]\nimg_samples = [[path.as_posix()] for path in sorted(Path('images').rglob('*.jpg'))]\nFunctions for the Gradio App\n#| export\n\ndef set_example_url(example: list) -&gt; dict:\n    print(gr.Textbox.update(value=example[0]))\n    return gr.Textbox.update(value=example[0])\n\ndef set_example_image(example: list) -&gt; dict:\n    return gr.Image.update(value=example[0])\n\ndef detect_objects(url_input:str, image_input:Image):\n    print(f\"{url_input=}\")\n    if validators.url(url_input):\n        image = Image.open(requests.get(url_input, stream=True).raw)\n    elif image_input:\n        image = image_input\n        \n    print(image)\n    print(image.size)\n    w, h = image.size\n    \n    if max(w, h) &gt; 1_200:\n        factor = 1_200 / max(w, h)\n        factor = 1\n        size = (int(w*factor), int(h*factor))\n        image = image.resize(size, resample=Image.Resampling.BILINEAR)\n    \n    resized_image_path = \"input_object_detection.jpg\"\n    image.save(resized_image_path)\n    \n    return custom_vision_detect_objects(resized_image_path)\n#| export\n\nwith gr.Blocks(css=css) as demo:\n    \n    gr.Markdown(title)\n    \n    with gr.Tabs():\n        with gr.TabItem(\"Image Upload\"):\n            with gr.Row():\n                image_input = gr.Image(type='pil')\n                image_output = gr.Image(shape=(650,650))\n                \n            with gr.Row(): \n                example_images = gr.Dataset(components=[image_input], samples=img_samples)\n            \n            image_button = gr.Button(\"Detect\")\n        \n        with gr.TabItem(\"Image URL\"):\n            with gr.Row():\n                url_input = gr.Textbox(lines=2, label='Enter valid image URL here..')\n                img_output_from_url = gr.Image(shape=(650,650))\n                \n            with gr.Row():\n                example_url = gr.Dataset(components=[url_input], samples=[[str(url)] for url in urls])\n            url_button = gr.Button(\"Detect\")\n            \n    url_button.click(detect_objects, inputs=[url_input,image_input], outputs=img_output_from_url)\n    image_button.click(detect_objects, inputs=[url_input,image_input], outputs=image_output)\n    \n    example_url.click(fn=set_example_url, inputs=[example_url], outputs=[url_input])\n    example_images.click(fn=set_example_image, inputs=[example_images], outputs=[image_input])\n\ndemo.launch()\nTo publish the script app.py from the notebook:\n\nfrom nbdev.export import nb_export\nnb_export('ObjectDetectionWithAzureCustomVision_Part_3', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Conslusions",
    "text": "Conslusions\n\nGradio is great for publishing our demo Apps."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "",
    "text": "Part 1. Create the service and upload the pictures"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\nWe are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 will cover:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "References",
    "text": "References\n\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.1. Create a Custom Vision Service",
    "text": "Part 1.1. Create a Custom Vision Service\nI’m not going to get into the details of creating the service. And the reason is that there is a detailed tutorial covering not just that, but also the code for uploading and training a simple model. I encourage you to try it first:\nDetect Objects in Images with Custom Vision\n\nFor this tutorial I created a Custom Vision with the following settings:\n\nCustom Vision service:\n\nResource: ai102cvision\nResource Kind: Custom Vision Training\n\nProject:\n\nName: Telecom Equipment Detection\nDescription: Detect different types of antennas\nResource: ai102cvision [F0]\nProject Types: Object Detection\nDomains: General"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.2. Upload the images",
    "text": "Part 1.2. Upload the images\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nTrainingEndpoint=YOUR_TRAINING_ENDPOINT\nTrainingKey=YOUR_TRAINING_KEY\nProjectID=YOUR_PROJECT_ID\n\n\n\n\n\n\nNote\n\n\n\nIn order to protect my credentials, I’m going to store .env file in a creds folder that isn’t being pushed to github.\n\n\n\nDOTENV_PATH = './.env'\n\n\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nFunctions\n\n# Borrowed from fastai library\ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\n    #except PIL.UnidentifiedImageError:\n\nThe SDK / API allows to upload images in batches but I didn’t find a way to match the local image name with the id generated by the service. Then I opted to create a function that uploads the pictures one by one.\n\nImageFileCreateEntry\nCustomVisionTrainingClient.create_images_from_files()\n\n\ndef Upload_Images_1by1(pictures: list[Path]) -&gt; list('dict'):\n    \"\"\"Upload the pictures from a list of paths,\n    one by one to keep track of the relation between\n    local image name and Azure image id.\n    And to track the ones that python fails opening\n    \"\"\"\n    print(\"Uploading images...\")\n\n    processed_ids = []\n    processed_status = []\n    picture_names = []\n\n    for pic_path in pictures:\n\n        if verify_image(pic_path):\n            with open(pic_path, mode=\"rb\") as image_data:\n                image_entry = ImageFileCreateEntry(\n                    name=pic_path.name, contents=image_data.read()\n                )\n            \n            # Upload the list of (1) images as a batch\n            upload_result = training_client.create_images_from_files(\n                custom_vision_project.id, \n                # Creates an ImageFileCreateBatch from a list of 1 ImageFileCreateEntry\n                ImageFileCreateBatch(images=[image_entry])\n            )\n            # Check for failure\n            if not upload_result.is_batch_successful:\n                pic_status = upload_result.images[0].status\n                pic_id = None\n            else:\n                pic_status = upload_result.images[0].status\n                pic_id = upload_result.images[0].image.id\n        else:\n            pic_status = \"ReadError\" # Equivalente to SDK `ErrorSource`\n            pic_id = None\n        \n        processed_status.append(pic_status)\n        processed_ids.append(pic_id)\n        picture_names.append(pic_path.name)\n        print(pic_path.name, \"//\", pic_id, \"//\", pic_status)\n    \n    return {\"image_name\": picture_names, \n            \"image_id\": processed_ids, \n            \"image_status\": processed_status}\n\n\n\nExplore pictures\n\npics_folder = Path('./train_images')\n\npictures = sorted(list(pics_folder.iterdir()))\n\nprint(f\"There are {len(pictures)} pictures\")\n\nThere are 203 pictures\n\n\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\nfor i, ax in enumerate(axes.flat):\n    im = Image.open( pictures[i*10] )\n    ax = show_img(im, ax=ax)\n\n\n\n\nAs you can see the pictures are very varied. Different cameras, lighting conditions, focus, resolutions and sizes…\n\n\nUpload the pictures to Custom Vision Service\n\nuploaded_images_df = pd.DataFrame(columns=[\"image_name\", \"image_id\", \"image_status\"])\n\n\nupload_batch = Upload_Images_1by1(pictures)\n\n\nuploaded_images_df = pd.DataFrame(upload_batch)\nuploaded_images_df\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\n\n\n\n\n0\n41.JPG\n452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\nOK\n\n\n1\nCIMG0030.JPG\n96b7774e-f5ad-4591-aa71-99ad5c71135e\nOK\n\n\n2\nCIMG0031.JPG\n3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\nOK\n\n\n3\nCIMG0056.JPG\n1320ab2e-3405-4853-bd7e-b0ef0f915d4b\nOK\n\n\n4\nCIMG0059.JPG\naa67eceb-3db0-4026-bf16-0842c006e6ac\nOK\n\n\n...\n...\n...\n...\n\n\n198\ntorre cerro el pavon 075.jpg\nb6dd061a-a68d-4d91-a39f-711968445571\nOK\n\n\n199\ntorre cerro el pavon 080.jpg\nd12264cf-3d7b-469c-9445-da8dce8dabef\nOK\n\n\n200\ntorre cerro el pavon 085.jpg\nc6d587fe-5f3a-46ea-bc04-7ff54f10b4ae\nOK\n\n\n201\ntorre cerro el pavon 086.jpg\nea34cbad-8d50-4b5f-aed0-91d7fe40a754\nOK\n\n\n202\ntorre cerro el pavon 087.jpg\n6e274dfc-411a-4bf3-9151-51b96f662248\nOK\n\n\n\n\n203 rows × 3 columns\n\n\n\n\nprint(f\"{sum(uploaded_images_df.image_status != 'OK')} \n      images failed when uploading\")\n\n0 images failed uploading\n\n\nSave a csv:\n\nuploaded_images_df.to_csv('20221012_203_Images_Uploaded.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.3. Explore Data from Custom Vision Service",
    "text": "Part 1.3. Explore Data from Custom Vision Service\n\nGet id’s of uploaded images\n\nCustomVisionTrainingClient.get_images()\n\n\ntrain_images = training_client.get_images(\n    project_id=custom_vision_project.id,\n    take=250,\n    skip=0\n)\n\n\nprint(f\"There are {len(train_images)} training images in the service.\")\nprint(f\"Each image has a type of {type(train_images[0])}.\")\n\nThere are 203 training images in the service.\nEach image has a type of &lt;class 'azure.cognitiveservices.vision.customvision.training.models._models_py3.Image'&gt;.\n\n\nSome properties of the image class:\n\nimage = train_images[0]\nprint(f\"image.id: {image.id}\")\nprint(f\"image.width: {image.width}\")\nprint(f\"image.height: {image.height}\")\n\nimage.id: 6e274dfc-411a-4bf3-9151-51b96f662248\nimage.width: 1188\nimage.height: 900\n\n\n\nimage.original_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/o-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=ru8DNhvBrpA46oZtmzNP7CRHSkwGugumb3R%2F3IzJaUE%3D'\n\n\n\nimage.resized_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/i-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=U5UQ6tjjdLF5gZHFR6wrrWk8B0w9at4cIUeYyxylx2E%3D'\n\n\nOf course there are no tags yet:\n\nprint(f\"image.tags: {image.tags}\")\n\nimage.tags: None\n\n\n\n\nThe images are resized when uploaded\nLet’s see the same image locally:\n\nuploaded_images_df[uploaded_images_df.image_id==image.id]\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\n\n\n\n\n202\ntorre cerro el pavon 087.jpg\n6e274dfc-411a-4bf3-9151-51b96f662248\nOK\n\n\n\n\n\n\n\n\nlocal_image = uploaded_images_df[\n    uploaded_images_df.image_id=='6e274dfc-411a-4bf3-9151-51b96f662248'\n].image_name.item()\nlocal_image\n\n'torre cerro el pavon 087.jpg'\n\n\n\nim = Image.open(pics_folder / local_image)\nim.size\n\n(2576, 1952)\n\n\nThe local image has a size of (2576, 1952) and was resized to (1188, 900) by the service\n\n\nKeep track of original size vs. size in the service\nTo get the real width and height we need to consider EXIF metadata. That’s because local images are sometimes rotated by the viewer with some app viewer.\n\nSize of local images\n\n# The image has some EXIF meta data including information about orientation (rotation)\n# https://stackoverflow.com/a/63950647\n    \nlocal_w = []\nlocal_h = []\n\nfor image in uploaded_images_df.image_name:\n    im = Image.open(pics_folder / image)\n    im = ImageOps.exif_transpose(im)\n\n    local_w.append(im.size[0])\n    local_h.append(im.size[1])\n\n\nlocal_w[:5], local_h[:5]\n\n([640, 1620, 1620, 2160, 2160], [480, 2160, 2160, 1620, 1620])\n\n\n\nuploaded_images_df['ori_w'] = local_w\nuploaded_images_df['ori_h'] = local_h\nuploaded_images_df.head(5)\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\nori_w\nori_h\n\n\n\n\n0\n41.JPG\n452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\nOK\n640\n480\n\n\n1\nCIMG0030.JPG\n96b7774e-f5ad-4591-aa71-99ad5c71135e\nOK\n1620\n2160\n\n\n2\nCIMG0031.JPG\n3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\nOK\n1620\n2160\n\n\n3\nCIMG0056.JPG\n1320ab2e-3405-4853-bd7e-b0ef0f915d4b\nOK\n2160\n1620\n\n\n4\nCIMG0059.JPG\naa67eceb-3db0-4026-bf16-0842c006e6ac\nOK\n2160\n1620\n\n\n\n\n\n\n\n\n\nSize of images in the service\n\nservice_ids = [im.id for im in train_images]\nservice_w = [im.width for im in train_images]\nservice_h = [im.height for im in train_images]\n\n\nservice_w = {id: w for id, w in zip(service_ids, service_w)}\nservice_h = {id: h for id, h in zip(service_ids, service_h)}\n\nuploaded_images_df['train_w'] = uploaded_images_df['image_id'].map(service_w)\nuploaded_images_df['train_h'] = uploaded_images_df['image_id'].map(service_h)\n\n\n\n\nChecking consistency in the ratios\n\nori_ratio = uploaded_images_df.ori_w / uploaded_images_df.ori_h\ntrain_ratio = uploaded_images_df.train_w / uploaded_images_df.train_h\nall(abs(ori_ratio - i_ratio) &gt; .3)\n\nFalse\n\n\nImages that has an inconsistent ratio:\n\nuploaded_images_df[abs(ori_ratio - train_ratio) &gt; 0.1]\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\nori_w\nori_h\ntrain_w\ntrain_h\n\n\n\n\n179\nTORRE EL TIGRITO 01.jpg\n2563fffe-d621-4799-8e81-6ad57049cdaa\nOK\n480\n640\n640\n480\n\n\n\n\n\n\n\n\nim = Image.open( pics_folder / 'TORRE EL TIGRITO 01.jpg' )\nshow_img(im);\n\n\n\n\n\nim.size\n\n(640, 480)\n\n\nImageOps.exif_transpose failed for this image.\nBut if you don’t use it, more images would be inconsistent.\nIf seems that exif_transpose keep track of manually rotated images.\n\nim = ImageOps.exif_transpose(im)\nim.size\n\n(480, 640)\n\n\n\nfilter = uploaded_images_df.image_id == '2563fffe-d621-4799-8e81-6ad57049cdaa'\nuploaded_images_df.loc[filter, ['ori_w', 'ori_h']] = (640, 480)\nuploaded_images_df[filter]\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\nori_w\nori_h\ntrain_w\ntrain_h\n\n\n\n\n179\nTORRE EL TIGRITO 01.jpg\n2563fffe-d621-4799-8e81-6ad57049cdaa\nOK\n640\n480\n640\n480\n\n\n\n\n\n\n\n\n\nExporting csv with size data\n\nuploaded_images_df.sample(10)\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\nori_w\nori_h\ntrain_w\ntrain_h\n\n\n\n\n155\nP1100700.JPG\nb10efb57-70a4-48d6-a846-121ded4546f8\nOK\n2048\n1360\n1355\n900\n\n\n7\nCUMACA 11.jpg\n2c55467b-5de5-4329-91d2-a2fafdedd080\nOK\n2592\n1944\n1200\n900\n\n\n49\nIMG_1170.JPG\nce2177ae-d03e-4a61-9dfb-4229542572fe\nOK\n480\n640\n480\n640\n\n\n141\nMVC-024S.JPG\n9ba84daa-e00c-4975-a07b-3ae23ef8f884\nOK\n640\n480\n640\n480\n\n\n136\nImagen008.jpg\nc861b4de-127a-4dc0-84ea-9cb96fb380f2\nOK\n640\n480\n640\n480\n\n\n202\ntorre cerro el pavon 087.jpg\n6e274dfc-411a-4bf3-9151-51b96f662248\nOK\n2576\n1952\n1188\n900\n\n\n145\nP1100611.JPG\n1148d437-fc44-4c51-af4a-4751e242b3b7\nOK\n2048\n1360\n1355\n900\n\n\n147\nP1100613.JPG\nc9dab11e-0663-42f8-8c93-4e2351b15d4c\nOK\n2048\n1360\n1355\n900\n\n\n171\nPICT0386.JPG\n0e51a561-b938-48f6-8bc6-3c3bf4c72c44\nOK\n2560\n1920\n1200\n900\n\n\n142\nMVC-025S.JPG\na8c2a746-2a65-4872-b7b5-0bd5edf965c9\nOK\n640\n480\n640\n480\n\n\n\n\n\n\n\n\nuploaded_images_df.to_csv('20221015_203_Images_Uploaded_WxH.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Conslusions",
    "text": "Conslusions\n\nIt was straightforward to upload images to the service.\nBig images got resized, but their ratios were kept.\nexif_transpose needs to be used to get the real width and height of the image, which may be different to the original size. For example when the image is rotated manually when looking at it. But somehow it failed with one of the images."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 2. Deploying the web app to Streamlit"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to take the SQLite database with the searchable index we created in Part 1 and use it as the search engine for a web app we are going to deploy to Streamlit.\nSo we are going to be able to search over the entire fastai channel.\nThe Python file was created with a Jupyter Notebook using nbdev.\nThe web app is just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)\nCreate A 🤗 Space From A Notebook"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Creating app.py",
    "text": "Creating app.py\n#|export\nimport streamlit as st\nimport sqlite3\n#| export\ndb_path = '/mnt/m/datamatica/posts/full-text-search-fastai-youtube-channel/'\n\ntry:\n    db = sqlite3.connect(db_path + 'fastai_yt.db')\nexcept:\n    db = sqlite3.connect('fastai_yt.db')\n\ncur = db.cursor()\n#| export\nplaylist = cur.execute('SELECT playlist_id, playlist_name FROM playlist').fetchall()\nvideo = cur.execute('SELECT video_id, video_name FROM video').fetchall()\nplaylist = {p: n for p, n in playlist}\nvideo = {p: n for p, n in video}\npl_sel = list(playlist.values())\npl_to_id = {v:k for k,v in playlist.items()}\n#| export\nst.title('Full-Text Search fastai Youtube Playlists')\n\n# https://discuss.streamlit.io/t/select-all-on-a-streamlit-multiselect/9799/2\n\nall_options = st.checkbox(\"Select all Playlists\",\n    key='sel_all', value=True)\n\ncontainer = st.container()\nif all_options:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, disabled=True)\nelse:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, pl_sel)\n\nif all_options: options = list(playlist.values())\nelse: options = sel_options\n\nst.write('Selected playlist(s):', options)\n#| export\ndef get_query(q, limit):\n    \n    search_in = 'text'\n    \n    if not( len(options)==len(pl_sel) or len(options)==0 ):\n        search_in = 'transcriptions_fts'\n        q_pl = '(playlist_id: '\n        for pl in options:\n            end = ' OR ' if pl != options[-1] else ')'\n            q_pl = q_pl + f'\"{pl_to_id[pl]}\"' + end\n        \n        q = f\"(text: {q}) AND \" + q_pl\n\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    return query\n\nwith st.form(\"Input\"):\n    queryText = st.text_area(\"Search query: \\ne.g. «fastc*», «fastcore OR paral*»\", height=1, max_chars=None)\n    limit_val = st.slider(\"Number of results:\", min_value=5, max_value=20)\n    btnResult = st.form_submit_button('Search')\n    \nif btnResult:\n    if not queryText:\n        st.text('Please enter a search query.')\n    else:\n        try:\n            st.text('Search query generated:')\n            # run query\n            st.write(get_query(queryText, limit_val).replace('*', '\\*'))\n            res = cur.execute(get_query(q=queryText, limit=limit_val)).fetchall()\n            st.text('Search results (click to go to YouTube):')\n\n            res_md = ('  \\n  '.join(['  \\n  '.join([\n                f\"{i}.- Playlist: `{playlist[each[0]]}`, Video: `{video[each[1]]}`\", \n                f\"Caption: '...[{each[4].replace('[','**'\n                ).replace(']','**')}](https://youtu.be/{each[1]}?t={str(int(each[2]))})...'\", \n                '\\n'])\n                for i, each in enumerate(res)\n            ]))\n\n            st.markdown(res_md)\n        except:\n            st.text('Invalid search query.')\n#| hide\nfrom nbdev.export import nb_export\nnb_export('_Deploy_Search_Engine_Streamlit.ipynb', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "",
    "text": "In this post we are going to train a Matrix Factorization model on MovieLens 20M Dataset.\nFor the training we are going to use raw PyTorch and miniai.\nAfter the model is trained, we will interpret the results by recommending movies to a given user, analyzing movies’ bias, and visualizing the embeddings by reducing dimensionality using PCA and t-SNE. We will also find similar movies to a given one using cosine similarity."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#overview",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#overview",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "",
    "text": "In this post we are going to train a Matrix Factorization model on MovieLens 20M Dataset.\nFor the training we are going to use raw PyTorch and miniai.\nAfter the model is trained, we will interpret the results by recommending movies to a given user, analyzing movies’ bias, and visualizing the embeddings by reducing dimensionality using PCA and t-SNE. We will also find similar movies to a given one using cosine similarity."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#hardware",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#hardware",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Hardware",
    "text": "Hardware\nI ran the training on an old GTX 1070 GPU. Maybe a modern CPU is enough to do the job. Or you can run this notebook in Colab. To install miniai in Colab run the next two cells.\n\n# Clone miniai repository\n! git clone https://github.com/fastai/course22p2.git\n# Install miniai\n! pip install -e course22p2\n\n\n# Automatically restart kernel after installs so that your environment can access the new packages\nimport IPython\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#references",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#references",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "References",
    "text": "References\n\nFastbook: Collaborative Filtering Deep Dive\nKaggle: Collaborative Filtering Deep Dive\nWikipedia: Matrix factorization (recommender systems)\nGoogle Dev: Collaborative Filtering\nGithub: Recommenders (Microsoft)\nKaggle: Visualizing Embeddings With t-SNE\nBuilding a cost-effective image vector search engine with CLIP"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#import-libraries",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#import-libraries",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Import libraries",
    "text": "Import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport urllib.request\nimport zipfile\nimport pickle\n\nfrom functools import partial\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW, lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, RandomSampler, BatchSampler\n)\n\nLets download a script to split the data from gist, based on code from Recommenders (Microsoft) Github repository.\n\ngist_id = 'f6fdb783ab99a37259bec182f864a988'\nurl = f\"https://gist.github.com/{gist_id}.git\"\nfolder = 'recommender_split'\n\n! git clone {url} {folder}\nfrom recommender_split.recommender_split import split_dataframe_by_group, report_splits\n\nCloning into 'recommender_split'...\nremote: Enumerating objects: 3, done.\nremote: Counting objects: 100% (3/3), done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nUnpacking objects: 100% (3/3), 1.30 KiB | 14.00 KiB/s, done.\n\n\n\nminiai\nWe are using miniai framework.\n\nfrom miniai.datasets import DataLoaders\nfrom miniai.learner import MetricsCB, DeviceCB, ProgressCB, TrainLearner\nfrom miniai.activations import set_seed\nfrom miniai.sgd import BatchSchedCB, RecorderCB"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#load-movielens-20m",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#load-movielens-20m",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Load Movielens 20M",
    "text": "Load Movielens 20M\n\nUSER = 'user_id'\nITEM = 'item_id'\nTARGET = 'rating'\n\n\nmovielens_20M_url = 'https://files.grouplens.org/datasets/movielens/ml-20m.zip'\n\nfilename = Path(movielens_20M_url).name\nzip_path = Path('.')\nextract_path = zip_path / Path(filename).stem\n\n\nif not Path(filename).exists():\n    with urllib.request.urlopen(movielens_20M_url) as response:\n        with open(filename, \"wb\") as f:\n          f.write(response.read())\n\nif not extract_path.exists():\n    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n        zip_ref.extractall(zip_path)\n\n\nratings_20m = pd.read_csv(\n    extract_path/'ratings.csv', header=0,\n    names=[USER, ITEM, TARGET, 'timestamp'])\n\nratings_20m = ratings_20m[[USER, ITEM, TARGET]].copy()\n\nmovies_20m = pd.read_csv(\n    extract_path/'movies.csv', encoding='utf-8',\n    usecols=(0, 1, 2), header=0, names=[ITEM, 'title', 'genres']\n)\n\nratings_20m\n\n\n\n\n\n\n\n\nuser_id\nitem_id\nrating\n\n\n\n\n0\n1\n2\n3.5\n\n\n1\n1\n29\n3.5\n\n\n2\n1\n32\n3.5\n\n\n3\n1\n47\n3.5\n\n\n4\n1\n50\n3.5\n\n\n...\n...\n...\n...\n\n\n20000258\n138493\n68954\n4.5\n\n\n20000259\n138493\n69526\n4.5\n\n\n20000260\n138493\n69644\n3.0\n\n\n20000261\n138493\n70286\n5.0\n\n\n20000262\n138493\n71619\n2.5\n\n\n\n\n20000263 rows × 3 columns"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#eda",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#eda",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "EDA",
    "text": "EDA\n\nRecords with more than one item_id per title\n\ng = movies_20m.groupby('title')['item_id'].count().reset_index()\ntitles = g[g.item_id&gt;1]\n\n\nmovies_20m[movies_20m.title.isin(titles.title)].sort_values('title').head(6)\n\n\n\n\n\n\n\n\nitem_id\ntitle\ngenres\n\n\n\n\n24064\n114130\n20,000 Leagues Under the Sea (1997)\nRomance|Sci-Fi\n\n\n20923\n102190\n20,000 Leagues Under the Sea (1997)\nAdventure|Romance|Sci-Fi\n\n\n582\n588\nAladdin (1992)\nAdventure|Animation|Children|Comedy|Musical\n\n\n24092\n114240\nAladdin (1992)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n24437\n115777\nBeneath (2013)\nHorror\n\n\n21429\n104035\nBeneath (2013)\nHorror\n\n\n\n\n\n\n\n\nBy inspecting movies with the link https://movielens.org/movies/&lt;item_id&gt;, we can conclude that each title corresponds to an unique movie, even though they share the same title and release year.\nSome titles in the dataset refer to the original title, for example item_id=67459 is “Chaos (2005)” and not “The Deadly Hostage (2005)”."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#merge-dataframes",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#merge-dataframes",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Merge dataframes",
    "text": "Merge dataframes\n\nratings_20m = ratings_20m.merge(movies_20m, how='left')\nratings_20m\n\n\n\n\n\n\n\n\nuser_id\nitem_id\nrating\ntitle\ngenres\n\n\n\n\n0\n1\n2\n3.5\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n1\n1\n29\n3.5\nCity of Lost Children, The (Cité des enfants p...\nAdventure|Drama|Fantasy|Mystery|Sci-Fi\n\n\n2\n1\n32\n3.5\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nMystery|Sci-Fi|Thriller\n\n\n3\n1\n47\n3.5\nSeven (a.k.a. Se7en) (1995)\nMystery|Thriller\n\n\n4\n1\n50\n3.5\nUsual Suspects, The (1995)\nCrime|Mystery|Thriller\n\n\n...\n...\n...\n...\n...\n...\n\n\n20000258\n138493\n68954\n4.5\nUp (2009)\nAdventure|Animation|Children|Drama\n\n\n20000259\n138493\n69526\n4.5\nTransformers: Revenge of the Fallen (2009)\nAction|Adventure|Sci-Fi|IMAX\n\n\n20000260\n138493\n69644\n3.0\nIce Age: Dawn of the Dinosaurs (2009)\nAction|Adventure|Animation|Children|Comedy|Rom...\n\n\n20000261\n138493\n70286\n5.0\nDistrict 9 (2009)\nMystery|Sci-Fi|Thriller\n\n\n20000262\n138493\n71619\n2.5\nCoco Before Chanel (Coco avant Chanel) (2009)\nDrama\n\n\n\n\n20000263 rows × 5 columns\n\n\n\n\nassert not any(ratings_20m.title.isna())\n\n\ndata = ratings_20m"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#train-and-valid-splits",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#train-and-valid-splits",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Train and Valid Splits",
    "text": "Train and Valid Splits\nThis function splits the dataframe. If filter_by is item it assures that all items specified in the columns col_item are going to be present in both train and valid, except for those with only one row (review).\n\nsplits = split_dataframe_by_group(\n    data, ratio=0.8, filter_by=\"item\",\n    seed=42, col_user=USER, col_item=ITEM\n)\n\n\ntrain_df = data.iloc[splits[0]]\nvalid_df = data.iloc[splits[1]]\n\n\nreport_splits(data, splits, USER, ITEM, ITEM)\n\nNumber of item_id with one review: 3972\nNumber of users in train: 138493\nNumber of items in train: 26744\nNumber of users in valid: 138333\nNumber of items in valid: 22772\n\n\nAll items with more than one review are present in valid_df: 26,744 = 22,772 + 3,972.\nBut there are users that are not present:\n\nusers_not_in_valid = set(train_df[USER]) - set(valid_df[USER])\nprint(f\"There are {len(users_not_in_valid)} users not present in valid split\")\n\nThere are 160 users not present in valid split\n\n\n\nModify splits to have all users in valid\nWe can now take the ratings dataframe with only the users not present in valid, and split it using user in the filter_by parameter. That way we assure that that set of users are going to be present both in train and valid.\nUsing the sort_column parameter means that the item with more reviews are the ones that goes to valid, so it is less likely to remove movies from training.\n\nsort_column = 'numrev_by_item'\nnumber_of_reviews_by_item = data.groupby(ITEM)[ITEM].transform('count')\n\ntmp_filter = data[USER].isin(users_not_in_valid)\n\ndf_new_splits = data[tmp_filter].copy()\ndf_new_splits[sort_column] = number_of_reviews_by_item[tmp_filter]\n\ndf_new_splits.head()\n\n\n\n\n\n\n\n\nuser_id\nitem_id\nrating\ntitle\ngenres\nnumrev_by_item\n\n\n\n\n92313\n641\n107\n2.5\nMuppet Treasure Island (1996)\nAdventure|Children|Comedy|Musical\n6376\n\n\n92314\n641\n135\n4.0\nDown Periscope (1996)\nComedy\n6305\n\n\n92315\n641\n405\n3.5\nHighlander III: The Sorcerer (a.k.a. Highlande...\nAction|Fantasy\n3554\n\n\n92316\n641\n542\n5.0\nSon in Law (1993)\nComedy|Drama|Romance\n3491\n\n\n92317\n641\n765\n4.0\nJack (1996)\nComedy|Drama\n4812\n\n\n\n\n\n\n\n\nnew_splits = split_dataframe_by_group(\n    df_new_splits, ratio=0.8, filter_by=\"user\",\n    seed=104577657, col_user=USER, col_item=ITEM, sort_column=sort_column\n)\n\nAdd the indices from both splits:\n\nfinal_splits = [splits[0] + new_splits[0], splits[1] + new_splits[1]]\n\n\ntrain_df = data.iloc[final_splits[0]]\nvalid_df = data.iloc[final_splits[1]]\n\n\nlen(train_df), len(valid_df)\n\n(15994636, 4009300)\n\n\n\nf\"{len(train_df)/len(data):.2f}\", f\"{len(valid_df)/len(data):.2f}\"\n\n('0.80', '0.20')\n\n\n\nreport_splits(data, final_splits, USER, ITEM, ITEM)\n\nNumber of item_id with one review: 3972\nNumber of users in train: 138493\nNumber of items in train: 26744\nNumber of users in valid: 138493\nNumber of items in valid: 22772\n\n\nNow we have all items in train and valid except those with only one review, and all users in train and valid."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#pytorch-dataset",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#pytorch-dataset",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "PyTorch Dataset",
    "text": "PyTorch Dataset\nFirst lets create the dictionaries to do the mapping between users, items and indices:\n\nusers = sorted(list(data[USER].unique()))\nitems = sorted(list(data[ITEM].unique()))\n\n# user index to user_id\nuidx2u = {k:v for k,v in enumerate(users)}\n# user_id to user index\nu2uidx = {k:v for v,k in uidx2u.items()}\n# item index to item_id\niidx2i = {k:v for k,v in enumerate(items)}\n# item_id to item index\ni2iidx = {k:v for v,k in iidx2i.items()}\n\n# item index to title\ndef iidx2title(iidx):\n    return movies_20m[movies_20m.item_id==iidx2i[iidx]].title.item()\n\n\nlen(uidx2u), len(iidx2i)\n\n(138493, 26744)\n\n\n\n# item_id==3351\niidx2title(i2iidx[3351]), i2iidx[3351]\n\n('Two Thousand Maniacs! (1964)', 3264)\n\n\n\n# item index 31 and 150\niidx2title(31), iidx2title(150)\n\n('Twelve Monkeys (a.k.a. 12 Monkeys) (1995)', 'Addiction, The (1995)')\n\n\n\nOptimized Dataset\nThis Dataset has some peculiarities:\n\nThe mapping of users and titles and the conversion to numpy is done for the full dataframe when the object is created. Another way would be to map and convert partially in the get_batch function. This last approach would benefit with the number of worker the dataloaders use. The one we are using here doesn’t, as we will see. but anyway it is faster.\nThe Dataset receives a list of indices instead of an individual index, and therefore directly returns a batch.\n\n\nclass CollabDatasetOpt(Dataset):\n    def __init__(self, df:pd.DataFrame, user_col='user', item_col='item', target='rating'):\n        self.users = df[user_col].map(u2uidx).to_numpy()\n        self.items = df[item_col].map(i2iidx).to_numpy()\n        self.target = df[target].to_numpy()\n    \n    def __len__(self):\n        return len(self.users)\n    \n    def get_batch(self, batch_idxs):\n        users_b = self.users[batch_idxs]\n        items_b = self.items[batch_idxs]\n        x_b = np.column_stack((users_b, items_b))\n        target_b = self.target[batch_idxs]\n        y_b = torch.tensor(target_b, dtype=torch.float)\n        return torch.tensor(x_b), y_b\n    \n    def __getitem__(self, batch_idxs):\n        return self.get_batch(batch_idxs)\n\n\nt_dataset = CollabDatasetOpt(train_df, user_col=USER, item_col=ITEM)\n\nv_dataset = CollabDatasetOpt(valid_df, user_col=USER, item_col=ITEM)"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#pytorch-dataloader",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#pytorch-dataloader",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "PyTorch Dataloader",
    "text": "PyTorch Dataloader\n\nBatch Sampler\nSince the dataset receives a batch of indices, lets create a BatchSampler first.\n\nbs = 64 * 50\n\nt_sampler = BatchSampler(\n    RandomSampler(t_dataset),\n    batch_size=bs, drop_last=False)\n\nv_sampler = BatchSampler(\n    RandomSampler(v_dataset),\n    batch_size=bs*2, drop_last=False)\n\n\nf\"Number of batches: {len(t_sampler):,}\"\n\n'Number of batches: 4,999'\n\n\nHow much it takes to loop the train dataset:\n\n%%time\nfor _ in t_sampler: \n    t_dataset[_]\n\nCPU times: user 21.3 s, sys: 967 ms, total: 22.3 s\nWall time: 20.2 s\n\n\n\n\nDataLoader\nLets test some values for num_workers:\n\nt_dataloader = DataLoader(t_dataset, sampler=t_sampler, batch_size=None, num_workers=4)\nv_dataloader = DataLoader(v_dataset, sampler=v_sampler, batch_size=None, num_workers=4)\n\n\n%%time\nfor _ in t_dataloader: pass\n\nCPU times: user 22.1 s, sys: 8.67 s, total: 30.8 s\nWall time: 29 s\n\n\n\nt_dataloader = DataLoader(t_dataset, sampler=t_sampler, batch_size=None, num_workers=2)\nv_dataloader = DataLoader(v_dataset, sampler=v_sampler, batch_size=None, num_workers=2)\n\n\n%%time\nfor _ in t_dataloader: pass\n\nCPU times: user 21.3 s, sys: 8.21 s, total: 29.6 s\nWall time: 28.8 s\n\n\n\nt_dataloader = DataLoader(t_dataset, sampler=t_sampler, batch_size=None, num_workers=0)\nv_dataloader = DataLoader(v_dataset, sampler=v_sampler, batch_size=None, num_workers=0)\n\n\n%%time\nfor _ in t_dataloader: pass\n\nCPU times: user 20.5 s, sys: 934 ms, total: 21.5 s\nWall time: 19.3 s"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#miniai-dataloaders",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#miniai-dataloaders",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "miniai DataLoaders",
    "text": "miniai DataLoaders\n\ndls = DataLoaders(t_dataloader, v_dataloader)\n\n\ndt = dls.train\nxb, yb = next(iter(dt))\n\nLets see how does a batch looks like:\n\nxb[:5,:], yb[:5]\n\n(tensor([[ 13804,   1276],\n         [111577,    436],\n         [ 75077,    206],\n         [ 38963,    481],\n         [ 51319,   1884]]),\n tensor([5., 4., 3., 3., 5.]))\n\n\nNumber of users and items in the dataset:\n\nn_users = len(data[USER].unique())\nn_items = len(data[ITEM].unique())\nn_users, n_items\n\n(138493, 26744)"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#model",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#model",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Model",
    "text": "Model\nLets create a DotProdcutBias model with the Embedding’s weights initialized as done in fastai.\n\n# functions from fastai repository\n\ndef sigmoid_range(x, low, high):\n    \"Sigmoid function with range `(low, high)`\"\n    return torch.sigmoid(x) * (high - low) + low\n\ndef trunc_normal_(x, mean=0., std=1.):\n    \"Truncated normal initialization (approximation)\"\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)\n\nclass Embedding(nn.Embedding):\n    \"Embedding layer with truncated normal initialization\"\n    def __init__(self, ni, nf, std=0.01):\n        super().__init__(ni, nf)\n        trunc_normal_(self.weight.data, std=std)\n\n\nclass DotProductBias(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0, 5.5)):\n        super().__init__()\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range).flatten()"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#training-with-miniai",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#training-with-miniai",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Training with miniai",
    "text": "Training with miniai\n\nHyperparameters Set # 1\nAfter playing with some hyperparameters, best results were achieved with n_factors equals 100.\n\nmodel = DotProductBias(n_users, n_items, n_factors=100)\n\n\nset_seed(42)\nlr, epochs = 5e-3, 5\ntmax = epochs * len(dls.train)\n\n\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nmetrics = MetricsCB()\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), BatchSchedCB(sched)]\n\nloss_f = nn.MSELoss()\n\n\nlearn = TrainLearner(\n    model, dls, loss_f, lr=lr, cbs=cbs, \n    opt_func=partial(AdamW, weight_decay=0.15)\n)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.908\n0\ntrain\n\n\n0.706\n0\neval\n\n\n0.692\n1\ntrain\n\n\n0.691\n1\neval\n\n\n0.656\n2\ntrain\n\n\n0.654\n2\neval\n\n\n0.586\n3\ntrain\n\n\n0.603\n3\neval\n\n\n0.471\n4\ntrain\n\n\n0.591\n4\neval\n\n\n\n\n\n\n\nHyperparameters Set # 2\nSimilar results were achieved by increasing the batch size and with n_factor equals 60.\n\nbs = 64 * 200\n\nt_sampler = BatchSampler(\n    RandomSampler(t_dataset),\n    batch_size=bs, drop_last=False)\n\nv_sampler = BatchSampler(\n    RandomSampler(v_dataset),\n    batch_size=bs*2, drop_last=False)\n\n\nf\"Number of batches: {len(t_sampler):,}\"\n\n'Number of batches: 1,250'\n\n\n\nt_dataloader = DataLoader(t_dataset, sampler=t_sampler, batch_size=None)\nv_dataloader = DataLoader(v_dataset, sampler=v_sampler, batch_size=None)\n\ndls = DataLoaders(t_dataloader, v_dataloader)\n\n\nmodel = DotProductBias(n_users, n_items, n_factors=60)\n\n\nset_seed(42)\nlr, epochs = 5e-3, 5\ntmax = epochs * len(dls.train)\n\n\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\nmetrics = MetricsCB()\ncbs = [DeviceCB(), metrics, ProgressCB(plot=False), BatchSchedCB(sched)]\n\nloss_f = nn.MSELoss()\n\n\nlearn = TrainLearner(\n    model, dls, loss_f, lr=lr, cbs=cbs, \n    opt_func=partial(AdamW, weight_decay=0.15)\n)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n1.027\n0\ntrain\n\n\n0.705\n0\neval\n\n\n0.655\n1\ntrain\n\n\n0.634\n1\neval\n\n\n0.564\n2\ntrain\n\n\n0.604\n2\neval\n\n\n0.472\n3\ntrain\n\n\n0.593\n3\neval\n\n\n0.399\n4\ntrain\n\n\n0.594\n4\neval"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#recommend-movies-to-users",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#recommend-movies-to-users",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Recommend movies to users",
    "text": "Recommend movies to users\nWe now have a model that can predict user ratings of a movie because it learned latent factors (implicit features) about all users and movies in the dataset. Let’s take, for example, the user user_id=1 and create an input array of the user index and all possible movie indices.\n\nuser_id = 1\nall_movies_idx = np.arange(0, n_items)\nuser_array = np.full((n_items, 2), u2uidx[user_id])\nuser_array[:, 1] = all_movies_idx\nuser_array\n\narray([[    0,     0],\n       [    0,     1],\n       [    0,     2],\n       ...,\n       [    0, 26741],\n       [    0, 26742],\n       [    0, 26743]])\n\n\nOr better yet, lets define a function to recommend to any user we want:\n\ndevice=torch.device(\"cuda:0\")\n#device=torch.device(\"cpu\")\n\ndef recommend(user_id=1, n_items=100, k=10, device=device, df=ratings_20m):\n    \"\"\"Recommend movies the user haven't rated\n    \"\"\"\n    all_movies_idx = np.arange(0, n_items)\n    user_array = np.full((n_items, 2), u2uidx[user_id])\n    user_array[:, 1] = all_movies_idx\n    i = 0\n    with torch.no_grad():\n        preds = learn.model(torch.tensor(user_array).to(device))\n    top_indices = preds.argsort(descending=True)\n    # movies that user already rated\n    items_rated_by_user = df[df.user_id==user_id].item_id.values\n        \n    print(f'User with id={user_id} would likely like:')\n    print(f'{\"Rating\"} - {\"Movie\"}')\n\n    for iidx in top_indices:\n        # print titles the user haven't rated\n        if iidx2i[iidx.item()] not in items_rated_by_user:\n            print(f'{preds[iidx]:&gt;6.2f} - {iidx2title(iidx.item())}')\n            i += 1\n        if i &gt;= k: break\n\n\nrecommend(user_id=1, n_items=n_items)\n\nUser with id=1 would likely like:\nRating - Movie\n  4.26 - Hobbit: An Unexpected Journey, The (2012)\n  4.22 - Star Wars: Episode VI - Return of the Jedi (1983)\n  4.21 - X-Men (2000)\n  4.18 - Dark Knight, The (2008)\n  4.18 - Spider-Man (2002)\n  4.17 - Star Trek (2009)\n  4.17 - Indiana Jones and the Temple of Doom (1984)\n  4.13 - Hobbit: The Desolation of Smaug, The (2013)\n  4.12 - Avengers, The (2012)\n  4.12 - Gladiator (2000)\n\n\nAnd user with id 50:\n\nrecommend(user_id=50, n_items=n_items)\n\nUser with id=50 would likely like:\nRating - Movie\n  4.69 - Princess Bride, The (1987)\n  4.68 - Three Colors: Red (Trois couleurs: Rouge) (1994)\n  4.65 - Big Lebowski, The (1998)\n  4.57 - Breakfast Club, The (1985)\n  4.56 - Cosmos (1980)\n  4.56 - My Neighbor Totoro (Tonari no Totoro) (1988)\n  4.55 - Royal Tenenbaums, The (2001)\n  4.55 - Inglourious Basterds (2009)\n  4.54 - Star Wars: Episode V - The Empire Strikes Back (1980)\n  4.53 - Three Colors: Blue (Trois couleurs: Bleu) (1993)"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#interpreting-results",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#interpreting-results",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Interpreting Results",
    "text": "Interpreting Results\nFor a comprehensive discussion about interpreting Embeddings (Factors) and Bias please refer to Kaggle: Collaborative Filtering Deep Dive.\n\nBias\nMovies with the lowest values:\n\nmovie_bias = learn.model.movie_bias.weight.squeeze()\nidxs = movie_bias.argsort()[:5]\n\n[iidx2title(i.item()) for i in idxs]\n\n['SuperBabies: Baby Geniuses 2 (2004)',\n 'From Justin to Kelly (2003)',\n 'Glitter (2001)',\n \"Barney's Great Adventure (1998)\",\n 'Gigli (2003)']\n\n\nMovies with the highest bias:\n\nidxs = movie_bias.argsort(descending=True)[:5]\n[iidx2title(i.item()) for i in idxs]\n\n['Shawshank Redemption, The (1994)',\n \"Schindler's List (1993)\",\n 'Usual Suspects, The (1995)',\n 'Godfather, The (1972)',\n 'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)']\n\n\nThe higher the bias of the movie means that even people that is not a good match for those movies like ‘The Usual Suspects’ or ‘The Godfather’, tends to give them a good rating."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#embeddings",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#embeddings",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Embeddings",
    "text": "Embeddings\n\nPopular Movies\nLets define popular movies as movies with more than 100 ratings.\n\nn_ratings = ratings_20m.groupby('item_id')['rating'].count()\n\npop_movies_df = movies_20m.merge(\n    n_ratings, left_on='item_id', right_index=True)\npop_movies_df.columns = ['item_id', 'title', 'genres', 'n_ratings']\n\npop_movies_df = pop_movies_df[pop_movies_df.n_ratings &gt;= 100]\npop_movies_df = pop_movies_df.sort_values(\n    by=['n_ratings', 'item_id'], ascending=(False, True))\n\npop_movies_df\n\n\n\n\n\n\n\n\nitem_id\ntitle\ngenres\nn_ratings\n\n\n\n\n293\n296\nPulp Fiction (1994)\nComedy|Crime|Drama|Thriller\n67310\n\n\n352\n356\nForrest Gump (1994)\nComedy|Drama|Romance|War\n66172\n\n\n315\n318\nShawshank Redemption, The (1994)\nCrime|Drama\n63366\n\n\n587\n593\nSilence of the Lambs, The (1991)\nCrime|Horror|Thriller\n63299\n\n\n476\n480\nJurassic Park (1993)\nAction|Adventure|Sci-Fi|Thriller\n59715\n\n\n...\n...\n...\n...\n...\n\n\n12271\n56336\nWrong Turn 2: Dead End (2007)\nAction|Horror|Thriller\n100\n\n\n12722\n59915\nStuck (2007)\nHorror|Thriller\n100\n\n\n14350\n71878\nStepfather, The (2009)\nHorror|Thriller\n100\n\n\n15860\n80469\nSuperman/Batman: Apocalypse (2010)\nAnimation\n100\n\n\n23754\n112911\nHercules (2014)\nAction|Adventure\n100\n\n\n\n\n8546 rows × 4 columns\n\n\n\n\npop_movies_id = pop_movies_df.item_id\n\n# item_id to dataloader's item index\npop_movies_idx = np.array([i2iidx[id] for id in pop_movies_id])\n\n# item index to titles\npop_movies_title = np.array([iidx2title(idx.item()) for idx in pop_movies_idx])\n\n\n\nPopular movies embeddings\n\npop_movies_emb = learn.model.movie_factors.weight[pop_movies_idx].cpu().detach()\npop_movies_emb.shape\n\ntorch.Size([8546, 60])\n\n\nThe embeddings are a vector of size 60.\n\npop_movies_emb[0]\n\ntensor([-0.2493,  0.3020,  0.9561,  0.5596,  0.7226,  0.3335, -0.3435,  0.0695,\n         0.4565, -0.3386, -0.4577, -1.2690, -0.1703, -0.4787, -0.4197,  0.8845,\n        -0.5262, -0.3538,  0.4294,  0.6587,  0.0867,  0.2583,  0.2544, -0.8050,\n         0.4216, -0.8228,  0.5268,  0.6913,  0.3080, -1.0402,  0.2570,  0.3762,\n         0.0897, -0.7355,  0.6587, -0.2353, -1.2990, -0.7492,  1.0346,  0.5964,\n         0.7656, -0.0978,  0.3987,  0.4051, -0.1287,  0.6400, -0.6192,  1.2921,\n        -0.5318,  0.6024, -0.7220, -0.5899, -0.7019,  0.5004, -0.2944, -0.5028,\n        -0.2923, -0.0144, -0.4991,  0.2179])\n\n\n\nall(pop_movies_title==pop_movies_df.title.values)\n\nTrue"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#visualizing-the-embeddings",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#visualizing-the-embeddings",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Visualizing the embeddings",
    "text": "Visualizing the embeddings\n\nDimensionality reduction: PCA\n\n# from fastai library\n\ndef pca(x:torch.tensor, k=2):\n    \"Compute PCA of `x` with `k` dimensions.\"\n    x = x-torch.mean(x,0)\n    U,S,V = torch.svd(x.t())\n    return torch.mm(x,U[:,:k])\n\n\nmovie_pca = pca(pop_movies_emb, 3)\nfac0, fac1, fac2 = movie_pca.t()\n\nidxs = list(range(30))\nX_pca = fac0\nY_pca = fac2\n\n\npop_movies_df['x_pca'] = X_pca\npop_movies_df['y_pca'] = Y_pca\n\n\n\nDimensionality reduction: t-SNE\nKaggle: Visualizing Embeddings With t-SNE\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=1, n_iter=1_500, metric=\"cosine\")\n\nmovie_tsne = tsne.fit_transform(\n    pop_movies_emb\n)\n\n/home/fmussari/mambaforge/envs/fastaiv3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n  warnings.warn(\n/home/fmussari/mambaforge/envs/fastaiv3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n  warnings.warn(\n/home/fmussari/mambaforge/envs/fastaiv3/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n  warnings.warn(\n\n\n\npop_movies_df['x'] = movie_tsne[:, 0]\npop_movies_df['y'] = movie_tsne[:, 1]\n\n\n\nHelper functions\nThese functions are a simplification of those in the aforementioned Kaggle notebook.\n\ndef plot_bg(df, x='x', y='y', bg_alpha=.01, figsize=(12, 8)):\n    \"\"\"Create and return a plot of all movie embeddings with very low opacity.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.scatter(df[x], df[y], alpha=bg_alpha)\n    return ax\n\ndef plot_by_title_pattern(df, pattern, x='x', y='y'):\n    \"\"\"Plot all movies whose titles match the given regex pattern.\n    \"\"\"\n    match = df[df.title.str.contains(pattern)]\n    return plot_with_annotations(df, match.index, x=x, y=y)\n\ndef plot_list_of_values(df, list_of_values, column='item_id'):\n    \"\"\"Plot all movies whose \"column\" values are in list_of_values.\n    \"\"\"\n    match = df[df[column].isin(list_of_values)]\n    return plot_with_annotations(df, match.index)\n\ndef plot_with_annotations(df, label_indices, x='x', y='y'):\n    \"\"\"Create a plot of all movie embeddings with very low opacity and\n    highlight values whose indices are in in label_indices.\n    \"\"\"\n    ax = plot_bg(df, x=x, y=y)\n    print(f'Highlighting {len(label_indices)} items')\n    Xlabeled = df[x].loc[label_indices]\n    Ylabeled = df[y].loc[label_indices]\n    ax.scatter(Xlabeled, Ylabeled, alpha=0.5, color='green')\n\n    return ax\n\ndef plot_region_around(df, title, margin=5, figsize=(10, 6)):\n    \"\"\"Plot the region of the mapping space bounded by the given x and y limits.\n    \"\"\"\n    xmargin = ymargin = margin\n    match = df[df.title == title]\n    assert len(match) == 1\n    row = match.iloc[0]\n    return plot_region(\n        df, row.x-xmargin, row.x+xmargin, row.y-ymargin, row.y+ymargin)\n\ndef plot_region(df, x0, x1, y0, y1, text=True, figsize=(10, 6)):\n    \"\"\"Plot the region of the mapping space bounded by the given x and y limits.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n    limits = (\n        (df.x &gt;= x0) & (df.x &lt;= x1) & (df.y &gt;= y0) & (df.y &lt;= y1))\n    pts = df[limits]\n    print(f\"Showing {sum(limits)} items\")\n    ax.scatter(pts.x, pts.y, alpha=.6)\n    ax.set_xlim(x0, x1)\n    ax.set_ylim(y0, y1)\n    if text:\n        texts = []\n        for label, x, y in zip(pts.title.values, pts.x.values, pts.y.values):\n            t = ax.annotate(label, xy=(x, y))\n            texts.append(t)\n    return ax\n\n\n\nPCA\n\nplot_bg(df=pop_movies_df, x='x_pca', y='y_pca', bg_alpha=.05);\n\n\n\n\n\n\nt-SNE\n\nplot_bg(df=pop_movies_df, x='x', y='y', bg_alpha=.05);\n\n\n\n\n\n\nHighlight movies\nRemember that the model only had user_id, item_id and ratings as features. All relations and patterns that we are going to see from now on were learned as latent factor only from the interaction between users and movies. With no data about movies or users themselves.\n\nHarry Potter\n\nplot_by_title_pattern(pop_movies_df, 'Harry Potter');\n\nHighlighting 8 items\n\n\n\n\n\nThe 8 Harry Potter movies are so close that are almost indistinguishable in the plot.\n\npop_movies_df[pop_movies_df.title.str.contains('Harry Potter')][['item_id', 'title', 'x', 'y']]\n\n\n\n\n\n\n\n\nitem_id\ntitle\nx\ny\n\n\n\n\n4800\n4896\nHarry Potter and the Sorcerer's Stone (a.k.a. ...\n-88.674049\n4.128831\n\n\n5717\n5816\nHarry Potter and the Chamber of Secrets (2002)\n-88.672592\n4.128721\n\n\n7769\n8368\nHarry Potter and the Prisoner of Azkaban (2004)\n-88.670090\n4.129076\n\n\n10600\n40815\nHarry Potter and the Goblet of Fire (2005)\n-88.668808\n4.128649\n\n\n11974\n54001\nHarry Potter and the Order of the Phoenix (2007)\n-88.666321\n4.128205\n\n\n13935\n69844\nHarry Potter and the Half-Blood Prince (2009)\n-88.664932\n4.127958\n\n\n17499\n88125\nHarry Potter and the Deathly Hallows: Part 2 (...\n-88.660225\n4.126957\n\n\n16191\n81834\nHarry Potter and the Deathly Hallows: Part 1 (...\n-88.659821\n4.126904\n\n\n\n\n\n\n\n\nplot_region_around(\n    pop_movies_df, 'Harry Potter and the Order of the Phoenix (2007)', margin=0.008);\n\nShowing 8 items\n\n\n\n\n\nBy looking at the axis scale, we can see that they are very, very close. Impressive.\n\n\nStar Wars\n\npop_movies_df[pop_movies_df.title.str.contains('Star Wars')]\n\n\n\n\n\n\n\n\nitem_id\ntitle\ngenres\nn_ratings\nx_pca\ny_pca\nx\ny\n\n\n\n\n257\n260\nStar Wars: Episode IV - A New Hope (1977)\nAction|Adventure|Sci-Fi\n54502\n-0.246074\n-0.417709\n-70.578560\n13.257028\n\n\n1184\n1210\nStar Wars: Episode VI - Return of the Jedi (1983)\nAction|Adventure|Sci-Fi\n46839\n0.144319\n-0.921739\n-70.577507\n13.240388\n\n\n1171\n1196\nStar Wars: Episode V - The Empire Strikes Back...\nAction|Adventure|Sci-Fi\n45313\n-0.387326\n-0.676300\n-70.583687\n13.270293\n\n\n2543\n2628\nStar Wars: Episode I - The Phantom Menace (1999)\nAction|Adventure|Sci-Fi\n29574\n0.924800\n-0.526612\n-70.411499\n10.423186\n\n\n5281\n5378\nStar Wars: Episode II - Attack of the Clones (...\nAction|Adventure|Sci-Fi|IMAX\n16425\n0.776779\n-0.675660\n-70.408615\n10.422228\n\n\n10117\n33493\nStar Wars: Episode III - Revenge of the Sith (...\nAction|Adventure|Sci-Fi\n12303\n0.525108\n-0.795317\n-70.412682\n10.452327\n\n\n12926\n61160\nStar Wars: The Clone Wars (2008)\nAction|Adventure|Animation|Sci-Fi\n843\n0.652965\n-0.284741\n-70.394539\n10.364397\n\n\n\n\n\n\n\n\nplot_by_title_pattern(pop_movies_df, 'Star Wars');\n\nHighlighting 7 items\n\n\n\n\n\n\nplot_region_around(\n    pop_movies_df, 'Star Wars: Episode I - The Phantom Menace (1999)', margin=0.06);\n\nShowing 4 items\n\n\n\n\n\n\nplot_region_around(\n    pop_movies_df, 'Star Wars: Episode IV - A New Hope (1977)', margin=0.06);\n\nShowing 3 items\n\n\n\n\n\nThat’s impressive, isn’t it? There are two clusters that at the same time are close. One cluster is for the ones released in the 1980s (original trilogy), and the other for the ones released after 1999. It is as if the years were a model’s feature.\n\n\nGodfather\n\npop_movies_df[pop_movies_df.title.str.contains('Godfather')]\n\n\n\n\n\n\n\n\nitem_id\ntitle\ngenres\nn_ratings\nx_pca\ny_pca\nx\ny\n\n\n\n\n843\n858\nGodfather, The (1972)\nCrime|Drama\n41355\n-1.144734\n-0.149714\n-0.402252\n66.513908\n\n\n1195\n1221\nGodfather: Part II, The (1974)\nCrime|Drama\n27398\n-1.063627\n-0.211216\n-0.408337\n66.517693\n\n\n1939\n2023\nGodfather: Part III, The (1990)\nCrime|Drama|Mystery|Thriller\n10141\n0.094486\n-0.535812\n71.090759\n54.931854\n\n\n7924\n8607\nTokyo Godfathers (2003)\nAdventure|Animation|Drama\n705\n-0.615767\n-0.214817\n43.958630\n21.984804\n\n\n\n\n\n\n\n\nlist_of_item_ids = [858, 1221, 2023]\n\n\nplot_list_of_values(pop_movies_df, list_of_item_ids, column='item_id');\n\nHighlighting 3 items\n\n\n\n\n\n\nplot_region_around(pop_movies_df, 'Godfather, The (1972)', margin=0.1);\n\nShowing 2 items\n\n\n\n\n\nThe two of them released in the 1970s were very close.\n\n\nToy Story\n\npop_movies_df[pop_movies_df.title.str.contains('Toy Story')]\n\n\n\n\n\n\n\n\nitem_id\ntitle\ngenres\nn_ratings\nx_pca\ny_pca\nx\ny\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n49695\n-0.146245\n0.237802\n24.761007\n-54.195663\n\n\n3027\n3114\nToy Story 2 (1999)\nAdventure|Animation|Children|Comedy|Fantasy\n22770\n-0.209496\n0.377863\n24.759005\n-54.147175\n\n\n15401\n78499\nToy Story 3 (2010)\nAdventure|Animation|Children|Comedy|Fantasy|IMAX\n5781\n-0.242506\n-0.096230\n24.653181\n-53.001858\n\n\n\n\n\n\n\n\nplot_by_title_pattern(pop_movies_df, 'Toy Story');\n\nHighlighting 3 items\n\n\n\n\n\n\nplot_region_around(pop_movies_df, 'Toy Story 3 (2010)', margin=1.25);\n\nShowing 9 items\n\n\n\n\n\nThe two of them released in the 1990s are very close. The one released in 2010 is also near. And in the surroundings we can see more Pixar movies. The model clustered almost perfectly Pixar movies that won the Oscars."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#highlight-genres",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#highlight-genres",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Highlight genres",
    "text": "Highlight genres\n\nDocumentary\n\ndocs = pop_movies_df[(pop_movies_df.genres == 'Documentary')]\nplot_with_annotations(pop_movies_df, docs.index);\n\nHighlighting 242 items\n\n\n\n\n\n\n\nHorror\n\ndocs = pop_movies_df[(pop_movies_df.genres == 'Horror')]\nplot_with_annotations(pop_movies_df, docs.index);\n\nHighlighting 156 items\n\n\n\n\n\nThere is definitely some clustering happening."
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#find-similar-movies",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#find-similar-movies",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Find similar movies",
    "text": "Find similar movies\nThe blog post Building a cost-effective image vector search engine with CLIP teaches us, among other things, how to calculate cosine similarity and find the nearest vector from a given one. This allows us to find movies that are similar to a given movie according to the embeddings our model has learned.\nLets first define some functions.\n\ndef normalize(a): return a / a.norm(dim=-1, keepdim=True)\ndef cosine_sim(a, b): return normalize(a) @ normalize(b).T\n\ndef search(embs, query_embs):\n    sims = cosine_sim(embs, query_embs).flatten()\n    indices = sims.argsort(descending=True)\n    return indices, sims[indices]\n\ndef most_similar_by_idx(items_emb, idx, k=10):\n    indices, sim = search(items_emb, items_emb[idx])\n    print(f'Movies similar to:\\n\"{pop_movies_title[idx]}\"')\n    print('  sim - movie')\n    for i, idx in enumerate(indices[1:k+1]):\n        print(f\"{sim[i]:.3f} - {pop_movies_title[idx]}\")\n        \ndef most_similar_by_title(items_emb, title, k=10):\n    idx = np.where(pop_movies_title==title)[0][0]\n    return most_similar_by_idx(items_emb, idx, k=10)\n   \ndef display_indices(word):\n    print(f'{\"index\":&lt;5}, titles')\n    for idx, title in enumerate(pop_movies_title):\n        if word in title:\n            print(f'{idx:&gt;5}, {title}')\n\n\ntitle = \"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\"\nmost_similar_by_title(pop_movies_emb, title=title)\n\nMovies similar to:\n\"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\"\n  sim - movie\n1.000 - Harry Potter and the Chamber of Secrets (2002)\n0.991 - Harry Potter and the Goblet of Fire (2005)\n0.956 - Harry Potter and the Prisoner of Azkaban (2004)\n0.955 - Harry Potter and the Order of the Phoenix (2007)\n0.925 - Harry Potter and the Half-Blood Prince (2009)\n0.907 - Harry Potter and the Deathly Hallows: Part 2 (2011)\n0.881 - Harry Potter and the Deathly Hallows: Part 1 (2010)\n0.862 - Hunger Games: Catching Fire, The (2013)\n0.561 - Chronicles of Narnia: The Lion, the Witch and the Wardrobe, The (2005)\n0.544 - Chronicles of Narnia: Prince Caspian, The (2008)\n\n\nAs we saw with t-SNE, the Harry Potter films are very closely related to each other.\nLets try with some less popular like Half Nelson.\n\nmost_similar_by_title(pop_movies_emb, title='Half Nelson (2006)')\n\nMovies similar to:\n\"Half Nelson (2006)\"\n  sim - movie\n1.000 - Rachel Getting Married (2008)\n0.786 - Requiem (2006)\n0.764 - Hollow Reed (1996)\n0.750 - Blue Is the Warmest Color (La vie d'Adèle) (2013)\n0.749 - And Your Mother Too (Y tu mamá también) (2001)\n0.748 - Squid and the Whale, The (2005)\n0.743 - Upstream Color (2013)\n0.739 - Calvary (2014)\n0.737 - Animal Kingdom (2010)\n0.735 - Three Burials of Melquiades Estrada, The (2006)\n\n\n\nmost_similar_by_title(pop_movies_emb, title='Blue Velvet (1986)')\n\nMovies similar to:\n\"Blue Velvet (1986)\"\n  sim - movie\n1.000 - Lolita (1962)\n0.724 - Eraserhead (1977)\n0.723 - Wild at Heart (1990)\n0.711 - Mulholland Drive (2001)\n0.709 - Tin Drum, The (Blechtrommel, Die) (1979)\n0.704 - Aguirre: The Wrath of God (Aguirre, der Zorn Gottes) (1972)\n0.698 - Videodrome (1983)\n0.697 - Barton Fink (1991)\n0.688 - Man Who Fell to Earth, The (1976)\n0.688 - Naked Lunch (1991)\n\n\nWe can see some other Lynch’s movies there and some Kubrick’s.\n\nmost_similar_by_title(pop_movies_emb, title=\"Full Metal Jacket (1987)\")\n\nMovies similar to:\n\"Full Metal Jacket (1987)\"\n  sim - movie\n1.000 - Apocalypse Now (1979)\n0.740 - Platoon (1986)\n0.730 - Goodfellas (1990)\n0.672 - Clockwork Orange, A (1971)\n0.660 - Deer Hunter, The (1978)\n0.640 - Scarface (1983)\n0.633 - Reservoir Dogs (1992)\n0.631 - Once Upon a Time in America (1984)\n0.602 - City of God (Cidade de Deus) (2002)\n0.601 - Straw Dogs (1971)\n\n\n\ndisplay_indices('Átame')\n\nindex, titles\n 2413, Tie Me Up! Tie Me Down! (¡Átame!) (1990)\n\n\n\nmost_similar_by_title(pop_movies_emb, title=\"Tie Me Up! Tie Me Down! (¡Átame!) (1990)\")\n\nMovies similar to:\n\"Tie Me Up! Tie Me Down! (¡Átame!) (1990)\"\n  sim - movie\n1.000 - Live Flesh (Carne trémula) (1997)\n0.776 - Women on the Verge of a Nervous Breakdown (Mujeres al borde de un ataque de nervios) (1988)\n0.750 - Sex and Lucia (Lucía y el sexo) (2001)\n0.748 - All About My Mother (Todo sobre mi madre) (1999)\n0.745 - 1900 (Novecento) (1976)\n0.737 - High Heels (Tacones lejanos) (1991)\n0.727 - Wild Reeds (Les roseaux sauvages) (1994)\n0.722 - What Have I Done to Deserve This? (¿Qué he hecho yo para merecer esto!!) (1984)\n0.716 - Keep the River on Your Right: A Modern Cannibal Tale (2000)\n0.716 - Thieves (Voleurs, Les) (1996)\n\n\nMostly Almodovar´s there.\nRemember that t-SNE placed The Godfather III far away from the other two? Well, as we can see here cosine similarity with the full embeddings place them really close.\n\nmost_similar_by_title(pop_movies_emb, title=\"Godfather: Part III, The (1990)\")\n\nMovies similar to:\n\"Godfather: Part III, The (1990)\"\n  sim - movie\n1.000 - Godfather: Part II, The (1974)\n0.626 - Godfather, The (1972)\n0.574 - Once Upon a Time in America (1984)\n0.538 - Scent of a Woman (1992)\n0.503 - Carlito's Way (1993)\n0.500 - Hollywood Ending (2002)\n0.482 - Scarface (1983)\n0.475 - Best Offer, The (Migliore offerta, La) (2013)\n0.474 - Casino Jack (2010)\n0.472 - Devil's Advocate, The (1997)"
  },
  {
    "objectID": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#conclusions",
    "href": "posts/2023-07_collab_filter_miniai/2023-07_MovieLens_20M_Part1.html#conclusions",
    "title": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M",
    "section": "Conclusions",
    "text": "Conclusions\n\nIt is very impressive how the model can cluster movies without knowing anything explicit about them with such a simple architecture like a DotProductBias model. It learned features that clustered movies by sagas, directors or studios.\nFor this case t-SNE was very good representing clusters in a reduced dimensionality."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "",
    "text": "Part 1 was about creating a Vertex AI pipeline for training and deploying Matrix Factorization model using BigQuery ML.\nWe ended with a model we can use from BigQuery, but that is also available as a Cloud Run endpoint, deployed as a Docker container.\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is better suited for running in Colab."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#overview",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#overview",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "",
    "text": "Part 1 was about creating a Vertex AI pipeline for training and deploying Matrix Factorization model using BigQuery ML.\nWe ended with a model we can use from BigQuery, but that is also available as a Cloud Run endpoint, deployed as a Docker container.\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is better suited for running in Colab."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#authenticate",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#authenticate",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Authenticate",
    "text": "Authenticate\n\nfrom google.colab import auth\n\nauth.authenticate_user()"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#get-recommendations-from-deployed-endpoint",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#get-recommendations-from-deployed-endpoint",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Get recommendations from deployed endpoint",
    "text": "Get recommendations from deployed endpoint\nAs defined in part 1, the service was deployed as recommender-model.\n\nSERVICE = \"recommender-model\"\nREGION = 'us-central1'\nPROJECT_ID = \"[your-project-id]\"\n\nTo get the endpoint url, you can navigate to Cloud Run console (insert your Project ID):\nhttps://console.cloud.google.com/run/detail//recommender-model/metrics?project=[PROJECT_ID]\nOr you can run the following command:\n\nservice_url = ! gcloud run services describe {SERVICE} --platform managed --region {REGION} --format 'value(status.url)'\nservice_url = service_url[0] + \"/v1/models/recommender_model:predict\"\nservice_url\n\nAnd the endpoint should look something like this:\nhttps://recommender-model-&lt;ENDPOINT_ID&gt;-uc.a.run.app/v1/models/recommender_model:predict\n\nCall the deployed model endpoint\nTo call the API simply use curl. In this case we get recommendations for the user_id equals 1:\n\n! curl --header \"Content-Type: application/json\" \\\n  --request POST \\\n  --data '{\"instances\": [{\"user_id\": 1}]}' \\\n  https://recommender-model-&lt;ENDPOINT_ID&gt;-uc.a.run.app/v1/models/recommender_model:predict\n\n\n\n{\n    \"predictions\": [\n        {\n            \"predicted_rating\": [7.1595457186221525, 6.931200454403859, 6.7487105522542379, 6.6099371431826093, 6.3368375925279663, 6.3162291916598949, 6.2034087300038294, 6.1908018877869777, 6.095065519595515, 6.0748385283824344, 6.0707896719918022, 6.0115152056191317, 5.9950883198035312, 5.9266960142259579, 5.8977393292303706, 5.8963543166167049, 5.8879213166109317, 5.8651216371567951, 5.8157813717342961, 5.7918400837633026, 5.7728366859651743, 5.7459233895973343, 5.7286289144902627, 5.7236862593334399, 5.6735891385114323, 5.6712837959564055, 5.6344651298840862, 5.6144777548802312, 5.6136065773986719, 5.5966244955484585, 5.5826125214031448, 5.5438993473737597, 5.5405328509132108, 5.5156535458862912, 5.5060371378140998, 5.4983916934699879, 5.4779312652757479, 5.4381644519356804, 5.436539915444734, 5.4274984607954586, 5.4162018042063238, 5.4131954911985822, 5.4112950178511134, 5.3977833272890878, 5.3924309392602101, 5.3921355023131454, 5.3795566101479775, 5.3784815662419794, 5.3724805166998593, 5.3716275159116167],\n            \"predicted_item_id\": [\"3382\", \"3920\", \"2178\", \"662\", \"469\", \"199\", \"215\", \"2073\", \"881\", \"3132\", \"1900\", \"1615\", \"3141\", \"2928\", \"445\", \"973\", \"3605\", \"766\", \"935\", \"1841\", \"2330\", \"944\", \"2314\", \"3447\", \"2171\", \"3327\", \"2419\", \"2351\", \"557\", \"3720\", \"156\", \"2066\", \"2475\", \"2970\", \"26\", \"2360\", \"3018\", \"2272\", \"2506\", \"3670\", \"1631\", \"414\", \"2165\", \"1177\", \"2264\", \"743\", \"2725\", \"1934\", \"2710\", \"3275\"]\n        }\n    ]\n}\n\n\nWhich return the top 50 items (movies) by predicted rating for the specified user.\nAfter processing the output, we can get a dataframe like this (remember this are predictions for user_id=1):\n\nendpoint_predictions_df.head(10)\n\n\n\n\n\n\n\n\npredicted_rating\npredicted_item_id\n\n\n\n\n0\n7.159546\n3382\n\n\n1\n6.931200\n3920\n\n\n2\n6.748711\n2178\n\n\n3\n6.609937\n662\n\n\n4\n6.336838\n469\n\n\n5\n6.316229\n199\n\n\n6\n6.203409\n215\n\n\n7\n6.190802\n2073\n\n\n8\n6.095066\n881\n\n\n9\n6.074839\n3132"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#bigquery",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#bigquery",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "BigQuery",
    "text": "BigQuery\n\nAuthenticate to BigQuery\n\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID)\n\n\nSet the same parameters as in Part 1\n\nMODEL_DATASET = \"[bq-model-dataset]\"  # @param {type:\"string\"}\nMOVIELENS_DATASET = \"[bq-data-dataset]\"  # @param {type:\"string\"}\n\n\n\n\nQuery the data tables\n\nmovies_df = client.query(f'''\n  SELECT *\n  FROM `{MOVIELENS_DATASET}.movie_titles`\n''').to_dataframe()\nmovies_df.head()\n\n\n\n\n\n\n\n\nmovie_id\nmovie_title\ngenre\n\n\n\n\n0\n632\nLand and Freedom (Tierra y libertad) (1995)\nWar\n\n\n1\n665\nUnderground (1995)\nWar\n\n\n2\n760\nStalingrad (1993)\nWar\n\n\n3\n777\nPharaoh's Army (1995)\nWar\n\n\n4\n1450\nPrisoner of the Mountains (Kavkazsky Plennik) ...\nWar\n\n\n\n\n\n\n\n\nuser_ratings_df = client.query(f'''\n  SELECT t1.user_id, t1.rating, t1.item_id, t2.movie_title, t2.genre\n  FROM `{MOVIELENS_DATASET}.movielens_1m` t1\n  LEFT JOIN {MOVIELENS_DATASET}.movie_titles t2\n  ON t1.item_id=t2.movie_id\n  --WHERE t1.user_id=1\n  ORDER BY rating DESC\n''').to_dataframe()\nuser_ratings_df.head()\n\n\n\n\n\n\n\n\nuser_id\nrating\nitem_id\nmovie_title\ngenre\n\n\n\n\n0\n1\n5.0\n1193\nOne Flew Over the Cuckoo's Nest (1975)\nDrama\n\n\n1\n1\n5.0\n2355\nBug's Life, A (1998)\nAnimation|Children's|Comedy\n\n\n2\n1\n5.0\n1287\nBen-Hur (1959)\nAction|Adventure|Drama\n\n\n3\n1\n5.0\n2804\nChristmas Story, A (1983)\nComedy|Drama\n\n\n4\n1\n5.0\n595\nBeauty and the Beast (1991)\nAnimation|Children's|Musical\n\n\n\n\n\n\n\n\n\nInformation about model(s) created\n\nmodels = client.list_models(MODEL_DATASET)\nprint(\"Models contained in '{}':\".format(MODEL_DATASET))\n\nmodels_fmid = []\nfor model in models:\n    full_model_id = f\"{model.project}.{model.dataset_id}.{model.model_id}\"\n    models_fmid.append(full_model_id)\n\n    print(f\"{full_model_id}\")\n\nModels contained in 'bqml_modelos':\npythonapi-205723.bqml_modelos.mf_model_pipe_01\npythonapi-205723.bqml_modelos.mf_model_pipe_02\npythonapi-205723.bqml_modelos.mf_model_pipe_03\n\n\nAs you can see, I created three models in bqml_modelos dataset.\n\nclient.get_model(models_fmid[0])\n\nModel(reference=ModelReference(project_id='pythonapi-205723', dataset_id='bqml_modelos', model_id='mf_model_pipe_01'))\n\n\n\nmodel.model_type\n\n'MATRIX_FACTORIZATION'\n\n\n\n\nTraining statistics\n\nclient.query(f'''\n  SELECT *\n  FROM ML.TRAINING_INFO(MODEL `{models_fmid[2]}`)\n''').to_dataframe()\n\n\n\n\n\n\n\n\ntraining_run\niteration\nloss\neval_loss\nduration_ms\n\n\n\n\n0\n0\n15\n0.259132\nNaN\n64410\n\n\n1\n0\n14\n0.261374\nNaN\n107980\n\n\n2\n0\n13\n0.265850\nNaN\n60169\n\n\n3\n0\n12\n0.268827\nNaN\n29616\n\n\n4\n0\n11\n0.274483\nNaN\n57306\n\n\n5\n0\n10\n0.278608\nNaN\n117195\n\n\n6\n0\n9\n0.286101\nNaN\n59622\n\n\n7\n0\n8\n0.292234\nNaN\n115613\n\n\n8\n0\n7\n0.302932\nNaN\n62337\n\n\n9\n0\n6\n0.313072\nNaN\n101301\n\n\n10\n0\n5\n0.330243\nNaN\n131648\n\n\n11\n0\n4\n0.350469\nNaN\n118377\n\n\n12\n0\n3\n0.385341\nNaN\n55254\n\n\n13\n0\n2\n0.449968\nNaN\n103391\n\n\n14\n0\n1\n0.581976\nNaN\n64919\n\n\n15\n0\n0\n4.047567\nNaN\n168618\n\n\n\n\n\n\n\n\n\nEvaluation\n\nclient.query(f'''\nSELECT *\nFROM ML.EVALUATE(MODEL `{models_fmid[2]}`,\n    (\n    SELECT\n      user_id,\n      item_id,\n      rating\n     FROM\n      {MOVIELENS_DATASET}.movielens_1m))\n''').to_dataframe()\n\n\n\n\n\n\n\n\nmean_absolute_error\nmean_squared_error\nmean_squared_log_error\nmedian_absolute_error\nr2_score\nexplained_variance\n\n\n\n\n0\n0.382252\n0.259132\n0.016406\n0.293064\n0.792348\n0.792348"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#rating-predictions",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#rating-predictions",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Rating predictions",
    "text": "Rating predictions\nLets get predicted ratings for all user-item pairs.\n\n\n\n\n\n\nNote\n\n\n\nThis query generates a predicted rating for every user-item pair. As a result, this can be a rather large output. It is recommended that the output is saved in a table which is then used in other queries for analysis.\n\n\nLets create one table to store all predictions called recommend_m2_1m and another one with top predictions by user called recommend_m2_1m_top.\n\nrecommend_m2_1m_df = client.query(f'''\nSELECT *\nFROM ML.RECOMMEND(MODEL bqml_modelos.mf_model_pipe_03)\n''').to_dataframe()\n\n\nrecommend_m2_1m_df\n\n\n\n\n\n\n\n\npredicted_rating\nuser_id\nitem_id\n\n\n\n\n0\n3.489891\n3575\n1376\n\n\n1\n6.627501\n2213\n1362\n\n\n2\n2.694415\n5876\n1330\n\n\n3\n-1.662953\n4817\n3042\n\n\n4\n2.208275\n5448\n563\n\n\n...\n...\n...\n...\n\n\n22384235\n3.183006\n933\n2282\n\n\n22384236\n1.201612\n5843\n391\n\n\n22384237\n2.886981\n4321\n3525\n\n\n22384238\n2.488141\n1000\n2881\n\n\n22384239\n2.805384\n5148\n1049\n\n\n\n\n22384240 rows × 3 columns\n\n\n\n\nprint(f\"Number of users: {len(user_ratings_df.user_id.unique()):,}\")\nprint(f\"Number of movies: {len(user_ratings_df.item_id.unique()):,}\")\nassert len(user_ratings_df.user_id.unique()) * len(user_ratings_df.item_id.unique()) == len(recommend_m2_1m_df)\n\nNumber of users: 6,040\nNumber of movies: 3,706\n\n\nAs we can see, the full table has 22,384,240 rows, equals to 6,040 users x 3,706 movies.\n\nCompare to endpoint results\n\nrecommend_m2_1m_df[recommend_m2_1m_df.user_id==1].sort_values(by='predicted_rating', ascending=False).head(10)\n\n\n\n\n\n\n\n\npredicted_rating\nuser_id\nitem_id\n\n\n\n\n50602\n7.159546\n1\n3382\n\n\n22190133\n6.931200\n1\n3920\n\n\n5356399\n6.748711\n1\n2178\n\n\n15157835\n6.609937\n1\n662\n\n\n4687764\n6.336838\n1\n469\n\n\n17892355\n6.316229\n1\n199\n\n\n1365223\n6.203409\n1\n215\n\n\n16742143\n6.190802\n1\n2073\n\n\n8094661\n6.095066\n1\n881\n\n\n21623544\n6.074839\n1\n3132\n\n\n\n\n\n\n\n\nendpoint_predictions_df.head(10)\n\n\n\n\n\n\n\n\npredicted_rating\npredicted_item_id\n\n\n\n\n0\n7.159546\n3382\n\n\n1\n6.931200\n3920\n\n\n2\n6.748711\n2178\n\n\n3\n6.609937\n662\n\n\n4\n6.336838\n469\n\n\n5\n6.316229\n199\n\n\n6\n6.203409\n215\n\n\n7\n6.190802\n2073\n\n\n8\n6.095066\n881\n\n\n9\n6.074839\n3132\n\n\n\n\n\n\n\nFor user_id=1 and item_id=3382, the predicted rating is 7.159546."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#latent-factors-embeddings-and-bias",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#latent-factors-embeddings-and-bias",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Latent Factors (Embeddings) and Bias",
    "text": "Latent Factors (Embeddings) and Bias\nThe matrix factorization model trained in BigQuery ML returns a predicted rating as the following equation:\npredicted = Global Bias + DotProduct(user_factors, item_factors) + user_bias + item_bias\nWhere Global Bias is a constant for all the pairs, user_factors and item_factors are vectors with 60 dimensions (as definied in the training process) and user_bias and item_bias are a distinct number for each user and for each item.\n\nGlobal Bias\n\nglobal_bias = client.query(f\"\"\"\n    SELECT intercept\n    FROM ML.WEIGHTS(model {MODEL_DATASET}.mf_model_pipe_03)\n    WHERE feature = 'global__INTERCEPT__'\n\"\"\").to_dataframe()\n\n\nglobal_bias_np = global_bias.loc[0].values\nglobal_bias_np\n\narray([3.58156445])\n\n\n\n\nMovie Embeddings\n\nitem_factors_df = client.query(f\"\"\"\n    SELECT feature AS item_id, intercept AS bias, fw.factor AS factor, fw.weight\n    FROM ML.WEIGHTS(model {MODEL_DATASET}.mf_model_pipe_03),\n    UNNEST(factor_weights) AS fw\n    WHERE processed_input = 'item_id'\n\"\"\").to_dataframe()\n\n\nitem_factors_df\n\n\n\n\n\n\n\n\nitem_id\nbias\nfactor\nweight\n\n\n\n\n0\n2\n-0.393036\n60\n-0.113832\n\n\n1\n2\n-0.393036\n59\n0.048548\n\n\n2\n2\n-0.393036\n58\n-0.112024\n\n\n3\n2\n-0.393036\n57\n0.118971\n\n\n4\n2\n-0.393036\n56\n-0.050817\n\n\n...\n...\n...\n...\n...\n\n\n222355\n3833\n-1.592288\n5\n0.123739\n\n\n222356\n3833\n-1.592288\n4\n0.234778\n\n\n222357\n3833\n-1.592288\n3\n-0.296197\n\n\n222358\n3833\n-1.592288\n2\n0.502392\n\n\n222359\n3833\n-1.592288\n1\n-0.232240\n\n\n\n\n222360 rows × 4 columns\n\n\n\n\n\nUser Embeddings\n\nuser_factors_df = client.query(\"\"\"\n    SELECT feature AS user_id, intercept AS bias, fw.factor AS factor, fw.weight\n    FROM ML.WEIGHTS(model bqml_modelos.mf_model_pipe_03),\n    UNNEST(factor_weights) AS fw\n    WHERE processed_input = 'user_id'\n\"\"\").to_dataframe()\n\n\nuser_factors_df\n\n\n\n\n\n\n\n\nuser_id\nbias\nfactor\nweight\n\n\n\n\n0\n256\n0.255283\n60\n0.713645\n\n\n1\n256\n0.255283\n59\n1.387058\n\n\n2\n256\n0.255283\n58\n-0.567324\n\n\n3\n256\n0.255283\n57\n-0.709545\n\n\n4\n256\n0.255283\n56\n-1.813568\n\n\n...\n...\n...\n...\n...\n\n\n362395\n5887\n-0.408196\n5\n1.493551\n\n\n362396\n5887\n-0.408196\n4\n0.960923\n\n\n362397\n5887\n-0.408196\n3\n-2.272016\n\n\n362398\n5887\n-0.408196\n2\n0.393577\n\n\n362399\n5887\n-0.408196\n1\n-2.099104\n\n\n\n\n362400 rows × 4 columns\n\n\n\n\n\nDo the math for a prediction\nWe already saw that for user_id=1 and item_id=3382, the predicted rating is 7.159546. Let’s do the math:\n\nuser_1_factors = user_factors_df[user_factors_df.user_id=='1'].weight.to_numpy()\nuser_1_bias = user_factors_df[user_factors_df.user_id=='1'].bias.to_numpy()[:1]\nuser_1_bias, user_1_factors[:5], user_1_factors.shape\n\n(array([0.04259326]),\n array([ 0.72214304, -1.06488835, -0.35848901, -0.08184539, -0.34282946]),\n (60,))\n\n\n\nitem_3382_factors = item_factors_df[item_factors_df.item_id==\"3382\"].weight.to_numpy()\nitem_3382_bias = item_factors_df[item_factors_df.item_id==\"3382\"].bias.to_numpy()[:1]\nitem_3382_bias, item_3382_factors[:5], item_3382_factors.shape\n\n(array([3.535388]),\n array([ 1.38777878e-17,  3.03576608e-18, -2.81892565e-18,  1.73472348e-18,\n        -5.02894452e-17]),\n (60,))\n\n\n\nimport numpy as np\nglobal_bias_np + np.dot(user_1_factors, item_3382_factors) + user_1_bias + item_3382_bias\n\narray([7.15954572])\n\n\nThere it is, same value for the predicted rating."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#full-pair-predictions",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#full-pair-predictions",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Full Pair Predictions",
    "text": "Full Pair Predictions\n\n# Change type\nuser_factors_df[['user_id', 'factor']] = user_factors_df[['user_id', 'factor']].astype('int')\n\n# Bias for each user\nuser_bias_df = user_factors_df.groupby('user_id')['bias'].max().reset_index()\n\n\n# Change type\nitem_factors_df[['item_id', 'factor']] = item_factors_df[['item_id', 'factor']].astype('int')\n\n# Bias for each movie\nitem_bias_df = item_factors_df.groupby('item_id')['bias'].max().reset_index()\n\n\nConvert to numpy\n\n# bias\nuser_bias_np = user_bias_df.sort_values(\"user_id\", ascending=True).bias.to_numpy()\n# embeddings\nuser_factors_np = user_factors_df.sort_values(\n    [\"user_id\", \"factor\"], ascending=[True, False]\n).weight.to_numpy().reshape((6_040, 60))\n\nuser_bias_np.shape, user_factors_np.shape\n\n((6040,), (6040, 60))\n\n\n\n# bias\nitem_bias_np = item_bias_df.sort_values(\"item_id\", ascending=True).bias.to_numpy()\n# embeddings\nitem_factors_np = item_factors_df.sort_values(\n    [\"item_id\", \"factor\"], ascending=[True, False]\n).weight.to_numpy().reshape((3_706, 60))\n\nitem_bias_np.shape, item_factors_np.shape\n\n((3706,), (3706, 60))\n\n\n\n\nMap user_id and item_id to numpy indices\n\nidx2item_id = {idx: item_id for idx, item_id in zip(item_bias_df.index, item_bias_df.item_id)}\n\n\n\nUser Item interaction\n\nuser_items_interactions = global_bias_np + np.dot(\n    user_factors_np, item_factors_np.T\n) + user_bias_np[:,None] + item_bias_np[None,:] \n\nuser_items_interactions.shape\n\n(6040, 3706)\n\n\nSo, user_id=1 is the index 0 for the numpy array.\n\ntop_10_1 = user_items_interactions[0].argsort()[-10:][::-1]\n[idx2item_id[idx] for idx in top_10_1]\n\n[3382, 3920, 2178, 662, 469, 199, 215, 2073, 881, 3132]\n\n\n\nendpoint_predictions_df.head(10)\n\n\n\n\n\n\n\n\npredicted_rating\npredicted_item_id\n\n\n\n\n0\n7.159546\n3382\n\n\n1\n6.931200\n3920\n\n\n2\n6.748711\n2178\n\n\n3\n6.609937\n662\n\n\n4\n6.336838\n469\n\n\n5\n6.316229\n199\n\n\n6\n6.203409\n215\n\n\n7\n6.190802\n2073\n\n\n8\n6.095066\n881\n\n\n9\n6.074839\n3132\n\n\n\n\n\n\n\nSame Top item_id from the endpoint. We are doing fine.\n\n\nTop 10 predictions for each pair\n\ntop_10_idxs = np.argsort(user_items_interactions, axis=1)[:,::-1][:,:10]\ntop_10_idxs\n\narray([[3152, 3673, 1997, ..., 1892,  823, 2916],\n       [3258, 3191, 2758, ...,  447, 2719, 1917],\n       [1763,  668,  984, ..., 3152, 1917, 1860],\n       ...,\n       [3152, 3504, 2122, ..., 2232, 1470,  543],\n       [ 986, 2964,  856, ...,  616, 2118, 2684],\n       [3237, 1298, 3579, ..., 2925, 2732, 1451]])\n\n\nTop 10 ratings for user_id=1\n\nuser_items_interactions[0][top_10_idxs[0]]\n\narray([7.15954572, 6.93120045, 6.74871055, 6.60993714, 6.33683759,\n       6.31622919, 6.20340873, 6.19080189, 6.09506552, 6.07483853])\n\n\n\ndf = pd.DataFrame(top_10_idxs[0])\ndf.columns = ['movie_id']\ndf.movie_id = df.movie_id.map(idx2item_id)\n\ndf = df.merge(movies_df)\ndf['predicted_rating'] = user_items_interactions[0][top_10_idxs[0]]\ndf\n\n\n\n\n\n\n\n\nmovie_id\nmovie_title\ngenre\npredicted_rating\n\n\n\n\n0\n3382\nSong of Freedom (1936)\nDrama\n7.159546\n\n\n1\n3920\nFaraway, So Close (In Weiter Ferne, So Nah!) (...\nDrama|Fantasy\n6.931200\n\n\n2\n2178\nFrenzy (1972)\nThriller\n6.748711\n\n\n3\n662\nFear (1996)\nThriller\n6.609937\n\n\n4\n469\nHouse of the Spirits, The (1993)\nDrama|Romance\n6.336838\n\n\n5\n199\nUmbrellas of Cherbourg, The (Parapluies de Che...\nDrama|Musical\n6.316229\n\n\n6\n215\nBefore Sunrise (1995)\nDrama|Romance\n6.203409\n\n\n7\n2073\nFandango (1985)\nComedy\n6.190802\n\n\n8\n881\nFirst Kid (1996)\nChildren's|Comedy\n6.095066\n\n\n9\n3132\nDaddy Long Legs (1919)\nComedy\n6.074839"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#getting-top-movies-for-each-user-from-bigquery",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys__part2.html#getting-top-movies-for-each-user-from-bigquery",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 2",
    "section": "Getting Top Movies for each User from BigQuery",
    "text": "Getting Top Movies for each User from BigQuery\nLet’s create a table containing the Top 5 movies by user.\n\nquery = f'''SELECT\n  user_id,\n  ARRAY_AGG(STRUCT(movie_title, genre, predicted_rating)\nORDER BY predicted_rating DESC LIMIT 5)\nFROM (\nSELECT\n  user_id,\n  item_id,\n  predicted_rating,\n  movie_title,\n  genre\nFROM\n  {MODEL_DATASET}.recommend_m2_1m\nJOIN\n  movielens.movie_titles\nON\n  item_id = movie_id)\nGROUP BY\n  user_id\n'''\njob_config = bigquery.QueryJobConfig()\njob_config.destination = \"{PROJECT_ID}.{MODEL_DATASET}.recommend_m2_1m_top\"\n\nquery_job = client.query(query, job_config)\nresults = query_job.result()\n\n\nitems_for_user_1_df = client.query(f\"\"\"\n    SELECT * \n    FROM `{PROJECT_ID}.{MODEL_DATASET}.recommend_m2_1m_top` \n    WHERE user_id=1\n\"\"\").to_dataframe()\n\nitems_for_user_1_df.f0_.loc[0]\n\narray([{'movie_title': 'Song of Freedom (1936)', 'genre': 'Drama', 'predicted_rating': 7.1595457186221525},\n       {'movie_title': 'Faraway, So Close (In Weiter Ferne, So Nah!) (1993)', 'genre': 'Drama|Fantasy', 'predicted_rating': 6.931200454403858},\n       {'movie_title': 'Frenzy (1972)', 'genre': 'Thriller', 'predicted_rating': 6.748710552254239},\n       {'movie_title': 'Fear (1996)', 'genre': 'Thriller', 'predicted_rating': 6.609937143182611},\n       {'movie_title': 'House of the Spirits, The (1993)', 'genre': 'Drama|Romance', 'predicted_rating': 6.336837592527967}],\n      dtype=object)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "",
    "text": "This is the third part of a series of posts dedicated to image classification of components that are pats of telecommunication structures.\nIn Part 1 we used all the magic of fastai directly, blindly, as was introduced in the firsts lessons of Practical Deep Learning for Coders:\n\nTelecommunication towers component clasification - Part 1. fastai\n\nIn Part 2 the idea was to apply the lessons from the course Walk with fastai, the missing pieces for success. It meant to get rid of the fastai magic except for the training, and use raw PyTorch for the dataset and dataloader creation and model setup.\n\nTelecommunication towers component clasification. Part 2. PyTorch\n\nHere we are going to use miniai, which is a simple and flexible framework that is being developed in Part 2 of Practical Deep Learning for Coders 2022. To install the framework go to: https://github.com/fastai/course22p2."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#introduction",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#introduction",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "",
    "text": "This is the third part of a series of posts dedicated to image classification of components that are pats of telecommunication structures.\nIn Part 1 we used all the magic of fastai directly, blindly, as was introduced in the firsts lessons of Practical Deep Learning for Coders:\n\nTelecommunication towers component clasification - Part 1. fastai\n\nIn Part 2 the idea was to apply the lessons from the course Walk with fastai, the missing pieces for success. It meant to get rid of the fastai magic except for the training, and use raw PyTorch for the dataset and dataloader creation and model setup.\n\nTelecommunication towers component clasification. Part 2. PyTorch\n\nHere we are going to use miniai, which is a simple and flexible framework that is being developed in Part 2 of Practical Deep Learning for Coders 2022. To install the framework go to: https://github.com/fastai/course22p2."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#import-libraries",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom torch import nn\nimport torch\nimport torchvision.transforms as tvtfms\n\nfrom PIL import Image\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#image-data",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#image-data",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Image Data",
    "text": "Image Data\nThere are 514 images (training + validation) in 8 relatively “easy” to distinguish categories (components). Plus there are 20 images for testing.\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nYou can download the pictures here.\nThere are two folders, one for the training (train) and the other for validation set (valid).\n\npath = Path(\"photos\")\n[folder.stem for folder in path.iterdir()]\n\n['test', 'train', 'valid']\n\n\n\ntrain_path = path / \"train\"\nvalid_path = path / \"valid\"\ntest_path = path / \"test\"\n\nAnd in each folder there is one folder for each label.\n\nlabels = [folder.stem for folder in train_path.iterdir()]\nnumber_of_labels = len(labels)\nprint(labels)\n\n['base_plate', 'grounding_bar', 'identification', 'ladder', 'light', 'lightning_rod', 'platform', 'transmission_lines']\n\n\n\nint_to_label = {k:v for k,v in enumerate(labels)}\nlabel_to_int = {k:v for v,k in int_to_label.items()}\nprint(label_to_int)\n\n{'base_plate': 0, 'grounding_bar': 1, 'identification': 2, 'ladder': 3, 'light': 4, 'lightning_rod': 5, 'platform': 6, 'transmission_lines': 7}"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#creating-pytorch-dataset",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#creating-pytorch-dataset",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Creating PyTorch Dataset",
    "text": "Creating PyTorch Dataset\n\nDataset Class\nThis class is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code.\n\nThis example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\n\nclass TowerPartsDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential, label_to_int:dict):\n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]\n        self.to_tensor = tvtfms.ToTensor()\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return label_to_int[label]\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\n\nItem Transforms\n\nfrom fastai.vision.data import imagenet_stats\nprint(imagenet_stats)\n\nitem_tfms = nn.Sequential(\n    tvtfms.Resize((224, 224)), \n    tvtfms.Normalize(*imagenet_stats)\n)\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nitem_tfms[0](Image.open(train_path / 'base_plate/CIMG4695.jpg'))\n\n\n\n\n\n\nTrain and validation datasets\n\ntrain_dataset = TowerPartsDataset(\n    train_path,\n    item_tfms,\n    label_to_int\n)\n\nvalid_dataset = TowerPartsDataset(\n    valid_path,\n    item_tfms,\n    label_to_int\n)\n\n\nx, y = train_dataset[0]\nx.shape, y\n\n(torch.Size([3, 224, 224]), 0)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#dataloader",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#dataloader",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "DataLoader",
    "text": "DataLoader\n\nbatch_size = 64\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=True,\n    batch_size=batch_size\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size * 2\n)\n\n\nminiai Dataloaders\n\nfrom miniai.datasets import DataLoaders, show_images\n\n\ndls = DataLoaders(train_dataloader, valid_dataloader)\n\n\ndt = dls.train\nxb, yb = next(iter(dt))\nxb.shape, yb[:10]\n\n(torch.Size([64, 3, 224, 224]), tensor([6, 7, 0, 0, 2, 6, 6, 6, 3, 5]))\n\n\n\n%%time\nfor _ in train_dataloader: pass\n\nCPU times: user 6min 35s, sys: 49.7 s, total: 7min 25s\nWall time: 1min 25s\n\n\n\nfrom operator import itemgetter\n\n# To avoid warning of clipping input data, a sigmoid is applied\nxbt = xb[:16].sigmoid()\nybt = yb[:16]\ntitles = itemgetter(*ybt.tolist())(int_to_label)\nshow_images(xbt, imsize=2.25, titles=titles)\n\n\n\n\nAs we can see, there are some pictures that have at least two components that could be classified as for our labels. So maybe the correct approach would be a multi-label classification."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#customizing-a-pytorch-model",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#customizing-a-pytorch-model",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Customizing a PyTorch Model",
    "text": "Customizing a PyTorch Model\nWhen loading a model to fastai learner, it is customized by changing the last two children (the Head).\nWhat we are going to do here is to change only what is essential for our problem, that is, the final linear layer in order to have 8 features (the number of labels need to classify).\nFor more detail on this I encourage you to take Walk with fastai, the missing pieces for success.\nUnlike Part 1 and Part 2, here we are going to try with resnet18.\n\nfrom torchvision.models import resnet18\nmodel = resnet18(pretrained=True)\n\n\nCustomizing the last linear layer\nWe need an output size equal to the number of labels we are trying to predict.\n\nmodel_child = list(model.children())\nmodel_child[-2:]\n\n[AdaptiveAvgPool2d(output_size=(1, 1)),\n Linear(in_features=512, out_features=1000, bias=True)]\n\n\nWe can access the original final layer with model.fc\n\nmodel.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\nIt has the 1,000 features ResNet was trained for. We need to change it to the 8 features (labels, classes) of the problem we are dealing here:\n\nmodel.fc = nn.Linear(512, out_features=number_of_labels, bias=True)\n\n\nmodel.fc\n\nLinear(in_features=512, out_features=8, bias=True)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#callbacks",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#callbacks",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Callbacks",
    "text": "Callbacks\nFor more info about miniai and its callback system:\n\nFastai Course Part 2 2022: Understanding CallBacks by Francesco Pochetti\nRedesign your Training Loop with CallBacks by Dien-Hoa Truong\n\n\nfrom miniai.learner import MetricsCB, DeviceCB, ProgressCB, TrainLearner\nfrom torcheval.metrics import MulticlassAccuracy\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#the-optimizer",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#the-optimizer",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "The Optimizer",
    "text": "The Optimizer\nLet’s use the same optimizer as fastai’s default: AdamW\n\nfrom miniai.activations import set_seed\nfrom miniai.sgd import BatchSchedCB, RecorderCB\n\nfrom torch.optim import AdamW\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\n\n\nRecord the scheduler’s parameters\n\ndef _lr(cb): return cb.pg['lr']\ndef _beta1(cb): return cb.pg['betas'][0]\n\nrec = RecorderCB(lr=_lr, mom=_beta1)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#training-with-miniai",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#training-with-miniai",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Training with miniai",
    "text": "Training with miniai\n\nset_seed(42)\nlr, epochs = 0.001, 4\n\n\nCustom training loop\nminiai callbacks allows us to easily modify the training loop. In this case we are going to store the validation loss for each sample of the validation dataset (without reducing it to a single value). Since the validation set has 120 pictures, and the validation batch size is 64 x 2, by storing the last loss as valid_loss is enough to then use those values and plot the top losses as we do with fastai.\nPredictions are stored in learn.preds as set in miniai TrainLearner.\n\nclass TrainValidLossTrack(TrainLearner):\n    def get_loss(self): \n        self.loss = self.loss_func(self.preds, self.batch[1])\n        # Store loss without reduction in the Learner\n        self.valid_loss = self.loss_func(self.preds, self.batch[1], reduction=\"none\")\n\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\nxtra = [BatchSchedCB(sched), rec]\nlearn = TrainValidLossTrack(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n\naccuracy\nloss\nepoch\ntrain\n\n\n\n\n0.401\n1.696\n0\ntrain\n\n\n0.850\n0.442\n0\neval\n\n\n0.977\n0.103\n1\ntrain\n\n\n0.842\n0.638\n1\neval\n\n\n1.000\n0.013\n2\ntrain\n\n\n0.875\n0.621\n2\neval\n\n\n1.000\n0.007\n3\ntrain\n\n\n0.925\n0.326\n3\neval\n\n\n\n\n\n\n\n\nGood news, it trained better than (I) expected! It looks like it overfit a little, although we only trained for 4 epochs and a smaller model (resnet18) vs Part 1 & 2 (resnet34).\n\n\nHow the the parameters were changed by the scheduler\n\nimport matplotlib.pyplot as plt\nrec.plot()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#classification-interpretation",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#classification-interpretation",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Classification Interpretation",
    "text": "Classification Interpretation\n\nlearn.preds.shape, learn.valid_loss.shape\n\n(torch.Size([120, 8]), torch.Size([120]))\n\n\n\nactuals = [label_to_int[l.parts[-2]] for l in valid_dataset.paths]\n\n\nConfusion Matrix\n\nconfusion_matrix = [[0]*number_of_labels for n in range(number_of_labels)]\nact_pred = list(zip(actuals, learn.preds.argmax(1).tolist()))\n\nfor act, pred in act_pred:\n    confusion_matrix[act][pred] += 1\n\n\nx = y = [x for x in labels]\n\nfig = px.imshow(confusion_matrix, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"# Pics\"),\n                text_auto=True, x=x, y=x,\n                color_continuous_scale='blues', title='Confusion Matrix')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot Top Losses\n\nsorted_losses_idcs = learn.valid_loss.argsort(descending=True).cpu()\n\n\ndv = dls.valid\nxb, yb = next(iter(dv))\nxbv = xb[sorted_losses_idcs[:12]].sigmoid()\n\n\nact_pred_plot = torch.tensor(act_pred)[sorted_losses_idcs[:12]].tolist()\n\n\ntitles = [f\"{int_to_label[act]} / \\n {int_to_label[pred]}\" for act, pred in act_pred_plot][:12]\n\n\nshow_images(xbv, imsize=3.0, title='Actual / Predicted', titles=titles)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#save-the-model",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#save-the-model",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Save the model",
    "text": "Save the model\n\nmdl_path = Path('models')\ntorch.save(learn.model,  mdl_path/'exported_resnet18_pytorch-miniai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#gpu-inference",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#gpu-inference",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "GPU Inference",
    "text": "GPU Inference\n\nloaded_model = torch.load(mdl_path/'exported_resnet18_pytorch-miniai.pth')\n\n\nmodel evaluation\n\nRemember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models\n\nloaded_model.eval();\n\n\n\nTest dataset\n\ntest_files = list(test_path.iterdir())\nprint(test_files)\n\n[Path('photos/test/archipielago-los-roques-078.jpg'), Path('photos/test/archipielago-los-roques-313.jpg'), Path('photos/test/cantv-mata-palo-350.jpg'), Path('photos/test/CIMG8119.jpg'), Path('photos/test/DSC00191.jpg'), Path('photos/test/DSC01399.jpg'), Path('photos/test/DSC01537.jpg'), Path('photos/test/DSC01628.jpg'), Path('photos/test/DSC01657.jpg'), Path('photos/test/DSC01723.jpg'), Path('photos/test/DSC01734.jpg'), Path('photos/test/DSC01955.jpg'), Path('photos/test/DSC01956.jpg'), Path('photos/test/DSC04892.jpg'), Path('photos/test/DSC05105.jpg'), Path('photos/test/DSC05130.jpg'), Path('photos/test/DSC09446.jpg'), Path('photos/test/DSC09524.jpg'), Path('photos/test/DSC09685.jpg'), Path('photos/test/DSC09908.jpg')]\n\n\n\nshow_images([item_tfms[0](Image.open(f)) for f in test_files], imsize=2.5)\n\n\n\n\n\nto_tensor = tvtfms.ToTensor()\n\ndef get_predictions(im_files:list, model:\"torchvision.models\"):\n    tensor_images = []\n    for file in im_files:\n        tensor_images.append(item_tfms(to_tensor(Image.open(file)).cuda()))\n        \n    return model(torch.stack(tensor_images)).argmax(1)\n\nprint([int_to_label[pred.item()] for pred in get_predictions(test_files, loaded_model)])\n\n['grounding_bar', 'ladder', 'light', 'platform', 'base_plate', 'light', 'platform', 'grounding_bar', 'base_plate', 'base_plate', 'ladder', 'light', 'lightning_rod', 'ladder', 'identification', 'ladder', 'lightning_rod', 'ladder', 'light', 'identification']\n\n\nAlmost all the images in the test set are predicted correctly. Thats ok for the purpuse of this post. But as said before, probably the approach would be to do a multi-label classification."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#conclusions",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Conclusions",
    "text": "Conclusions\n\nIt was great to being able to train with more than 90% accuracy using PyTorch and miniai, starting with a pretrained resnet18 model.\nWe implemented Confusion Matrix and Plot Top Losses from scratch, techniques available in fastai’s Classification Interpretation.\nWe also did batch inference in the GPU."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "",
    "text": "The first attempt for developing this idea started as the homework assignment for lesson 1: Deep Learning 2019 (v3) fast.ai course.\nFor that homework I toke my domain expertise on telecommunication towers to build an image clasifier which could hopefuly recognize different tower components.\nIn may 2022 I was coursing live the 2022 version of the course, which is done in collaboration with The University of Queensland and its now called Practical Deep Learning for Coders.\nI’m running this notebook on an old GTX-1070 NVIDIA GPU."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#introduction",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#introduction",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "",
    "text": "The first attempt for developing this idea started as the homework assignment for lesson 1: Deep Learning 2019 (v3) fast.ai course.\nFor that homework I toke my domain expertise on telecommunication towers to build an image clasifier which could hopefuly recognize different tower components.\nIn may 2022 I was coursing live the 2022 version of the course, which is done in collaboration with The University of Queensland and its now called Practical Deep Learning for Coders.\nI’m running this notebook on an old GTX-1070 NVIDIA GPU."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#import-libraries",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom fastai.vision.all import *\nimport timm\nimport plotly.express as px"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataset",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataset",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Dataset",
    "text": "Dataset\nI curated an image dataset of multiple towers I worked with in the past several years. And choosed 514 images in 8 relatively “easy” to distinguish categories (components).\nThe dataset was stored in google drive and is shared here.\n\nLocal Dataset\nFor the 2022 course, thanks to the help of many great people in fastai forums, I was able to install fastai locally and to use my local GPU on WSL2.\n\npath = Path(\"photos\")\n\n\ntrain_path = path / 'train'\nvalid_path = path / 'valid'\n\nlabels = [label.parts[-1] for label in train_path.iterdir()]\ntrain_quantity = [len(list(each.iterdir())) for each in train_path.iterdir()]\nvalid_quantity = [len(list(each.iterdir())) for each in valid_path.iterdir()]\ndf = pd.DataFrame()\n\ndf['label'] = labels * 2\ndf['set'] = ['train'] * 8 + ['valid'] * 8\ndf['quantity'] = train_quantity + valid_quantity\n\n\ndf\n\n\n\n\n\n\n\n\nlabel\nset\nquantity\n\n\n\n\n0\nbase_plate\ntrain\n87\n\n\n1\ngrounding_bar\ntrain\n52\n\n\n2\nidentification\ntrain\n30\n\n\n3\nladder\ntrain\n37\n\n\n4\nlight\ntrain\n69\n\n\n5\nlightning_rod\ntrain\n33\n\n\n6\nplatform\ntrain\n57\n\n\n7\ntransmission_lines\ntrain\n29\n\n\n8\nbase_plate\nvalid\n29\n\n\n9\ngrounding_bar\nvalid\n15\n\n\n10\nidentification\nvalid\n8\n\n\n11\nladder\nvalid\n11\n\n\n12\nlight\nvalid\n22\n\n\n13\nlightning_rod\nvalid\n10\n\n\n14\nplatform\nvalid\n16\n\n\n15\ntransmission_lines\nvalid\n9\n\n\n\n\n\n\n\n\nis_train = df.set == 'train'\ndf[is_train].quantity.sum(), df[~is_train].quantity.sum()\n\n(394, 120)\n\n\n\nfig = px.bar(\n    df, x=\"set\", y=\"quantity\",\n    color='label', barmode='group',\n    height=400\n)\nfig.show()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#the-data",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#the-data",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "The Data",
    "text": "The Data\nThe data was hand picked from a huge tower photoset. To start, I choose these eight easy distinguishable components to be clasified:\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nThere are two folders, one for the training (train) and the other for the validation set (valid).\n\nprint(path.ls())   \nprint('*'*100)\n(path/'train').ls()\n\n[Path('photos/train'), Path('photos/valid')]\n****************************************************************************************************\n\n\n(#8) [Path('photos/train/base_plate'),Path('photos/train/grounding_bar'),Path('photos/train/identification'),Path('photos/train/ladder'),Path('photos/train/light'),Path('photos/train/lightning_rod'),Path('photos/train/platform'),Path('photos/train/transmission_lines')]\n\n\n\ntower_parts_fns = get_image_files(path)\ntower_parts_fns\n\n(#514) [Path('photos/train/base_plate/Ac102-Corozopando-(64).jpg'),Path('photos/train/base_plate/Ac102-Corozopando-(75).jpg'),Path('photos/train/base_plate/camaguan-087.jpg'),Path('photos/train/base_plate/camaguan-098.jpg'),Path('photos/train/base_plate/cantv el yoco 015.JPG'),Path('photos/train/base_plate/cantv-capanaparo-011.jpg'),Path('photos/train/base_plate/cantv-cinaruco-018.jpg'),Path('photos/train/base_plate/cantv-cinaruco-025.jpg'),Path('photos/train/base_plate/cartanal-(7).jpg'),Path('photos/train/base_plate/CHUSPITA-II-AC-72-MTS-002.jpg')...]\n\n\n\nfailed = verify_images(tower_parts_fns)\nprint(failed)\n\n[]"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataloaders",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataloaders",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "DataLoaders",
    "text": "DataLoaders\n\ntower_parts = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='train', valid_name='valid'),\n    get_y=parent_label,\n    item_tfms=Resize(224)\n)\n\n\ndls = tower_parts.dataloaders(path)\n\n\n%%time\nfor _ in dls.train: pass\n\nCPU times: user 449 ms, sys: 506 ms, total: 955 ms\nWall time: 20.4 s\n\n\n\ndls.train.show_batch(max_n=16, nrows=4, figsize=(10,10))"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#learner",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#learner",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Learner",
    "text": "Learner\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n\nfastai’s lr_find\n\n%%time\nlearn.lr_find()\n\n\n\n\n\n\n\n\nCPU times: user 24.4 s, sys: 8.72 s, total: 33.2 s\nWall time: 5min 38s\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\ndls.num_workers\n\n1\n\n\n\n\n\n\n\n\nNote\n\n\n\nI experimented by setting dls.num_workers = 4 and it didn’t make any difference in the time it takes to run lr_find(), even though that, by watching at the progress bar, it seemed that the bottleneck was in pre-processing the batch. Not in GPU.\n\n\n\nlen(dls.train), len(dls.train.get_idxs())\n\n(6, 394)\n\n\n\nlen(dls.valid), len(dls.valid.get_idxs())\n\n(2, 120)\n\n\n\ndls.drop_last\n\nTrue"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#training",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#training",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Training",
    "text": "Training\n\n%%time\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.751579\n0.932106\n0.316667\n00:44\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.000472\n0.463910\n0.166667\n00:43\n\n\n1\n0.632878\n0.272529\n0.091667\n00:43\n\n\n2\n0.429861\n0.208375\n0.050000\n00:43\n\n\n\n\n\nCPU times: user 8.31 s, sys: 4.01 s, total: 12.3 s\nWall time: 2min 55s\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(8,8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9)\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.export('models/tower_parts_model')\nlearn.save(\"exported_model_from_fastai\", with_opt=False)\n\nPath('models/exported_model_from_fastai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#conclusions",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Conclusions",
    "text": "Conclusions\n\nI took about 500 pictures and 3 epochs to finetune a small pre-trained model to make it recognize 8 components with an error rate of about 6%.\nlearn.lr_find() took about 5 minutes to run, more that the fine tuning.\nThere are some categories that normally appear in one picture at the same time. I solved a classification problem to simplify, but maybe the actual problem should be a multi-class classificacion."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Francisco Mussari, blogging from Caracas about data, ai, cloud computing, engineering and social sciences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datamatica",
    "section": "",
    "text": "Collaborative Filtering with PyTorch and miniai - MovieLens 20M\n\n\n\n\n\n\n\nMovielens\n\n\nminiai\n\n\nPyTorch\n\n\nRecSys\n\n\nRecommender\n\n\nMovielens 20M\n\n\nt-SNE\n\n\nEmbeddings\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nRecSys MLOps. BigQuery ML and Vertex AI. Part 2\n\n\n\n\n\n\n\nMovielens\n\n\nBigQuery\n\n\nBigQuery ML\n\n\nRecSys\n\n\nRecommender\n\n\nPipeline\n\n\nVertex AI\n\n\nMLOps\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nRecSys MLOps. BigQuery ML and Vertex AI. Part 1\n\n\n\n\n\n\n\nMovielens\n\n\nBigQuery\n\n\nBigQuery ML\n\n\nRecSys\n\n\nRecommender\n\n\nPipeline\n\n\nVertex AI\n\n\nMLOps\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nDimensional Modelling with dbt and different flavors of PostgreSQL\n\n\n\n\n\n\n\ndbt\n\n\nData-Engineer-Camp\n\n\nDatawarehousing\n\n\nKimball\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecommunication towers component clasification. Part 3. PyTorch and miniai\n\n\n\n\n\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\nfastai\n\n\nminiai\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecommunication towers component clasification. Part 2. PyTorch\n\n\n\n\n\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecommunication towers component clasification - Part 1. fastai\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nWalk with fastai\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nExploring fastai API levels to create DataLoaders\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Full-Text Search Engine for fastai YouTube channel\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\nStreamlit\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Full-Text Search Engine for fastai YouTube channel\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\npytube\n\n\nyoutube-transcript-api\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecom Equipment Detection with Azure Custom Vision (Free) - Part 3\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\ngradio\n\n\nhugging face\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecom Equipment Detection with Azure Custom Vision (Free) - Part 2\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\nTelecom Equipment Detection with Azure Custom Vision (Free) - Part 1\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "",
    "text": "Zach Mueller proposed us to create a Gradio app as a homework for for the course Walk with fastai, the missing pieces for success.\nI had already deployed a Tower Classificacion App as a homework for Practical Deep Learning for Coders.\nHere the link to Part 1 post:\nTelecommunication towers component clasification - Part 1. fastai\nThe new thing here is that everythong but the training is going to be done using pure PyTorch, which was teched in Zach’s course."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#introduction",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#introduction",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "",
    "text": "Zach Mueller proposed us to create a Gradio app as a homework for for the course Walk with fastai, the missing pieces for success.\nI had already deployed a Tower Classificacion App as a homework for Practical Deep Learning for Coders.\nHere the link to Part 1 post:\nTelecommunication towers component clasification - Part 1. fastai\nThe new thing here is that everythong but the training is going to be done using pure PyTorch, which was teched in Zach’s course."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#import-libraries",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom pathlib import Path\nfrom miniai.datasets import show_images\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport torch\nimport torchvision.transforms as tvtfms\n\nfrom PIL import Image\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#image-data",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#image-data",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Image Data",
    "text": "Image Data\nThere are 514 images in 8 relatively “easy” to distinguish categories (components).\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nYou can download the pictures here.\nThere are two folders, one for the training (train) and the other for validation set (valid).\n\npath = Path(\"photos\")\n[folder.stem for folder in path.iterdir()]\n\n['test', 'train', 'valid']\n\n\n\ntrain_path = path / \"train\"\nvalid_path = path / \"valid\"\n\nAnd in each folder there is one folder for each label.\n\nlabels = [folder.stem for folder in train_path.iterdir()]\nnumber_of_labels = len(labels)\nprint(labels)\n\n['base_plate', 'grounding_bar', 'identification', 'ladder', 'light', 'lightning_rod', 'platform', 'transmission_lines']\n\n\n\nint_to_label = {k:v for k,v in enumerate(labels)}\nlabel_to_int = {k:v for v,k in int_to_label.items()}\nprint(label_to_int)\n\n{'base_plate': 0, 'grounding_bar': 1, 'identification': 2, 'ladder': 3, 'light': 4, 'lightning_rod': 5, 'platform': 6, 'transmission_lines': 7}\n\n\n\nVisualizing the distribution of labels\n\nlabels = [label.parts[-1] for label in train_path.iterdir()]\ntrain_quantity = [len(list(each.iterdir())) for each in train_path.iterdir()]\nvalid_quantity = [len(list(each.iterdir())) for each in valid_path.iterdir()]\ndf = pd.DataFrame()\n\ndf['label'] = labels * 2\ndf['set'] = ['train'] * 8 + ['valid'] * 8\ndf['number of pics'] = train_quantity + valid_quantity\n\n\nfig = px.bar(\n    df, x=\"set\", y=\"number of pics\",\n    color='label', barmode='group',\n    height=400\n)\nfig.show()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-pytorch-dataset",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-pytorch-dataset",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Creating PyTorch Dataset",
    "text": "Creating PyTorch Dataset\n\nDataset Class\nThis class is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code.\n\nThis example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\n\nclass TowerPartsDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential, label_to_int:dict):\n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]\n        self.to_tensor = tvtfms.ToTensor()\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return label_to_int[label]\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\n\nItem Transforms\n\nfrom fastai.vision.data import imagenet_stats\nprint(imagenet_stats)\n\nitem_tfms = nn.Sequential(\n    tvtfms.Resize((224, 224)), \n    tvtfms.Normalize(*imagenet_stats)\n)\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nitem_tfms[0](Image.open(train_path / 'base_plate/CIMG4695.jpg'))\n\n\n\n\n\n\nTrain and validation datasets\n\ntrain_dataset = TowerPartsDataset(\n    train_path,\n    item_tfms,\n    label_to_int\n)\n\nvalid_dataset = TowerPartsDataset(\n    valid_path,\n    item_tfms,\n    label_to_int\n)\n\n\nx, y = train_dataset[0]\nx.shape, y\n\n(torch.Size([3, 224, 224]), 0)\n\n\n\n\nPreviewing transformed images\nThe warning occurs because the images has been normalized.\n\nfigure = plt.figure(figsize=(10, 10))\ncols, rows = 3, 3\n\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    label = int_to_label[label]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.permute(1, 2, 0))\n    \nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#dataloader",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#dataloader",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "DataLoader",
    "text": "DataLoader\n\nbatch_size = 64\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=True,\n    batch_size=batch_size\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size\n)\n\n\n%%time\n# This should be the PyTorch dataloaders\nfor _ in train_dataloader: pass\n\nCPU times: user 6min 31s, sys: 38 s, total: 7min 9s\nWall time: 1min 22s"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-a-pytorch-model",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-a-pytorch-model",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Creating a PyTorch Model",
    "text": "Creating a PyTorch Model\nWhen loading a model to fastai learner, it is customized by changing the last two children (the Head).\nWhat we are going to do here is to change only what is essential for our problem, that is, the final linear layer in order to have 8 features (the number of labels need to classify).\nFor more detail on this I encourage you to take Walk with fastai, the missing pieces for success\n\nfrom torchvision.models import resnet18, resnet34\nmodel = resnet34(pretrained=True)\n\n\nCustomizing the last linear layer\nThese are the two layers that are changed by fastai when creating the learner:\n\nmodel_child = list(model.children())\nmodel_child[-2:]\n\n[AdaptiveAvgPool2d(output_size=(1, 1)),\n Linear(in_features=512, out_features=1000, bias=True)]\n\n\nWe can access the original final layer with model.fc\n\nmodel.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\nIt has the 1,000 features resnet was trained for. We need to change it to the 8 features (labels, classes) of the problem we are dealing here:\n\nmodel.fc = nn.Linear(512, out_features=number_of_labels, bias=True)\n\n\nmodel.fc\n\nLinear(in_features=512, out_features=8, bias=True)\n\n\n\n\nGradual Unfreezing\nOne other thing fastai does is it freeze the backbone (Body) of the model. We can achieve that as follows:\n\n\"\"\"for layer in list(model.children())[:-1]:\n    if hasattr(layer, \"requires_grad_\"):\n        layer.requires_grad_(False)\"\"\";\n\n\n\n\n\n\n\nNote\n\n\n\nZach used gradual unfreezing and fit_one_cycle in Walk with fastai, the missing pieces for success. Here I’m going to use fine_tune instead to compare with results in Part 1."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#the-optimizer",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#the-optimizer",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "The Optimizer",
    "text": "The Optimizer\nLet’s use the same optimizer as fastai’s default: AdamW\n\nfrom torch.optim import AdamW\nfrom functools import partial\nfrom fastai.optimizer import OptimWrapper\n\n\nopt_func = partial(OptimWrapper, opt=AdamW)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#bringing-in-fastai-and-training",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#bringing-in-fastai-and-training",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Bringing in fastai and Training!",
    "text": "Bringing in fastai and Training!\n\nfrom fastai.losses import CrossEntropyLossFlat\nfrom fastai.metrics import accuracy, error_rate\nfrom fastai.learner import Learner\nfrom fastai.callback.schedule import Learner # To get `fit_one_cycle`, `lr_find`, and more\n\nfrom fastai.data.core import DataLoaders\n\n\nmodel.cuda();\n\n\ndls = DataLoaders(train_dataloader, valid_dataloader)\n\n\nlearn = Learner(\n    dls, \n    model, \n    opt_func=opt_func, \n    loss_func=CrossEntropyLossFlat(), \n    metrics=error_rate\n)\n\n\n%%time\nlearn.lr_find()\n\n\n\n\n\n\n\n\nCPU times: user 1h 46min 47s, sys: 12min 46s, total: 1h 59min 33s\nWall time: 22min 55s\n\n\nSuggestedLRs(valley=5.248074739938602e-05)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Part 1 it took 5 minutes for lr_find() to run, and the suggested lr was about 0.001. Now, with PyTorch’s Dataloaders things got very different. Should keep digging in the causes.\n\n\n\nFine Tuning\n\nfine_tuning is geared towards transfer learning specifically. “fine_tune” vs. “fit_one_cycle”\n\nfine_tune does one epoch to train only the last layer (the linear layer we just modified). It means that the parameter of all the other layers are not changed, are “freezed”. But after that epoch, all the model is “unfreezed” and trained. So the parameters of all its layers are updated or optmized.\n\n%%time\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.041510\n1.549582\n0.283333\n02:02\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.095129\n1.055718\n0.208333\n01:59\n\n\n1\n0.171666\n2.795403\n0.300000\n02:02\n\n\n2\n0.146215\n0.879028\n0.166667\n02:01\n\n\n\n\n\nCPU times: user 35min 42s, sys: 4min 46s, total: 40min 29s\nWall time: 8min 7s\n\n\n\nlearn.fit(2, lr=5.25e-05)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.047634\n0.423191\n0.116667\n02:00\n\n\n1\n0.033922\n0.314130\n0.100000\n01:58\n\n\n\n\n\n\nfrom fastai.interpret import ClassificationInterpretation\n\ntry:\n    interp = ClassificationInterpretation.from_learner(learn)\n    interp.plot_confusion_matrix(figsize=(8,8))\nexcept Exception as inst:\n    print(inst)\n\n'DataLoader' object has no attribute 'new'\n\n\n\nlearn.save(\"exported_model_from_pytorch-fastai\", with_opt=False)\n\nPath('models/exported_model_from_pytorch-fastai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#conclusions",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Conclusions",
    "text": "Conclusions\n\nFor some reason when starting with a PyTorch Dataloader lr_find takes more than 20 minutes to run. 4 times what it took in Part 1 where fastai’s Dataloaders were used.\nIt takes 2 minutes to train each epoch, which in Part 1 took less than a minute.\nThe error rate wasn’t that good after the 3 epochs of fine tuning. 16% vs 6% in Part 1. It took two more epochs with the learning rate suggested by lr_find to lower the error rate to 10%. Maybe the customized fastai head plays an important role here.\nfastai’s Dataloaders created from PyTorch’s Dataloader missed some attributed needed for fastai’s ClassificationInterpretation to run."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "",
    "text": "The exploration I’m documenting here about dbt (Data Build Tool) is almost a recreation of the awsome tutorial Building a Kimball dimensional model with dbt but with some twists.\nI encourage you to read that tutorial for a discussion of what a dimensional model is, its benefits, and the differences with other modeling techniques such as Third Normal Form (3NF) and One Big Table (OBT).\nWhat I did different from that tutorial was the following:\n\nWorked with the AdventureWorks data already loaded into PostgreSQL, and not seeded from .csv files. For that I managed to create a backup that you can restore to your Postgres database. You can download it here.\nExperimented with three flavors of Postgres:\n\nLocal PostgreSQL.\nSupabase PostgreSQL as-a-service (free).\nAWS Aurora Serverless for PostgreSQL.\n\nCreated custom schema names Deploy to custom schemas & override dbt defaults.\nCreated two date dimension tables, one with calogical dbt-date extension, which is a wraper around dbt_utils.date_spine, and the other with date_spine itself. I managed to create the calendar dynamically ranging from the first day in the fact table to the last one."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#overview",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#overview",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "",
    "text": "The exploration I’m documenting here about dbt (Data Build Tool) is almost a recreation of the awsome tutorial Building a Kimball dimensional model with dbt but with some twists.\nI encourage you to read that tutorial for a discussion of what a dimensional model is, its benefits, and the differences with other modeling techniques such as Third Normal Form (3NF) and One Big Table (OBT).\nWhat I did different from that tutorial was the following:\n\nWorked with the AdventureWorks data already loaded into PostgreSQL, and not seeded from .csv files. For that I managed to create a backup that you can restore to your Postgres database. You can download it here.\nExperimented with three flavors of Postgres:\n\nLocal PostgreSQL.\nSupabase PostgreSQL as-a-service (free).\nAWS Aurora Serverless for PostgreSQL.\n\nCreated custom schema names Deploy to custom schemas & override dbt defaults.\nCreated two date dimension tables, one with calogical dbt-date extension, which is a wraper around dbt_utils.date_spine, and the other with date_spine itself. I managed to create the calendar dynamically ranging from the first day in the fact table to the last one."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#setting-up-the-project",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#setting-up-the-project",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Setting up the project",
    "text": "Setting up the project\nIt is not in the scope of this document to explain how to get started with dbt. I had never used dbt and it was straight forward with these two tutorials:\n\nBuilding a Kimball dimensional model with dbt\nIntro to Data Build Tool (dbt) // Create your first project!\n\nYou can explore the whole experiment I did in this repository: fmussari/dbt-dimensional-modeling-experiment"
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#defining-the-databases-in-profiles.yml",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#defining-the-databases-in-profiles.yml",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Defining the databases in profiles.yml",
    "text": "Defining the databases in profiles.yml\ndbt is a transformation tool, so it only works with one database at a time, meaning that the same database is the source and the target. In this case we have the normalized AdventureWorks database and we are creating the transformations to create a model with dimensions and fact tables. I found interesting that multiple databases can be defined in a project, so you can test, for example, in your local database, and then create the models into the cloud database.\nFor storing the credentials I used conda, based on setting environment variables.\nThe profiles.yml ended looking like this:\n\n\nprofiles.yml\n\ndbt_dimensional_demo:\n\n  outputs:\n\n    postgresAdventure:\n      type: postgres\n      threads: 4\n      host: localhost\n      port: 5432\n      user: postgres\n      pass: '10042076'\n      database: Adventureworks\n      schema: target  # this is override in each model's sql\n\n    supabaseAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('SUPABASE_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('SUPABASE_USER') }}\"\n      pass: \"{{ env_var('SUPABASE_PASS') }}\"\n      database: Adventureworks\n      schema: t\n    \n    auroraAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('AURORA_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('AURORA_USER') }}\"\n      pass: \"{{ env_var('AURORA_PASS') }}\"\n      database: Adventureworks\n      schema: t\n\n  target: supabaseAdventure\n\nAs you can see your experiment can run in any database specified in the file, by only changing the target."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#creating-the-dimensional-model",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#creating-the-dimensional-model",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Creating the Dimensional Model",
    "text": "Creating the Dimensional Model\nThis part is almost a copy from the aforementioned tutorial. The only difference being that based on the video Deploy to custom schemas & override dbt defaults I added some lines to configure the Schema in the transformations file.\nAnother difference was that I specified my sources in in the file schema.yml that looks like this:\n\n\nschema.yml\n\n# Copied from: https://docs.getdbt.com/docs/build/sources\n\nversion: 2\n\nsources:\n  - name: pg_production\n    database: Adventureworks\n    description: 'Adventureworks, production schema'\n    schema: production\n    tables:\n      - name: product\n      - name: productcategory\n      - name: productsubcategory\n\n  - name: pg_person\n    database: Adventureworks\n    description: 'Adventureworks, person schema'\n    schema: person\n    tables:\n      - name: address\n      - name: stateprovince\n      - name: countryregion\n      - name: person\n\n  - name: pg_sales\n    database: Adventureworks\n    description: 'Adventureworks, sales schema, customer table'\n    schema: sales\n    tables:\n      - name: customer\n      - name: store\n      - name: creditcard\n      - name: salesorderheader\n      - name: salesorderdetail\n\nSo dim_address.sql had the lines on materialization and schema configuration, pointed to the sources defined in the previous .yml, and ended looking like this:\n\n\ndim_address.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\nwith stg_address as (\n    select *\n    from {{ source('pg_person', 'address') }}\n),\n\nstg_stateprovince as (\n    select *\n    from {{ source('pg_person', 'stateprovince') }}\n),\n\nstg_countryregion as (\n    select *\n    from {{ source('pg_person', 'countryregion') }}\n)\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['stg_address.addressid']) }} as address_key,\n    stg_address.addressid,\n    stg_address.city as city_name,\n    stg_stateprovince.name as state_name,\n    stg_countryregion.name as country_name\nfrom stg_address\nleft join stg_stateprovince on stg_address.stateprovinceid = stg_stateprovince.stateprovinceid\nleft join stg_countryregion on stg_stateprovince.countryregioncode = stg_countryregion.countryregioncode\n\nIn the same way as the tutorial, there were created the other dimensions:\n\ndim_credit_card.sql\ndim_customer.sql\ndim_order_status.sql\ndim_product.sql\n\n\nFact table:\n\nfact_sales.sql"
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#date-dimension",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#date-dimension",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Date Dimension",
    "text": "Date Dimension\nThis is where I did most of my own experiments. I wanted to create a Date Domension or Calendar table ranging from the earliest to the latest date in the fact table, that comes from salesorderheader. I had little to no experience at all with Jinja templates, which is what dbt relies on for transformations, so maybe there are better ways to achieve what I intented to do, but I must say it did worked fine.\n\ndbt_utils.date_spine\nThe first version of the date dimension was created using dbt_utils, based on the tutorial As of Date Tables. I simply changed start_date and end_date statements to get the aforementioned earliest and latest dates from salesorderheader. The transformation ended looking like this:\n\n\ndim_dates.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\n{%- set datepart = \"day\" -%}\n{%- set start_date = \"(select MIN(cast(orderdate as date)) from sales.salesorderheader)\" -%}\n{%- set end_date = \"(select MAX(cast(orderdate as date)) + (interval '1 day') from sales.salesorderheader)\" -%}\n\nwith as_of_date AS (\n    {{ dbt_utils.date_spine(\n        datepart=datepart, \n        start_date=start_date,\n        end_date=end_date\n       ) \n    }}\n)\n\nSELECT * FROM as_of_date\n\nThis isn’t a full calendar but a sequence of dates from earliest to latest. For a full calendar we then would need to create all the columns as year, month, day, quarter…\n\n\nStatement Blocks\nI then made some experiments with statement blocks and created one called get_dates to generate a view with the earliest and latest dates from salesorderheader:\n\n\ndim_view_date_range.sql\n\n{%- call statement('get_dates', fetch_result=True) -%}\n\n      SELECT  MIN(cast(orderdate as date)), MAX(cast(orderdate as date)) \n      FROM {{ source('pg_sales', 'salesorderheader') }}\n\n{%- endcall -%}\n\n{%- set earliest = load_result('get_dates')['data'][0][0] -%}\n{%- set latest = load_result('get_dates')['data'][0][1] -%}\n\nSELECT cast('{{ earliest }}' as date) AS earliest, cast('{{ latest }}' as date) AS latest\n\nIt worked, now we know how to create a statement that gets dates from salesorderheader. Lets use it for our final step in this experiment which is creating the Calendar with calogical.\n\n\ndbt_date.get_date_dimension\nThis wraper creates a full calendar dimensio, the transformation\n\n\ndim_dates_calogical.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\n{%- call statement('get_dates', fetch_result=True) -%}\n\n      SELECT  MIN(cast(orderdate as date)), MAX(cast(orderdate as date)) + (interval '1 day')\n      FROM {{ source('pg_sales', 'salesorderheader') }}\n\n{%- endcall -%}\n\n{%- set earliest = load_result('get_dates')['data'][0][0] -%}\n{%- set latest = load_result('get_dates')['data'][0][1] -%}\n\n{{ dbt_date.get_date_dimension(\n        start_date=earliest, \n        end_date=latest\n    ) \n}}\n\nThis script creates the following columns: date_day, prior_date_day, next_date_day, prior_year_date_day, prior_year_over_year_date_day, day_of_week, day_of_week_name, day_of_week_name_short, day_of_month, day_of_year, week_start_date, week_end_date, prior_year_week_start_date, prior_year_week_end_date, week_of_year, iso_week_start_date, iso_week_end_date, prior_year_iso_week_start_date, prior_year_iso_week_end_date, iso_week_of_year, prior_year_week_of_year, prior_year_iso_week_of_year, month_of_year, month_name, month_name_short, month_start_date, month_end_date, prior_year_month_start_date, prior_year_month_end_date, quarter_of_year, quarter_start_date, quarter_end_date, year_number, year_start_date and year_end_date."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#wrap-up",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#wrap-up",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Wrap Up",
    "text": "Wrap Up\ndbt is one of most representative tool of the called Modern Data Stack and the Analytics Engineering, claiming the incorporation of Software Engineering practices to the data stack. What I found interesting is the fact that (at least in theory) we can create the tranformations collaboratively, version control them and then deploy those models to any supported datawarehouse.\nTo get deepen in the data landscape, Modern Data Stack and dbt I highly recommend:\n\nHot Takes on the Modern Data Stack\nMAD 2023, PART III: TRENDS IN DATA INFRASTRUCTURE\nThe Next Layer of the Modern Data Stack | dbt’s Tristan Handy"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "",
    "text": "This post is about training a Matrix Factorization model with BigQuery ML and deploying it as Docker container. The end-to-end process is orchestrated through a Vertex AI pipeline.\nThe post is strongly based on this tutorial: - YouTube: Recommendation Engine Pipeline with BigQuery ML and Vertex AI Pipelines using Matrix Factorization\nBut there are some key differences:\n\nThis post documents the whole process, from loading the data into BigQuery to how to make recommendations in different ways.\nOn July 5th there was a Transition to BigQuery editions which resulted in some changes being made to adapt the scripts shown in the original video.\nWhen trying to replicate the video tutorial I had to solve some issues with the pipeline failing to run. Most of the issues were very hard to debug, with misleading error messages and layers over layer of abstraction between python libraries and component definitions. At the end most of the errors were about the Default Service Account not having the requiered permission. So,\nIn this post we can see and run each step and command to create a Service Account and grant this account granular permissions to the Google Cloud resources needed for the end to end process to run. This is what the Google documentation recommends.\nThere are also the commands to enable each service API needed for the project.\n\nAt the end we shoud get a flow like this:\n\n\n\n\nYouTube: Lesson 7: Practical Deep Learning for Coders 2022 - Collaborative filtering deep dive\nTutorial: Use BigQuery ML to make recommendations from Google analytics data\n\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is better suited for running in Colab."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#overview",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#overview",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "",
    "text": "This post is about training a Matrix Factorization model with BigQuery ML and deploying it as Docker container. The end-to-end process is orchestrated through a Vertex AI pipeline.\nThe post is strongly based on this tutorial: - YouTube: Recommendation Engine Pipeline with BigQuery ML and Vertex AI Pipelines using Matrix Factorization\nBut there are some key differences:\n\nThis post documents the whole process, from loading the data into BigQuery to how to make recommendations in different ways.\nOn July 5th there was a Transition to BigQuery editions which resulted in some changes being made to adapt the scripts shown in the original video.\nWhen trying to replicate the video tutorial I had to solve some issues with the pipeline failing to run. Most of the issues were very hard to debug, with misleading error messages and layers over layer of abstraction between python libraries and component definitions. At the end most of the errors were about the Default Service Account not having the requiered permission. So,\nIn this post we can see and run each step and command to create a Service Account and grant this account granular permissions to the Google Cloud resources needed for the end to end process to run. This is what the Google documentation recommends.\nThere are also the commands to enable each service API needed for the project.\n\nAt the end we shoud get a flow like this:\n\n\n\n\nYouTube: Lesson 7: Practical Deep Learning for Coders 2022 - Collaborative filtering deep dive\nTutorial: Use BigQuery ML to make recommendations from Google analytics data\n\n\n\n\n\n\n\nNote\n\n\n\nThis notebook is better suited for running in Colab."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#blog-post-parts",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#blog-post-parts",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Blog Post Parts",
    "text": "Blog Post Parts\n\nPart 1. Train and Deploy\n\nLoading MovieLens data into BigQuery\nSetup services and accounts for the Vertex AI Pipeline to run\nCreate and run the Pipeline and its Components\nMake sure no reservation or slot assignment remains active\n\n\n\nPart 2. Inference\n\nUse the Cloud Run endpoint to get recommendations\nGet recommendations from BigQuery ML\nGet Weights and Bias (Embeddings) and do the math"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#before-you-begin",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#before-you-begin",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Before you begin",
    "text": "Before you begin\n\nSelect or create a Google Cloud project.\nMake sure that billing is enabled for your project.\nYou can run the code locally or in Colab. If you are locally you need to install the gcloud CLI."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#loading-data-into-bigquery",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#loading-data-into-bigquery",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Loading data into BigQuery",
    "text": "Loading data into BigQuery\nReference: Load the Movielens dataset into BigQuery\n\nAuthenticate your Google Cloud account\n\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    # If you are running this notebook locally, replace the string below with the\n    # path to your service account key and run this cell to authenticate your GCP\n    # account.\n    %env GOOGLE_APPLICATION_CREDENTIALS ''\n\n\n\nSet your project ID\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n\n\n\nDownload MovieLens 1M movie ratings dataset\n\n! curl -O 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n! unzip -o ml-1m.zip\n\nChange te existing :: delimiter to comma and save the files as .csv:\n\n! sed 's/::/,/g' ml-1m/ratings.dat &gt; ratings.csv\n! sed 's/::/@/g' ml-1m/movies.dat &gt; movie_titles.csv\n\n\n\nCreate BigQuery datasets and populate the tables\nDefine the names of the datasets.\n\nMODEL_DATASET = \"[bq-model-dataset]\"  # @param {type:\"string\"}\nMOVIELENS_DATASET = \"[bq-data-dataset]\"  # @param {type:\"string\"}\n\n\n# To store the model\n! bq mk --location=US --dataset {PROJECT_ID}:{MODEL_DATASET}\n\n# To store movies and reviews tables\n! bq mk --location=US --dataset {PROJECT_ID}:{MOVIELENS_DATASET}\n\nCreate and populate the tables movielens_1m and movie_titles.\n\n# Reviews table\n! bq load --project_id={PROJECT_ID} --source_format=CSV {PROJECT_ID}:{MOVIELENS_DATASET}.movielens_1m ratings.csv user_id:INT64,item_id:INT64,rating:FLOAT64,timestamp:TIMESTAMP\n\n# Movies table\n! bq load --project_id={PROJECT_ID} --source_format=CSV --field_delimiter=@ {PROJECT_ID}:{MOVIELENS_DATASET}.movie_titles movie_titles.csv movie_id:INT64,movie_title:STRING,genre:STRING"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#vertex-ai-pipeline.-install-libraries-and-setup-services-and-accounts",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#vertex-ai-pipeline.-install-libraries-and-setup-services-and-accounts",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Vertex AI Pipeline. Install Libraries and Setup Services and Accounts",
    "text": "Vertex AI Pipeline. Install Libraries and Setup Services and Accounts\n\nInstall Libraries\n\n%%capture\n! pip install google-cloud-aiplatform==1.21.0 --upgrade\n! pip install kfp==2.0.1 --upgrade\n! pip install google-cloud-pipeline-components==2.0.0 --upgrade\n\n\n\nRestart the Kernel\n\n# Automatically restart kernel after installs so that your environment can access the new packages\nimport IPython\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n\n\n\nSet Project Variables\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"              # @param {type: \"string\"}\n\nBUCKET_NAME = \"[pipeline-bucket]\" # @param {type: \"string\"}\n\nPIPELINE_ROOT = f\"gs://{BUCKET_NAME}/\"   \n\nMODEL_DIR = PIPELINE_ROOT + \"recommender_model\"    # @param {type: \"string\"}\n\n\n\nAuthenticate your Google Cloud account (again)\nSince the kernel was restarted, authenticate again.\n\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    # If you are running this notebook locally, replace the string below with the\n    # path to your service account key and run this cell to authenticate your GCP\n    # account.\n    %env GOOGLE_APPLICATION_CREDENTIALS &lt;path-to-json-credential&gt;\n\n\n! gcloud config set project {PROJECT_ID}\n\n\n\nEnable Service APIs\nEnabling the APIs with commands instead of doing it in the UI keeps everythong documented. For this project the following services are needed: - Identity and Access Management (IAM) API - Vertex AI API - Cloud Build API - BigQuery API (Although this should be enabled already if the datasets were created and the tables populated) - BigQuery Reservation API - Cloud Run Admin API - Cloud Storage API\n\n! gcloud services enable iam.googleapis.com --project={PROJECT_ID}\n! gcloud services enable aiplatform.googleapis.com --project={PROJECT_ID}\n! gcloud services enable cloudbuild.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigquery.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigqueryreservation.googleapis.com --project={PROJECT_ID}\n! gcloud services enable run.googleapis.com --project={PROJECT_ID}\n! gcloud services enable storage-component.googleapis.com --project={PROJECT_ID}\n\n\n\nCreate a Service Account for the pipeline to run\n\nSERVICE_ACCOUNT_ID = \"[your-service-account-id]\"  # @param {type:\"string\"}\n\nIf the following cell returns an error in Colab, run it in your local machine. Or create the account directly in the Google Cloud Console UI.\n\n! gcloud iam service-accounts create {SERVICE_ACCOUNT_ID} --description=\"Vertex AI Pipeline Service Account\" --display-name=\"vertex_service_account\"\n\n\nSERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_ID}@{PROJECT_ID}.iam.gserviceaccount.com\"\n\n\nIf not, use the Default Service Account instead\n\nshell_output = ! gcloud projects describe {PROJECT_ID}\nPROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\nDEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n\nimport sys\n\nIS_COLAB = \"google.colab\" in sys.modules\n\nif (\n    SERVICE_ACCOUNT_ID == \"\"\n    or SERVICE_ACCOUNT_ID is None\n    or SERVICE_ACCOUNT_ID == \"[your-service-account-id]\"\n):\n    # Get your service account from gcloud\n    if not IS_COLAB:\n        shell_output = !gcloud auth list 2&gt;/dev/null\n        DEFAULT_SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n\n    if IS_COLAB:\n        shell_output = ! gcloud projects describe {PROJECT_ID}\n        PROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n        DEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n    print(\"Service Account:\", DEFAULT_SERVICE_ACCOUNT)\n    SERVICE_ACCOUNT = DEFAULT_SERVICE_ACCOUNT\n\n\n\n\nGrant the Service Account granular permissions to GCP resources\nThe most challenging part of the project was figuring out how to give the service account the right granular permissions. I didn’t want to give the service account the Editor or Owner rol. It took me a while to find the right roles. As I mentioned some errors in the pipeline were because lack of permissions, but it was hard to troubleshoot as there wasn’t any mention to the actual rol needed.\n\nGrant aiplatform.user rol\n\nservice_arg = f\"serviceAccount:{SERVICE_ACCOUNT}\"\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/aiplatform.user\"\n\n\n\nCreate the GCS bucket and assign storage.objectAdmin rol\nI first assigned storage.objectCreator and objectViewer roles. Took me hours to find out that the trained model couldn’t be exported without the rol of storage.objectAdmin, which is needed to modify existing data in the bucket.\n\n! gsutil mb -p {PROJECT_ID} -l {REGION} {PIPELINE_ROOT}\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectAdmin {PIPELINE_ROOT}\n\n\n\nCloud Run’s run.developer rol\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/run.developer\"\n\n\n\nCloud Build’s cloudbuild.builds.editor rol\nI thought that Cloud Run developer rol was enough for the model to be deployed. Took me a while to find out the service account needed a permission from Cloud Build service also.\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/cloudbuild.builds.editor\"\n\n\n\nAssign roles to Cloud Build’s service account\nWhen Cloud Build’s API is enabled, its service account is automatically created.\n\nBy default – for security reasons – the Cloud Build Service Account does not have the permissions to manage Cloud Run. Google Cloud Build + Google Cloud Run\n\nFirst lets grab the default Cloud Build and Compute Engine service accounts.\n\n# Cloud Build default service account\ncloud_build_sa = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\nservice_build_arg = f\"serviceAccount:{cloud_build_sa}\"\n# Compute Engine default service account\ncompute_engine_sa = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_build_arg} --role=\"roles/run.admin\"\n\nAnd the last part was to “Grant the IAM Service Account User role to the Cloud Build service account on the Cloud Run runtime service account”.\nMeaning that Cloud Build service account is going to be able to impersonate Cloud Run runtime service account, which is Compute Engine default service account. More on impersonation here: Youtube: Service Account Impersonation in Google Cloud - IAM in GCP\n\n! gcloud iam service-accounts add-iam-policy-binding {compute_engine_sa} --member={service_build_arg} --role=\"roles/iam.serviceAccountUser\"\n\nAnd that’s it, all services are enabled and the service accounts has the granular permission for the pipeline to run."
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#vertex-ai-pipeline---create-and-run-the-pipeline",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#vertex-ai-pipeline---create-and-run-the-pipeline",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Vertex AI Pipeline - Create and Run the Pipeline",
    "text": "Vertex AI Pipeline - Create and Run the Pipeline\n\nImport Libraries\n\nimport kfp\nfrom typing import NamedTuple\nfrom kfp.dsl import (\n    pipeline, component, OutputPath, InputPath, Model, \n    Input, Artifact, Output, Metrics\n)\nfrom kfp import compiler\n\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\n\n\n\nInitialize Vertex AI SDK for Python\n\naiplatform.init(project=PROJECT_ID, location=REGION)\n\n\n\nDeclare Pipeline Components\n\nCreate BigQuery Reservation Component\n\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef create_reservation(\n    project: str,\n    location: str,\n    commitment_slots: int,\n    reservation_id: str\n) -&gt; NamedTuple(\"outputs\", [(\"reservation_name\", str), (\"assignment_name\", str)]):\n    \"\"\"\n    Create BigQuery Reservation and Assignment\n    \"\"\"\n    print('project', project, 'location', location, 'commitment_slots', commitment_slots, 'reservation_id', reservation_id)\n\n    # import libraries\n    import time\n    from google.cloud.bigquery_reservation_v1 import (\n        CapacityCommitment, Reservation, Assignment, ReservationServiceClient\n    )\n\n    reservation_client = ReservationServiceClient()\n    parent_arg = f\"projects/{project}/locations/{location}\"\n\n    # Reservation (autoscaling)\n    autosc = Reservation.Autoscale(current_slots=0, max_slots=commitment_slots) # NEW Jul-05 Update\n    #reservation_slots = commitment_slots\n    slot_capacity = 0\n    reservation_config = Reservation(\n        #slot_capacity=reservation_slots,\n        slot_capacity=slot_capacity, autoscale=autosc,  # NEW Jul-05 Update\n        edition='ENTERPRISE',   # NEW Jul-05 Update (Could be \"STANDARD\" ?)\n        ignore_idle_slots=False\n    )\n    reservation = reservation_client.create_reservation(\n        parent=parent_arg, reservation_id=reservation_id, reservation=reservation_config\n    )\n    reservation_name = reservation.name\n    print('reservation_name', reservation_name)\n\n    # Assignment\n    print(\"Creating Assignment...\")\n    assignment_config = Assignment(\n        job_type='QUERY', assignee='projects/{}'.format(project)\n    )\n    assignment = reservation_client.create_assignment(\n        parent=reservation_name, assignment=assignment_config\n    )\n    assignment_name = assignment.name\n    print('assignment_name', assignment_name)\n\n    # it can take a lot for the slots to be available\n    print(\"Waiting for 300 seconds...\")\n    time.sleep(300)\n\n    return reservation_name, assignment_name\n\n\n\n\n\n\n\nThis is Important\n\n\n\nAfter the reservation is created, the account is going to be billed by time and number of slots. It can cost about $3 for each pipeline run, but you need to be very cautious about the pipeline failing before the reservation is deleted. To be sure the reservation is deleted you can go to https://console.cloud.google.com/bigquery/admin/reservations and select the project of interest. Check that SLOT RESERVATIONS and SLOT COMMITMENTS are empty and look like this:\nSLOT RESERVATIONS\n\nSLOT COMMITMENTS\n\nIf not empty, delete them in the UI. Below you can find some scripts you can run to delete the reservations in case the pipeline fails before doing it.\n\n\n\n\nDelete Reservation Component\n\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef delete_reservation(\n    reservation_name: str,\n    assignment_name: str\n):\n    # import libraries\n    from google.cloud.bigquery_reservation_v1 import ReservationServiceClient\n\n    # Delete Assignment, Reservation and Capacity\n    reservation_client = ReservationServiceClient()\n    reservation_client.delete_assignment(name=assignment_name)\n    reservation_client.delete_reservation(name=reservation_name)\n\n    print('reservation_name', reservation_name, 'deleted')\n    print('assignment_name', assignment_name, 'deleted')\n\n\n\nLog Eval Metrics Component\n\n@component()\ndef log_eval_metrics(\n    eval_metrics: Input[Artifact], metrics: Output[Metrics]\n) -&gt; dict:\n    # import libraries\n    import math\n\n    metadata = eval_metrics.metadata\n    for r in metadata[\"rows\"]:\n        rows = r[\"f\"]\n        schema = metadata[\"schema\"][\"fields\"]\n        output = {}\n        for metric, value in zip(schema, rows):\n            metric_name = metric[\"name\"]\n            val = float(value[\"v\"])\n            output[metric_name] = val\n            metrics.log_metric(metric_name, val)\n\n    print(output)\n\n\n\nDeploy Model Component\nThe names in this function are hard coded. It assumes you keep recommender_model as the model dir. If it was changed previously, change it here accordingly. Also edit [PIPELINE_ROOT] with your chosen name.\n\n@component(packages_to_install=[\n    \"google-cloud-build==3.18.0\",\n    \"google-api-python-client==2.93.0\"])\ndef deploy_recommendations_model(\n    artifact_uri: str,\n    project: str\n):\n    # import libraries\n    from google.cloud.devtools import cloudbuild\n    from googleapiclient.discovery import build\n\n    # Deploy Model\n    client = cloudbuild.CloudBuildClient()\n    build = cloudbuild.Build()\n\n    # Fill [PIPELINE_ROOT] with your selected parameter\n    # If you didn't keep `recommender_model` change it accordingly\n    build.steps = [\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/recommender_model', \".\"]  },\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/dockerfile/Dockerfile/', \"Dockerfile\"] },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"build\", \"-t\", f\"gcr.io/{project}/recommender_model\", \".\" ]   },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"push\", f\"gcr.io/{project}/recommender_model\"]    },\n        {   \"name\": \"gcr.io/cloud-builders/gcloud\",\n            \"args\": [\n                \"run\", \"deploy\",\n                \"recommender-model\", \"--image\", f\"gcr.io/{project}/recommender_model\",\n                \"--region\", \"us-central1\", \"--platform\", \"managed\", \"--memory\", \"500Mi\",\n                \"--allow-unauthenticated\", \"--max-instances\", \"5\", \"--port\", \"8501\"\n            ]\n        }\n    ]\n\n    operation = client.create_build(project_id=project, build=build)\n    # Print the in-progress operation\n    print(\"IN PROGRESS:\")\n    print(operation.metadata)\n\n    result = operation.result()\n    # Print the completed status\n    print(\"RESULT:\", result.status)\n\n\n\nCreate the Docker file and copy it to GCS\nKeep recommender_model or change it accordingly.\n\n%%writefile Dockerfile\nFROM tensorflow/serving:2.4.4\nENTRYPOINT [\"/usr/bin/env\"]\nENV MODEL_NAME=recommender_model\nENV PORT=8501\nCOPY recommender_model /models/recommender_model/1\nCMD tensorflow_model_server --port=8500 --rest_api_port=$PORT --model_base_path=/models/recommender_model --model_name=$MODEL_NAME\n\n\ndocker_path = PIPELINE_ROOT + 'dockerfile/'\n! gsutil cp Dockerfile {docker_path}\n\n\n\n\nPipeline and Job\n\nDeclare the Pipeline\n\n@pipeline(\n    name=\"bigquery-recommender-pipeline\",\n    pipeline_root=PIPELINE_ROOT + \"bigquery-recommender-pipeline\"\n)\ndef recommendation_pipeline(\n    artifact_uri: str,\n    display_name: str\n):\n\n    # import libraries\n    from google_cloud_pipeline_components.v1.bigquery import (\n        BigqueryCreateModelJobOp,\n        BigqueryEvaluateModelJobOp,\n        BigqueryExportModelJobOp\n    )\n\n    # NODE create-reservation\n    project = PROJECT_ID\n    location = \"us\"\n    slots = 100\n    reservation_id = \"matrix-factorization-reservation\"\n\n    create_reservation_task = create_reservation(\n        project=project, location=location, commitment_slots=slots, reservation_id=reservation_id\n    )\n\n    # NODE create-model-task\n    # Query for training\n    num_factors = 60\n    q = f\"\"\"\n    CREATE OR REPLACE MODEL {MODEL_DATASET}.model_00\n    OPTIONS\n        (model_type='matrix_factorization',\n        user_col='user_id',\n        item_col='item_id',\n        RATING_COL='rating',\n        feedback_type='EXPLICIT',\n        l2_reg=9.83,\n        num_factors={num_factors}) AS\n    SELECT user_id, item_id, rating FROM {MOVIELENS_DATASET}.movielens_1m\n    \"\"\"\n    create_model_task = BigqueryCreateModelJobOp(\n        project=project,\n        location=location,\n        query=q,\n    ).after(create_reservation_task)\n\n    # NODE evaluate-model-task\n    bq_evaluate_task = BigqueryEvaluateModelJobOp(\n        project=project, location=location, model=create_model_task.outputs[\"model\"]\n    ).after(create_model_task)\n\n    # NODE delete-reservation\n    delete_reservation_task = delete_reservation(\n        reservation_name=create_reservation_task.outputs[\"reservation_name\"],\n        assignment_name=create_reservation_task.outputs[\"assignment_name\"]\n    ).after(bq_evaluate_task)\n\n    # NODE log-eval-metrics\n    bqml_eval_metrics_raw = bq_evaluate_task.outputs[\"evaluation_metrics\"]\n    log_eval_metrics_task = log_eval_metrics(\n        eval_metrics=bqml_eval_metrics_raw\n    )\n\n    # NODE export-bq-model-to-gcs\n    bq_export_task = BigqueryExportModelJobOp(\n        project=project,\n        location=location,\n        model=create_model_task.outputs[\"model\"],\n        model_destination_path=artifact_uri,\n    ).after(create_model_task)\n\n    # NODE deploy-model-cloud-run\n    deploy_recommendations_model_task = deploy_recommendations_model(\n        artifact_uri=artifact_uri,\n        project=project\n    ).after(bq_export_task)\n\n\n\nCompile the Pipeline\n\ncompiler.Compiler().compile(\n    pipeline_func=recommendation_pipeline,\n    package_path=\"recommendation_pipeline.json\"\n)\n\n\n\nCreate and Run the Job\n\njob = pipeline_jobs.PipelineJob(\n    enable_caching=False,\n    display_name=\"recommendation-pipeline\",\n    template_path=\"recommendation_pipeline.json\",\n    parameter_values={\n        \"artifact_uri\": MODEL_DIR,\n        \"display_name\": \"recommendation\",\n    }\n)\n\n\njob.run(\n    service_account=SERVICE_ACCOUNT,\n    sync=False\n)\n\nThe pipeline can be monitored here: - GCP COnsole: Vertex AI Pipelines\nAnd if all went fine, you must see a beautiful picture like this:"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#make-sure-no-reservation-or-slot-assignment-is-active",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#make-sure-no-reservation-or-slot-assignment-is-active",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Make sure no reservation or slot assignment is active",
    "text": "Make sure no reservation or slot assignment is active\nIf the pipeline fails before deleting the reservation, this command lists the reservations, if any.\n\n! bq ls --reservation --project_id={PROJECT_ID} --location=us\n\nAnd this command shows the assignment.\n\n! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n\n\nDelete any reservation and assignment\n\nshell_output = ! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n\nassignment_id = shell_output[2].split(' ')[2].split('.')[-1]\nreservation_id = 'matrix-factorization-reservation' # As declared in the pipeline\n\n# remove assignment\n! bq rm --project_id={PROJECT_ID} --location=us --reservation_assignment {reservation_id}.{assignment_id}\n# remove reservation\n! bq rm --project_id={PROJECT_ID} --location=us --reservation {reservation_id}"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#monitor-the-process",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#monitor-the-process",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Monitor the process",
    "text": "Monitor the process\n\nhttps://console.cloud.google.com/cloud-build/builds?project=[your-project-id]\nhttps://console.cloud.google.com/run?project=[your-project-id]\nhttps://console.cloud.google.com/vertex-ai/pipelines/runs?project=[your-project-id]"
  },
  {
    "objectID": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#conclusions",
    "href": "posts/2023-07_bigqueryml_vertexai_recsys/bqml_vertex_recsys___part1.html#conclusions",
    "title": "RecSys MLOps. BigQuery ML and Vertex AI. Part 1",
    "section": "Conclusions",
    "text": "Conclusions\nAn that’s about it. The automated process for training an deploying the model using BigQuery ML and Vertex AI pipeline. In Part 2 we will see how to do inference and how to get insight from the embeddings (latent factors) learned by the model for each item and user."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "",
    "text": "In the first lesson of the course Walk with fastai, the missing pieces for success, Zachary Mueller explained to us the three levels he sees in fastai APIs. This is different to Jeremy Howard’s consideration, who sees two levels.\nBy the second lesson we had made use of those levels with different datasets.\nIn this post we are going to explore the three levels with MNIST dataset, and in the process we will also go step by step in some fastai pieces that were not obvious for me in my beginnings.\nAs an extra we are going to create the Datasets and Dataloader in raw PyTorch."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#intro",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#intro",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "",
    "text": "In the first lesson of the course Walk with fastai, the missing pieces for success, Zachary Mueller explained to us the three levels he sees in fastai APIs. This is different to Jeremy Howard’s consideration, who sees two levels.\nBy the second lesson we had made use of those levels with different datasets.\nIn this post we are going to explore the three levels with MNIST dataset, and in the process we will also go step by step in some fastai pieces that were not obvious for me in my beginnings.\nAs an extra we are going to create the Datasets and Dataloader in raw PyTorch."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Importing fastai library and data",
    "text": "Importing fastai library and data\n\nfrom fastai.vision.all import *\n\n\nImageDataLoaders??\n\n\npath = untar_data(URLs.MNIST); path, path.ls()\n\n(Path('/home/fmussari/.fastai/data/mnist_png'),\n (#2) [Path('/home/fmussari/.fastai/data/mnist_png/training'),Path('/home/fmussari/.fastai/data/mnist_png/testing')])\n\n\n\n(path/'training').ls()\n\n(#10) [Path('/home/fmussari/.fastai/data/mnist_png/training/4'),Path('/home/fmussari/.fastai/data/mnist_png/training/7'),Path('/home/fmussari/.fastai/data/mnist_png/training/9'),Path('/home/fmussari/.fastai/data/mnist_png/training/5'),Path('/home/fmussari/.fastai/data/mnist_png/training/8'),Path('/home/fmussari/.fastai/data/mnist_png/training/0'),Path('/home/fmussari/.fastai/data/mnist_png/training/2'),Path('/home/fmussari/.fastai/data/mnist_png/training/1'),Path('/home/fmussari/.fastai/data/mnist_png/training/6'),Path('/home/fmussari/.fastai/data/mnist_png/training/3')]\n\n\nWe can see that the data is stored in training and testing folders.\nEach image is then stored in folders that represent its labels: 0, 1, 2, … 9."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "File names and transforms",
    "text": "File names and transforms\n\nfnames = get_image_files(path)\nfnames\n\n(#70000) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/49105.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/746.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/13451.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/54187.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30554.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30886.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/52580.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/38515.png')...]\n\n\n\nItem and Batch transforms\n\nitem_tfms = [CropPad(34), RandomCrop(size=28), ToTensor()]\nbatch_tfms = [IntToFloatTensor(), Normalize()]\n\n\nTransforms in action\nHere we are going to explore what each transform does.\n\nim = PILImageBW.create(fnames[0])\nim\n\n\n\n\n\nCropPad(50)(im)\n\n\n\n\n\nRandomCrop(size=28)(CropPad(50)(im))\n\n\n\n\nHave you ever tried reduce? Let’s the first two transforms sequentially with it:\n\nfrom functools import reduce\nreduce(lambda t,f: f(t), item_tfms[:2], im)\n\n\n\n\nWhich is equivalent to apply the transforms with a for loop:\n\nim0 = im\nfor tfm in item_tfms[:2]:\n    im0 = tfm(im0)\nim0\n\n\n\n\nWhat about applying the three transforms, also including ToTensor()\n\nreduce(lambda t,f: f(t), item_tfms, im).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nToTensor()(RandomCrop(size=28)(CropPad(34)(im))).shape\n\ntorch.Size([1, 28, 28])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "High Level API",
    "text": "High Level API\nFor this level we are going to use ImageDataLoaders, let’s explore some of its methods:\n\nprint([f for f in dir(ImageDataLoaders) if f[:4]=='from'])\n\n['from_csv', 'from_dblock', 'from_df', 'from_dsets', 'from_folder', 'from_lists', 'from_name_func', 'from_name_re', 'from_path_func', 'from_path_re']\n\n\nFor this dataset from_folder method is the way to go.\n\nhelp(ImageDataLoaders.from_folder)\n\nHelp on method from_folder in module fastai.vision.data:\n\nfrom_folder(path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, item_tfms=None, batch_tfms=None, img_cls=&lt;class 'fastai.vision.core.PILImage'&gt;, *, bs: 'int' = 64, val_bs: 'int' = None, shuffle: 'bool' = True, device=None) method of builtins.type instance\n    Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)\n\n\n\n\nData Loaders\n\ndls = ImageDataLoaders.from_folder(\n    path, train='training', valid='testing',\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dls)\n\nfastai.data.core.DataLoaders\n\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs #default batch size\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...],\n (#10000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Mid Level API",
    "text": "Mid Level API\nWith the mid level API we define blocks according to the problem. In this case we have image inputs and categories to predict.\n\nblocks\n\nblocks = (ImageBlock(cls=PILImageBW), CategoryBlock)\n\n\nGrandparentSplitter and parent_label\nIt was transparent to us in the high level API, but what from_folder method used to split the dataset into train and valid was GrandparentSplitter. Lets see how it works:\n\nsplitter = GrandparentSplitter(train_name='training', valid_name='testing')\n\nLet’s create a tiny subset of fnames, having training and testing samples, and see how splitter works:\n\nsub_fnames = fnames[:2] + fnames[-2:]\nsub_fnames\n\n(#4) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/7829.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/2300.png')]\n\n\n\nsplitter(sub_fnames)\n\n([0, 1], [2, 3])\n\n\nWe can see it is returnin a tuple with two list of indices, one for training, and one for validations, according to the folder the images are located in.\n\nt, v = splitter(fnames)\nlen(t), len(v)\n\n(60000, 10000)\n\n\nThe high level API also used the parent_label function under the hood. It returns the label from the parent folder of each image.\n\nparent_label(sub_fnames[0]), parent_label(sub_fnames[2])\n\n('4', '3')\n\n\n\nPILImageBW.create(sub_fnames[2])\n\n\n\n\n\n\n\nDataBlock\n\ndblock = DataBlock(\n    blocks=blocks,\n    get_items=get_image_files, \n    splitter=splitter,\n    get_y=parent_label,\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dblock)\n\nfastai.data.block.DataBlock\n\n\n\n\nData Loaders\n\ndls = dblock.dataloaders(path, bs=64)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Low Level API",
    "text": "Low Level API\nWe already created a splitter, lets now split the data.\n\nsplits=splitter(fnames)\nlen(splits[0]), len(splits[1])\n\n(60000, 10000)\n\n\n\nDatasets\n\ndsrc = Datasets(\n    items=fnames,\n    tfms=[[PILImageBW.create], [parent_label, Categorize]],\n    splits=splits\n)\n\n\ntype(dsrc)\n\nfastai.data.core.Datasets\n\n\n\nshow_at(dsrc.train, 3);\n\n\n\n\n\nExploring what each tfms does\n\nPILImageBW.create(fnames[0])\n\n\n\n\n\nl = parent_label(fnames[0])\nl\n\n'4'\n\n\n\nCategorize(l)\n\nCategorize -- {'vocab': ['4'], 'sort': True, 'add_na': False}:\nencodes: (Tabular,object) -&gt; encodes\n(object,object) -&gt; encodes\ndecodes: (Tabular,object) -&gt; decodes\n(object,object) -&gt; decodes\n\n\n\n\n\nData Loaders\n\ndls = dsrc.dataloaders(\n    bs=128,\n    after_item=item_tfms,\n    after_batch=batch_tfms\n)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n128\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Raw PyTorch",
    "text": "Raw PyTorch\nJust as the previous fastai examples, this is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code. &gt; This example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\nfrom torchvision.transforms import ToTensor, Normalize, Pad, RandomCrop\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nTrain and validation Datasets\n\nclass MNISTDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential=None):       \n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]     \n        self.to_tensor = ToTensor()\n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return int(label)\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\ntrain_transforms = nn.Sequential(\n    Pad(6),\n    RandomCrop(28)\n)\n\n\ntrain_dataset = MNISTDataset(path/'training', train_transforms)\nvalid_dataset = MNISTDataset(path/'testing')\nlen(train_dataset), len(valid_dataset)\n\n(60000, 10000)\n\n\nOne of the items in the dataset:\n\nx,y = train_dataset[0]\nx.shape, y\n\n(torch.Size([1, 28, 28]), 4)\n\n\n\n\nPyTorch Dataloader\n\nbs = 128\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=False,\n    batch_size=bs\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=bs*2\n)\n\n\nnext(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[1].shape\n\n(torch.Size([128, 1, 28, 28]), torch.Size([128]))\n\n\n\nfigure = plt.figure(figsize=(4, 4))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\nThe next cell is just a curiosity leaned from twitter:\n\na = [1,2,3]\nb = [4,5,6]\n[i for i in (*a,*b)]\n\n[1, 2, 3, 4, 5, 6]"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe ended with the same fastai’s DataLoaders in three different ways, and then in an extra raw PyTorch way.\nAs Zachary puts it, the higher the level of the API, the lesser the flexibility, but there is also less complexity.\nHe also pointed out that the mid level API is made with the building blocks of the framework."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 1. Extracting transcriptions and creating SQLite’s searchable index"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to get the transcriptions of YouTube videos from one or more given Playlists. Here we are going to do it for fastai channel, but it can be done for any given list of playlists (if the videos have transcriptions).\nAfter we get the transcriptions, we are going to build a search engine with SQLite’s full-text search functionality provided by its FTS5 extension.\nIn Part 2 we are going to build and share the search engine as a Streamlit web app, just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Get YouTube Transcriptions",
    "text": "Get YouTube Transcriptions\n\nInstall and Import Libraries\nWe need to first install the libraries we need (pytube and youtube-transcript-api).\nWe can use pip:\n$ pip install pytube\n$ pip install youtube_transcript_api\nOr conda:\n$ conda install -c conda-forge pytube\n$ conda install -c conda-forge youtube-transcript-api\n\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom pytube import YouTube, Playlist\n\nimport sqlite3\n\n\n\nYouTube Playlists\nLet’s create a list of YouTube playlist ids. We can get them browsing YouTube playlist. The id is in the url which has the following format:\nhttps://www.youtube.com/playlist?list={PLAYLIST_ID}\n\nbase_pl = 'https://www.youtube.com/playlist?list='\nbase_yt = 'https://youtu.be/'\n\nyt_pl_ids = [\n    'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', # fast.ai APL Study Group #2022\n    'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU', # Practical Deep Learning for Coders 2022\n    'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM', # fast.ai live coding & tutorials #2022\n    'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM', # Practical Deep Learning for Coders (2020)\n    'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj', # Deep Learning from the Foundations #2019\n    'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq', # fastai v2 code walk-thrus #2019\n    'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn', # Practical Deep Learning for Coders 2019\n    'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96', # Introduction to Machine Learning for Coders\n    'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia', # Cutting Edge Deep Learning for Coders 2 #2018\n    'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM', # Practical Deep Learning For Coders 2018\n]\n\n\n\nGet Transcriptions\nLet’s explore the methods:\n\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU')\nprint(playlist.title)\nvideo = YouTube(playlist[0])\nprint(video.title)\nprint(playlist[0])\nvideo_id = playlist[0].split('=')[1]\nscript = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\nprint(script[0])\n\nPractical Deep Learning for Coders 2022\nLesson 1: Practical Deep Learning for Coders 2022\nhttps://www.youtube.com/watch?v=8SF_h3xF3cE\n{'text': 'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0', 'start': 2.0, 'duration': 8.0}\n\n\n\nDownload all transcriptions\nNow we are going to download all the transcriptions. Let’s create three dictionaries to store the data: - playlists to store each playlist as {playlist_id: playlist_name} - videos to store videos as {video_id: video_name} - database to store all captions as {playlist_id: {video_id: {'start': caption}}.\n\nplaylists = dict()\nvideos = dict()\ndatabase = dict()\n\nfor pl_id in yt_pl_ids:\n    playlist = Playlist(base_pl + pl_id)\n    print(playlist.title)\n    playlists[pl_id] = playlist.title\n    database[pl_id] = dict()\n\n    for video in playlist:\n        video_id = video.split(\"=\")[1]\n        videos[video_id] = YouTube(video).title\n        database[pl_id][video_id] = dict()\n        # Manually created transcripts are returned first\n        script = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\n\n        for txt in script:\n            database[pl_id][video_id][txt['start']] = txt['text']\n\nfast.ai APL Study Group\nPractical Deep Learning for Coders 2022\nfast.ai live coding & tutorials\nPractical Deep Learning for Coders (2020)\nDeep Learning from the Foundations\nfastai v2 code walk-thrus\nPractical Deep Learning for Coders 2019\nIntroduction to Machine Learning for Coders\nCutting Edge Deep Learning for Coders 2\nPractical Deep Learning For Coders 2018"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Building the Search Engine",
    "text": "Building the Search Engine\n\nFormatting the data to facilitate insertion into SQLite\n\n# https://stackoverflow.com/a/60932565/10013187\nrecords = [\n    (level1, level2, level3, leaf)\n    for level1, level2_dict in database.items()\n    for level2, level3_dict in level2_dict.items()\n    for level3, leaf in level3_dict.items()\n]\nprint(\"(playlist_id, video_id, start, text)\")\nprint(records[100])\n\n(playlist_id, video_id, start, text)\n('PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', 'CGpR2ILao5M', 294.18, 'gonna go watch them or anything all')\n\n\n\n\nCreating the database\n\ndb = sqlite3.connect('fastai_yt.db')\ncur = db.cursor()\n\n\n# virtual table configured to allow full-text search\ncur.execute('DROP TABLE IF EXISTS transcriptions_fts;') \ncur.execute('CREATE VIRTUAL TABLE transcriptions_fts USING fts5(playlist_id, video_id, start, text, tokenize=\"porter unicode61\");')\n\n# dimension like tables\ncur.execute('DROP TABLE IF EXISTS playlist;')\ncur.execute('CREATE TABLE playlist (playlist_id, playlist_name);')\ncur.execute('DROP TABLE IF EXISTS video;')\ncur.execute('CREATE TABLE video (video_id, video_name);')\n\n&lt;sqlite3.Cursor&gt;\n\n\n\n# bulk index records\ncur.executemany('INSERT INTO transcriptions_fts (playlist_id, video_id, start, text) values (?,?,?,?);', records)\ncur.executemany('INSERT INTO playlist (playlist_id, playlist_name) values (?,?);', playlists.items())\ncur.executemany('INSERT INTO video (video_id, video_name) values (?,?);', videos.items())\ndb.commit()\n\nExample of a simple query:\n\ncur.execute('SELECT start, text FROM transcriptions_fts WHERE video_id=\"8SF_h3xF3cE\" LIMIT 5').fetchall()\n\n[(2.0,\n  'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0'),\n (11.44,\n  \"and it's the first new one we've done\\xa0\\nin two years. So, we've got a lot of\\xa0\\xa0\"),\n (15.12,\n  \"cool things to cover! It's amazing how much has\\xa0\\nchanged. Here is an xkcd from the end of 2015.\\xa0\\xa0\"),\n (28.0,\n  'Who here has seen xkcd comics before?\\xa0\\n…Pretty much everybody. Not surprising.\\xa0\\xa0'),\n (35.36,\n  \"So the basic joke here is… I'll let you\\xa0\\nread it, and then I'll come back to it.\")]\n\n\nfastai_yt.db. Once we have the database populated, we can use it in any application we want without the need to get the transcriptions from YouTube."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Search queries",
    "text": "Search queries\n\ndef print_search_results(res):\n    for each in res:\n        print()\n        print(playlists[each[0]], \"-&gt;\", videos[each[1]])\n        print(f'\"... {each[4]}...\"')\n        print('https://youtu.be/' + each[1] + \"?t=\" + str(int(each[2])))\n\ndef get_query(q, limit):\n    search_in = 'text'\n    if 'text:' in q: search_in = 'transcriptions_fts'\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    print(query)\n    return query\n\n\nSearch for a word\n\nq = 'fastc*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'fastc*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -&gt; Live coding 2\n\"... fastgen so [fastchan] is a channel that...\"\nhttps://youtu.be/0pWjZByJ3Lk?t=3720\n\n\n\nq = 'deleg*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'deleg*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfastai v2 code walk-thrus -&gt; fastai v2 walk-thru #9\n\"... [delegated] down to that so [delegates] down...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2462\n\nDeep Learning from the Foundations -&gt; Lesson 9 (2019) - How to train your model\n\"... [delegate] get attribute to the other...\"\nhttps://youtu.be/AcA8HAYh7IE?t=6435\n\nfastai v2 code walk-thrus -&gt; fastai v2 walk-thru #9\n\"... [delegate] everything Sodor in Python...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2304\n\nDeep Learning from the Foundations -&gt; Lesson 13 (2019) - Basics of Swift for Deep Learning\n\"... default [delegates] is probably going to...\"\nhttps://youtu.be/3TqN_M1L4ts?t=6750\n\nfastai v2 code walk-thrus -&gt; fastai v2 walk-thru #2\n\"... this [delegates] decorator and what the...\"\nhttps://youtu.be/yEe5ZUMLEys?t=4756\n\n\n\n\nFaceted Search\nWe can limit the search to specific playlists in a faceted like search.\n\nplaylists\n\n{'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU': 'fast.ai APL Study Group',\n 'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU': 'Practical Deep Learning for Coders 2022',\n 'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM': 'fast.ai live coding & tutorials',\n 'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM': 'Practical Deep Learning for Coders (2020)',\n 'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj': 'Deep Learning from the Foundations',\n 'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq': 'fastai v2 code walk-thrus',\n 'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn': 'Practical Deep Learning for Coders 2019',\n 'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96': 'Introduction to Machine Learning for Coders',\n 'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia': 'Cutting Edge Deep Learning for Coders 2',\n 'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM': 'Practical Deep Learning For Coders 2018'}\n\n\n\npl_lst = list(playlists.keys())\n\n\n# Search in playlist 'Practical Deep Learning for Coders 2022' or\n# 'fast.ai live coding & tutorials'\nq = f\"\"\"\n(text: fastcore OR paral*) AND \n(playlist_id: \"{pl_lst[1]}\" OR \"{pl_lst[2]}\")\n\"\"\"\nres = cur.execute(get_query(q, limit=10)).fetchall()\n\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE transcriptions_fts MATCH '\n(text: fastcore OR paral*) AND \n(playlist_id: \"PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU\" OR \"PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM\")\n' ORDER BY rank\n    LIMIT \"10\" \n    \n\nPractical Deep Learning for Coders 2022 -&gt; Lesson 6: Practical Deep Learning for Coders 2022\n\"... but my [fastcore] library has a [parallel] sub module \nwhich can basically do anything that you can do  ...\"\nhttps://youtu.be/AdhG64NF76E?t=3799\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -&gt; Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1155\n\nfast.ai live coding & tutorials -&gt; Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1160\n\nfast.ai live coding & tutorials -&gt; Live coding 3\n\"... and import [fastcore] it can't find it...\"\nhttps://youtu.be/B6BQiIgiEks?t=3401\n\nfast.ai live coding & tutorials -&gt; Live coding 8\n\"... for [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1049\n\nfast.ai live coding & tutorials -&gt; Live coding 15\n\"... somewhat in [parallel]...\"\nhttps://youtu.be/6JGoes9_bPs?t=5589"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Conclusions",
    "text": "Conclusions\n\nWe used youtube-transcript-api and pytube Python libraries to extract YouTube captions based on the given playlists.\nWe indexed the captions using the capabilities of the ubiquitous SQLite and FTS5.\nWe did some powerful full-text search queries and simulated a faceted search.\nWe can go exactly to the video part the search is returning.\nIn Part 2 we are going to deploy an web app to Streamlit."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "",
    "text": "Part 2. Label images with Smart Labeler"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\n-&gt; We are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "References",
    "text": "References\n\nCustom Vision Documentation: Label images faster with Smart Labeler\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.1. Labeling the Images",
    "text": "Part 2.1. Labeling the Images\nSmart Labeler is a simple tool for labeling images. It can be used for classification and object detection problems. When working in this problem I missed the ability to zoom-in when labeling some small objects, but as I said, this is a straightforward tool.\nFor speeding up bigger projects it might be usefull that you can first label some pictures, then train and get suggestions for the untagged images, but I didn’t use it. By default the labeler tries to give suggestions even without that first training.\nThe process is simple and you can the use the annotation to train models outside the service (as we are going to try after this series, hopefully, using fastai).\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region, ImageRegionCreateEntry\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nDOTENV_PATH = './.env'\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nCreating Labels\nSince I already did the manual tagging, we can use those tags in this new project.\nFirst we need to create the labels/tags in the service: - CustomVisionTrainingClient.create_tag()\n\ntags = ['Grid', 'Panel', 'Radome', 'RRU', 'Shroud', 'Solid']\ndesc = ['Grid Antenna', 'Panel Cel. Antenna', 'Radome Antenna', \n        'RRU Equipment', 'Shroud Antenna', 'Solid Antenna']\n\n\nservice_tags = []\nfor i, tag in enumerate(tags):\n    service_tags.append(\n        training_client.create_tag(\n            project_id=project_id, name=tag,\n            description=desc[i]\n        )\n    )\nservice_tags\n\n[&lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;,\n &lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;,\n &lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;,\n &lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;,\n &lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;,\n &lt;azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag&gt;]\n\n\nNow we can see this in the service:\n\n\nCustomVisionTrainingClient.get_tags()\n\n\nservice_tags = training_client.get_tags(project_id=project_id)\n\n\nservice_tag_ids = {tag.name: tag.id for tag in service_tags}\nservice_tag_ids\n\n{'RRU': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n 'Shroud': '4e413c15-141a-419b-a958-1485008b2904',\n 'Solid': '3f13d9b0-7b4d-4679-8fb8-7855cea0a118',\n 'Radome': 'a1020654-79c5-4d8a-867c-93dfb2a4a81d',\n 'Grid': 'e016b6a4-49e6-4897-a0c7-d8fc64d032f1',\n 'Panel': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e'}\n\n\n\n\nUpload Regions from json file\nAs I pointed before, you can create all the regions with Smart Labeler. Since I did that already in a previos project, I updated the region’s image ids and tags to the ones in this project and save them as a json.\n\nCustomVisionTrainingClient.create_image_regions()\n\nWe can see from the documentation that “There is a limit of 64 entries in a batch.”\n\nwith open(\"20221016_CreateImageRegions_Body.json\") as json_file:\n    regions_dict = json.load(json_file)\n\nprint(f'We have a total of {len(regions_dict[\"regions\"]):_} regions.')\nprint()\nprint('The first two regions:')\nregions_dict['regions'][:2]\n\nWe have a total of 1_279 regions.\n\nThe first two regions:\n\n\n[{'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n  'left': 0.6395582,\n  'top': 0.0,\n  'width': 0.10740108,\n  'height': 0.14776269},\n {'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e',\n  'left': 0.772766,\n  'top': 0.16059849,\n  'width': 0.22664931,\n  'height': 0.40633526}]\n\n\n\n# Create batches of 60 regions\n\nregions = regions_dict['regions']\n\nfor i in range(int(1_279 / 60)+1):\n    \n    batch_regions = []\n    print(f'Creating Regions {i*60+1:&gt;{5}_} to {min((i+1)*60, 1_279):&gt;{5}_}')\n    \n    for region in regions[i*60: (i+1)*60]:\n        batch_regions.append(\n            ImageRegionCreateEntry(\n                image_id=region['imageId'],\n                tag_id=region['tagId'],\n                left=region['left'], top=region['top'],\n                width=region['width'], height=region['height']\n        ))\n\n    training_client.create_image_regions(\n        project_id=project_id, \n        regions=batch_regions\n    )\n\nCreating Regions     1 to    60\nCreating Regions    61 to   120\nCreating Regions   121 to   180\nCreating Regions   181 to   240\nCreating Regions   241 to   300\nCreating Regions   301 to   360\nCreating Regions   361 to   420\nCreating Regions   421 to   480\nCreating Regions   481 to   540\nCreating Regions   541 to   600\nCreating Regions   601 to   660\nCreating Regions   661 to   720\nCreating Regions   721 to   780\nCreating Regions   781 to   840\nCreating Regions   841 to   900\nCreating Regions   901 to   960\nCreating Regions   961 to 1_020\nCreating Regions 1_021 to 1_080\nCreating Regions 1_081 to 1_140\nCreating Regions 1_141 to 1_200\nCreating Regions 1_201 to 1_260\nCreating Regions 1_261 to 1_279\n\n\nExample image, capture from the service:\n\n\n\nVerifying the number of created Regions\n\nCustomVisionTrainingClient.get_images()\n\n\nall_tagged_images = training_client.get_images(\n    project_id=project_id,\n    tagging_status=\"Tagged\", \n    take=250   # Max 256\n)\ni = 0\nfor im in all_tagged_images: i += len(im.regions)\nprint(f\"Number of created Regions: {i:_}\")\n\nNumber of created Regions: 1_279\n\n\n\n\nDraw some regions\n\nimages_df = pd.read_csv('20221015_203_Images_Uploaded_WxH.csv')\nimages_df.index = images_df.image_id\nimages_df.head(5)\n\n\n\n\n\n\n\n\nimage_name\nimage_id\nimage_status\nori_w\nori_h\ntrain_w\ntrain_h\n\n\nimage_id\n\n\n\n\n\n\n\n\n\n\n\n452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n41.JPG\n452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\nOK\n640\n480\n640\n480\n\n\n96b7774e-f5ad-4591-aa71-99ad5c71135e\nCIMG0030.JPG\n96b7774e-f5ad-4591-aa71-99ad5c71135e\nOK\n1620\n2160\n900\n1200\n\n\n3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\nCIMG0031.JPG\n3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\nOK\n1620\n2160\n900\n1200\n\n\n1320ab2e-3405-4853-bd7e-b0ef0f915d4b\nCIMG0056.JPG\n1320ab2e-3405-4853-bd7e-b0ef0f915d4b\nOK\n2160\n1620\n1200\n900\n\n\naa67eceb-3db0-4026-bf16-0842c006e6ac\nCIMG0059.JPG\naa67eceb-3db0-4026-bf16-0842c006e6ac\nOK\n2160\n1620\n1200\n900\n\n\n\n\n\n\n\nCreate a dictionary to easily access all regions from an image id:\n\nimg2ann = dict()\n\nfor image in all_tagged_images:\n    img2ann[image.id] = tuple([list(), list()])\n    image_w = image.width; image_h = image.height\n    ori_w = images_df.loc[image.id].ori_w\n    ori_h = images_df.loc[image.id].ori_h\n    for region in image.regions:\n        img2ann[image.id][1].append(region.tag_name)\n        img2ann[image.id][0].append([\n            region.left*ori_w, \n            region.top*ori_h, \n            region.width*ori_w, \n            region.height*ori_h\n        ])\n\n\npics_folder = Path('./train_images')\n\n\n# https://youtu.be/Z0ssNAbe81M?t=4636\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()\n    ])\n\ndef draw_rect(ax, b):\n    patch = ax.add_patch(\n        patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=1)\n    )\n    draw_outline(patch, 4)\n\ndef draw_text(ax, xy, txt, sz=14):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_regions(index=0):\n    im = Image.open( pics_folder / images_df.iloc[index].image_name )\n    ax = show_img(im, figsize=(8,8))\n\n    reg, lab = img2ann[images_df.iloc[index].image_id]\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(index=0)\n\n[[329.19859199999996, 205.3586496, 53.42696959999999, 114.2365248], [249.3986112, 264.75866399999995, 112.4269696, 85.23652799999999]]\n\n\n\n\n\nA dragon-fly was cought in that picture!\n\ndraw_regions(index=100)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.2. Train and Test a Model",
    "text": "Part 2.2. Train and Test a Model\n\nTrain the model\n\nCustomVisionTrainingClient.train_project()\n\n\ntrain_iteration = training_client.train_project(\n    project_id=project_id,\n    training_type='Regular'\n)\n\n\ntrain_iteration.as_dict()\n\n{'id': 'd0006e20-33dd-4806-9fe9-cfc3fca82552',\n 'name': 'Iteration 1',\n 'status': 'Training',\n 'created': '2022-10-12T13:34:38.120Z',\n 'last_modified': '2022-10-22T14:56:30.406Z',\n 'project_id': 'f6cb4ba7-5bbe-46a4-8836-69654dc86f3a',\n 'exportable': False,\n 'training_type': 'Regular',\n 'reserved_budget_in_hours': 0,\n 'training_time_in_minutes': 0}\n\n\n\nCustomVisionTrainingClient.get_iteration_performance()\n\n\nperformance = training_client.get_iteration_performance(\n    project_id=project_id,\n    iteration_id=train_iteration.id\n).as_dict()\n\n   \nfor tag in performance['per_tag_performance']:\n    print('/'*20)\n    print('tag:', tag['name'])\n    print('image count:', training_client.get_tag(\n        project_id=project_id, tag_id=service_tags[tag['name']]\n    ).image_count)\n    print('recall:', tag['recall'])\n    print('average_precision:', tag['average_precision'])\n\n////////////////////\ntag: Shroud\nimage count: 140\nrecall: 0.35789475\naverage_precision: 0.7280897\n////////////////////\ntag: Panel\nimage count: 68\nrecall: 0.11392405\naverage_precision: 0.3710658\n////////////////////\ntag: Solid\nimage count: 88\nrecall: 0.21428572\naverage_precision: 0.4641156\n////////////////////\ntag: Grid\nimage count: 80\nrecall: 0.10526316\naverage_precision: 0.3784035\n////////////////////\ntag: Radome\nimage count: 20\nrecall: 0.0\naverage_precision: 0.051538005\n////////////////////\ntag: RRU\nimage count: 32\nrecall: 0.13043478\naverage_precision: 0.48053658\n\n\nSome things that I would take into account now that negatively impact the model performance: - I choose many images with small boxes. - Some tags are not represented equally, so we ended an unbalanced distribution. - And of course lets remember we only did a quick train.\nThis is a very good thread on some tips and tricks to improve object detection:\n\n\n😨 Training an Object Detection Model is a very challenging task and involves tweaking so many knobsHere is an exhaustive 🎁 tips & tricks list 🎁 that you could use to boost your model performance 🧵 pic.twitter.com/sOvEUhCCwg\n\n— AI Fast Track (@ai_fast_track) October 20, 2022\n\n\n\n\nTest the model (Quick Test)\nQuick test allows to test the model without publishing a prediction API.\n\n# Load image and get height, width and channels\nimage_file = Path(\"./test_images/las-palmas-at-60-(20).jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\nCustomVisionTrainingClient.quick_test_image()\n\n\n# Detect objects in the test image\nprint('Detecting objects in', image_file)\n\nwith open(image_file, mode=\"rb\") as image_data:\n    results = training_client.quick_test_image(\n        project_id=project_id, \n        image_data=image_data,\n        iteration_id=train_iteration.id\n    )\n\nDetecting objects in test_images/las-palmas-at-60-(20).jpg\n\n\n\ndef get_reg_lab(results):\n    reg = []; lab = []\n    for prediction in results.predictions:\n        # Only show objects with a &gt; 50% probability\n        if (prediction.probability*100) &gt; 50:\n            left = prediction.bounding_box.left * w\n            top = prediction.bounding_box.top * h\n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n            reg.append([left, top, width, height])\n            lab.append(prediction.tag_name)\n    return reg, lab\n\n\ndef draw_regions(image):\n    \n    ax = show_img(image, figsize=(8,8))\n    reg, lab = get_reg_lab(results)\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(image)\n\n\n\n\nAs you can see, it didn’t detect some antennas. But taking into account we did a regular training and the limitations mentioned in the training data, it is impressive that it got some right in such a complex problem as object detection.\n\nimage_file = Path(\"./test_images/DSC09399.jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\ndraw_regions(image)\n\n\n\n\nNot a good job in this one. But this result is with a “regular” training (quick).\nYou can see in the Gradio demo Telecom Object Detection with Azure Custom Vision that the model trained for 1 hour (free tier limit) does a better job with this picture."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Conslusions",
    "text": "Conslusions\n\nObject detection is a complex problem. The fact that the service does a reasonable good job with unbalanced training photos and with such a limited training time talks about the great margin for improvement."
  }
]