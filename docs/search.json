[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Iâ€™m Francisco Mussari, blogging from Caracas about data, ai, cloud computing, engineering and social sciences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datamatica",
    "section": "",
    "text": "fastai\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\nStreamlit\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\npytube\n\n\nyoutube-transcript-api\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\ngradio\n\n\nhugging face\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "",
    "text": "In the first lesson of the course Walk with fastai, the missing pieces for success, Zachary Mueller explained to us the three levels he sees in fastai APIs. This is different to Jeremy Howardâ€™s consideration, who sees two levels.\nBy the second lesson we had made use of those levels with different datasets.\nIn this post we are going to explore the three levels with MNIST dataset, and in the process we will also go step by step in some fastai pieces that were not obvious for me in my beginnings.\nAs an extra we are going to create the Datasets and Dataloader in raw PyTorch."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Importing fastai library and data",
    "text": "Importing fastai library and data\n\nfrom fastai.vision.all import *\n\n\npath = untar_data(URLs.MNIST); path, path.ls()\n\n(Path('/home/fmussari/.fastai/data/mnist_png'),\n (#2) [Path('/home/fmussari/.fastai/data/mnist_png/training'),Path('/home/fmussari/.fastai/data/mnist_png/testing')])\n\n\n\n(path/'training').ls()\n\n(#10) [Path('/home/fmussari/.fastai/data/mnist_png/training/4'),Path('/home/fmussari/.fastai/data/mnist_png/training/7'),Path('/home/fmussari/.fastai/data/mnist_png/training/9'),Path('/home/fmussari/.fastai/data/mnist_png/training/5'),Path('/home/fmussari/.fastai/data/mnist_png/training/8'),Path('/home/fmussari/.fastai/data/mnist_png/training/0'),Path('/home/fmussari/.fastai/data/mnist_png/training/2'),Path('/home/fmussari/.fastai/data/mnist_png/training/1'),Path('/home/fmussari/.fastai/data/mnist_png/training/6'),Path('/home/fmussari/.fastai/data/mnist_png/training/3')]\n\n\nWe can see that the data is stored in training and testing folders.\nEach image is then stored in folders that represent its labels: 0, 1, 2, â€¦ 9."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "File names and transforms",
    "text": "File names and transforms\n\nfnames = get_image_files(path)\nfnames\n\n(#70000) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/49105.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/746.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/13451.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/54187.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30554.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30886.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/52580.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/38515.png')...]\n\n\n\nItem and Batch transforms\n\nitem_tfms = [CropPad(34), RandomCrop(size=28), ToTensor()]\nbatch_tfms = [IntToFloatTensor(), Normalize()]\n\n\nTransforms in action\nHere we are going to explore what each transform does.\n\nim = PILImageBW.create(fnames[0])\nim\n\n\n\n\n\nCropPad(50)(im)\n\n\n\n\n\nRandomCrop(size=28)(CropPad(50)(im))\n\n\n\n\nHave you ever tried reduce? Letâ€™s the first two transforms sequentially with it:\n\nfrom functools import reduce\nreduce(lambda t,f: f(t), item_tfms[:2], im)\n\n\n\n\nWhich is equivalent to apply the transforms with a for loop:\n\nim0 = im\nfor tfm in item_tfms[:2]:\n    im0 = tfm(im0)\nim0\n\n\n\n\nWhat about applying the three transforms, also including ToTensor()\n\nreduce(lambda t,f: f(t), item_tfms, im).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nToTensor()(RandomCrop(size=28)(CropPad(34)(im))).shape\n\ntorch.Size([1, 28, 28])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "High Level API",
    "text": "High Level API\nFor this level we are going to use ImageDataLoaders, letâ€™s explore some of its methods:\n\nprint([f for f in dir(ImageDataLoaders) if f[:4]=='from'])\n\n['from_csv', 'from_dblock', 'from_df', 'from_dsets', 'from_folder', 'from_lists', 'from_name_func', 'from_name_re', 'from_path_func', 'from_path_re']\n\n\nFor this dataset from_folder method is the way to go.\n\nhelp(ImageDataLoaders.from_folder)\n\nHelp on method from_folder in module fastai.vision.data:\n\nfrom_folder(path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, item_tfms=None, batch_tfms=None, img_cls=<class 'fastai.vision.core.PILImage'>, *, bs: 'int' = 64, val_bs: 'int' = None, shuffle: 'bool' = True, device=None) method of builtins.type instance\n    Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)\n\n\n\n\nData Loaders\n\ndls = ImageDataLoaders.from_folder(\n    path, train='training', valid='testing',\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dls)\n\nfastai.data.core.DataLoaders\n\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs #default batch size\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...],\n (#10000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Mid Level API",
    "text": "Mid Level API\nWith the mid level API we define blocks according to the problem. In this case we have image inputs and categories to predict.\n\nblocks\n\nblocks = (ImageBlock(cls=PILImageBW), CategoryBlock)\n\n\nGrandparentSplitter and parent_label\nIt was transparent to us in the high level API, but what from_folder method used to split the dataset into train and valid was GrandparentSplitter. Lets see how it works:\n\nsplitter = GrandparentSplitter(train_name='training', valid_name='testing')\n\nLetâ€™s create a tiny subset of fnames, having training and testing samples, and see how splitter works:\n\nsub_fnames = fnames[:2] + fnames[-2:]\nsub_fnames\n\n(#4) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/7829.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/2300.png')]\n\n\n\nsplitter(sub_fnames)\n\n([0, 1], [2, 3])\n\n\nWe can see it is returnin a tuple with two list of indices, one for training, and one for validations, according to the folder the images are located in.\n\nt, v = splitter(fnames)\nlen(t), len(v)\n\n(60000, 10000)\n\n\nThe high level API also used the parent_label function under the hood. It returns the label from the parent folder of each image.\n\nparent_label(sub_fnames[0]), parent_label(sub_fnames[2])\n\n('4', '3')\n\n\n\nPILImageBW.create(sub_fnames[2])\n\n\n\n\n\n\n\nDataBlock\n\ndblock = DataBlock(\n    blocks=blocks,\n    get_items=get_image_files, \n    splitter=splitter,\n    get_y=parent_label,\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dblock)\n\nfastai.data.block.DataBlock\n\n\n\n\nData Loaders\n\ndls = dblock.dataloaders(path, bs=64)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Low Level API",
    "text": "Low Level API\nWe already created a splitter, lets now split the data.\n\nsplits=splitter(fnames)\nlen(splits[0]), len(splits[1])\n\n(60000, 10000)\n\n\n\nDatasets\n\ndsrc = Datasets(\n    items=fnames,\n    tfms=[[PILImageBW.create], [parent_label, Categorize]],\n    splits=splits\n)\n\n\ntype(dsrc)\n\nfastai.data.core.Datasets\n\n\n\nshow_at(dsrc.train, 3);\n\n\n\n\n\nExploring what each tfms does\n\nPILImageBW.create(fnames[0])\n\n\n\n\n\nl = parent_label(fnames[0])\nl\n\n'4'\n\n\n\nCategorize(l)\n\nCategorize -- {'vocab': ['4'], 'sort': True, 'add_na': False}:\nencodes: (Tabular,object) -> encodes\n(object,object) -> encodes\ndecodes: (Tabular,object) -> decodes\n(object,object) -> decodes\n\n\n\n\n\nData Loaders\n\ndls = dsrc.dataloaders(\n    bs=128,\n    after_item=item_tfms,\n    after_batch=batch_tfms\n)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n128\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Raw PyTorch",
    "text": "Raw PyTorch\nJust as the previous fastai examples, this is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code. > This example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\nfrom torchvision.transforms import ToTensor, Normalize, Pad, RandomCrop\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n\nTrain and validation Datasets\n\nclass MNISTDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential=None):       \n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]     \n        self.to_tensor = ToTensor()\n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return int(label)\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\ntrain_transforms = nn.Sequential(\n    Pad(6),\n    RandomCrop(28)\n)\n\n\ntrain_dataset = MNISTDataset(path/'training', train_transforms)\nvalid_dataset = MNISTDataset(path/'testing')\nlen(train_dataset), len(valid_dataset)\n\n(60000, 10000)\n\n\nOne of the items in the dataset:\n\nx,y = train_dataset[0]\nx.shape, y\n\n(torch.Size([1, 28, 28]), 4)\n\n\n\n\nPyTorch Dataloader\n\nbs = 128\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=False,\n    batch_size=bs\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=bs*2\n)\n\n\nnext(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[1].shape\n\n(torch.Size([128, 1, 28, 28]), torch.Size([128]))\n\n\n\nfigure = plt.figure(figsize=(4, 4))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\nThe next cell is just a curiosity leaned from twitter:\n\na = [1,2,3]\nb = [4,5,6]\n[i for i in (*a,*b)]\n\n[1, 2, 3, 4, 5, 6]"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe ended with the same fastaiâ€™s DataLoaders in three different ways, and then in an extra raw PyTorch way.\nAs Zachary puts it, the higher the level of the API, the lesser the flexibility, but there is also less complexity.\nHe also pointed out that the mid level API is made with the building blocks of the framework."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 2. Deploying the web app to Streamlit"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to take the SQLite database with the searchable index we created in Part 1 and use it as the search engine for a web app we are going to deploy to Streamlit.\nSo we are going to be able to search over the entire fastai channel.\nThe Python file was created with a Jupyter Notebook using nbdev.\nThe web app is just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)\nCreate A ðŸ¤— Space From A Notebook"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Creating app.py",
    "text": "Creating app.py\n#|export\nimport streamlit as st\nimport sqlite3\n#| export\ndb_path = '/mnt/m/datamatica/posts/full-text-search-fastai-youtube-channel/'\n\ntry:\n    db = sqlite3.connect(db_path + 'fastai_yt.db')\nexcept:\n    db = sqlite3.connect('fastai_yt.db')\n\ncur = db.cursor()\n#| export\nplaylist = cur.execute('SELECT playlist_id, playlist_name FROM playlist').fetchall()\nvideo = cur.execute('SELECT video_id, video_name FROM video').fetchall()\nplaylist = {p: n for p, n in playlist}\nvideo = {p: n for p, n in video}\npl_sel = list(playlist.values())\npl_to_id = {v:k for k,v in playlist.items()}\n#| export\nst.title('Full-Text Search fastai Youtube Playlists')\n\n# https://discuss.streamlit.io/t/select-all-on-a-streamlit-multiselect/9799/2\n\nall_options = st.checkbox(\"Select all Playlists\",\n    key='sel_all', value=True)\n\ncontainer = st.container()\nif all_options:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, disabled=True)\nelse:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, pl_sel)\n\nif all_options: options = list(playlist.values())\nelse: options = sel_options\n\nst.write('Selected playlist(s):', options)\n#| export\ndef get_query(q, limit):\n    \n    search_in = 'text'\n    \n    if not( len(options)==len(pl_sel) or len(options)==0 ):\n        search_in = 'transcriptions_fts'\n        q_pl = '(playlist_id: '\n        for pl in options:\n            end = ' OR ' if pl != options[-1] else ')'\n            q_pl = q_pl + f'\"{pl_to_id[pl]}\"' + end\n        \n        q = f\"(text: {q}) AND \" + q_pl\n\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    return query\n\nwith st.form(\"Input\"):\n    queryText = st.text_area(\"Search query: \\ne.g. Â«fastc*Â», Â«fastcore OR paral*Â»\", height=1, max_chars=None)\n    limit_val = st.slider(\"Number of results:\", min_value=5, max_value=20)\n    btnResult = st.form_submit_button('Search')\n    \nif btnResult:\n    if not queryText:\n        st.text('Please enter a search query.')\n    else:\n        try:\n            st.text('Search query generated:')\n            # run query\n            st.write(get_query(queryText, limit_val).replace('*', '\\*'))\n            res = cur.execute(get_query(q=queryText, limit=limit_val)).fetchall()\n            st.text('Search results (click to go to YouTube):')\n\n            res_md = ('  \\n  '.join(['  \\n  '.join([\n                f\"{i}.- Playlist: `{playlist[each[0]]}`, Video: `{video[each[1]]}`\", \n                f\"Caption: '...[{each[4].replace('[','**'\n                ).replace(']','**')}](https://youtu.be/{each[1]}?t={str(int(each[2]))})...'\", \n                '\\n'])\n                for i, each in enumerate(res)\n            ]))\n\n            st.markdown(res_md)\n        except:\n            st.text('Invalid search query.')\n#| hide\nfrom nbdev.export import nb_export\nnb_export('_Deploy_Search_Engine_Streamlit.ipynb', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 1. Extracting transcriptions and creating SQLiteâ€™s searchable index"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to get the transcriptions of YouTube videos from one or more given Playlists. Here we are going to do it for fastai channel, but it can be done for any given list of playlists (if the videos have transcriptions).\nAfter we get the transcriptions, we are going to build a search engine with SQLiteâ€™s full-text search functionality provided by its FTS5 extension.\nIn Part 2 we are going to build and share the search engine as a Streamlit web app, just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Get YouTube Transcriptions",
    "text": "Get YouTube Transcriptions\n\nInstall and Import Libraries\nWe need to first install the libraries we need (pytube and youtube-transcript-api).\nWe can use pip:\n$ pip install pytube\n$ pip install youtube_transcript_api\nOr conda:\n$ conda install -c conda-forge pytube\n$ conda install -c conda-forge youtube-transcript-api\n\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom pytube import YouTube, Playlist\n\nimport sqlite3\n\n\n\nYouTube Playlists\nLetâ€™s create a list of YouTube playlist ids. We can get them browsing YouTube playlist. The id is in the url which has the following format:\nhttps://www.youtube.com/playlist?list={PLAYLIST_ID}\n\nbase_pl = 'https://www.youtube.com/playlist?list='\nbase_yt = 'https://youtu.be/'\n\nyt_pl_ids = [\n    'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', # fast.ai APL Study Group #2022\n    'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU', # Practical Deep Learning for Coders 2022\n    'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM', # fast.ai live coding & tutorials #2022\n    'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM', # Practical Deep Learning for Coders (2020)\n    'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj', # Deep Learning from the Foundations #2019\n    'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq', # fastai v2 code walk-thrus #2019\n    'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn', # Practical Deep Learning for Coders 2019\n    'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96', # Introduction to Machine Learning for Coders\n    'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia', # Cutting Edge Deep Learning for Coders 2 #2018\n    'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM', # Practical Deep Learning For Coders 2018\n]\n\n\n\nGet Transcriptions\nLetâ€™s explore the methods:\n\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU')\nprint(playlist.title)\nvideo = YouTube(playlist[0])\nprint(video.title)\nprint(playlist[0])\nvideo_id = playlist[0].split('=')[1]\nscript = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\nprint(script[0])\n\nPractical Deep Learning for Coders 2022\nLesson 1: Practical Deep Learning for Coders 2022\nhttps://www.youtube.com/watch?v=8SF_h3xF3cE\n{'text': 'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0', 'start': 2.0, 'duration': 8.0}\n\n\n\nDownload all transcriptions\nNow we are going to download all the transcriptions. Letâ€™s create three dictionaries to store the data: - playlists to store each playlist as {playlist_id: playlist_name} - videos to store videos as {video_id: video_name} - database to store all captions as {playlist_id: {video_id: {'start': caption}}.\n\nplaylists = dict()\nvideos = dict()\ndatabase = dict()\n\nfor pl_id in yt_pl_ids:\n    playlist = Playlist(base_pl + pl_id)\n    print(playlist.title)\n    playlists[pl_id] = playlist.title\n    database[pl_id] = dict()\n\n    for video in playlist:\n        video_id = video.split(\"=\")[1]\n        videos[video_id] = YouTube(video).title\n        database[pl_id][video_id] = dict()\n        # Manually created transcripts are returned first\n        script = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\n\n        for txt in script:\n            database[pl_id][video_id][txt['start']] = txt['text']\n\nfast.ai APL Study Group\nPractical Deep Learning for Coders 2022\nfast.ai live coding & tutorials\nPractical Deep Learning for Coders (2020)\nDeep Learning from the Foundations\nfastai v2 code walk-thrus\nPractical Deep Learning for Coders 2019\nIntroduction to Machine Learning for Coders\nCutting Edge Deep Learning for Coders 2\nPractical Deep Learning For Coders 2018"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Building the Search Engine",
    "text": "Building the Search Engine\n\nFormatting the data to facilitate insertion into SQLite\n\n# https://stackoverflow.com/a/60932565/10013187\nrecords = [\n    (level1, level2, level3, leaf)\n    for level1, level2_dict in database.items()\n    for level2, level3_dict in level2_dict.items()\n    for level3, leaf in level3_dict.items()\n]\nprint(\"(playlist_id, video_id, start, text)\")\nprint(records[100])\n\n(playlist_id, video_id, start, text)\n('PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', 'CGpR2ILao5M', 294.18, 'gonna go watch them or anything all')\n\n\n\n\nCreating the database\n\ndb = sqlite3.connect('fastai_yt.db')\ncur = db.cursor()\n\n\n# virtual table configured to allow full-text search\ncur.execute('DROP TABLE IF EXISTS transcriptions_fts;') \ncur.execute('CREATE VIRTUAL TABLE transcriptions_fts USING fts5(playlist_id, video_id, start, text, tokenize=\"porter unicode61\");')\n\n# dimension like tables\ncur.execute('DROP TABLE IF EXISTS playlist;')\ncur.execute('CREATE TABLE playlist (playlist_id, playlist_name);')\ncur.execute('DROP TABLE IF EXISTS video;')\ncur.execute('CREATE TABLE video (video_id, video_name);')\n\n<sqlite3.Cursor>\n\n\n\n# bulk index records\ncur.executemany('INSERT INTO transcriptions_fts (playlist_id, video_id, start, text) values (?,?,?,?);', records)\ncur.executemany('INSERT INTO playlist (playlist_id, playlist_name) values (?,?);', playlists.items())\ncur.executemany('INSERT INTO video (video_id, video_name) values (?,?);', videos.items())\ndb.commit()\n\nExample of a simple query:\n\ncur.execute('SELECT start, text FROM transcriptions_fts WHERE video_id=\"8SF_h3xF3cE\" LIMIT 5').fetchall()\n\n[(2.0,\n  'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0'),\n (11.44,\n  \"and it's the first new one we've done\\xa0\\nin two years. So, we've got a lot of\\xa0\\xa0\"),\n (15.12,\n  \"cool things to cover! It's amazing how much has\\xa0\\nchanged. Here is an xkcd from the end of 2015.\\xa0\\xa0\"),\n (28.0,\n  'Who here has seen xkcd comics before?\\xa0\\nâ€¦Pretty much everybody. Not surprising.\\xa0\\xa0'),\n (35.36,\n  \"So the basic joke here isâ€¦ I'll let you\\xa0\\nread it, and then I'll come back to it.\")]\n\n\nfastai_yt.db. Once we have the database populated, we can use it in any application we want without the need to get the transcriptions from YouTube."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Search queries",
    "text": "Search queries\n\ndef print_search_results(res):\n    for each in res:\n        print()\n        print(playlists[each[0]], \"->\", videos[each[1]])\n        print(f'\"... {each[4]}...\"')\n        print('https://youtu.be/' + each[1] + \"?t=\" + str(int(each[2])))\n\ndef get_query(q, limit):\n    search_in = 'text'\n    if 'text:' in q: search_in = 'transcriptions_fts'\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    print(query)\n    return query\n\n\nSearch for a word\n\nq = 'fastc*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'fastc*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfast.ai live coding & tutorials -> Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -> Live coding 2\n\"... fastgen so [fastchan] is a channel that...\"\nhttps://youtu.be/0pWjZByJ3Lk?t=3720\n\n\n\nq = 'deleg*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'deleg*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #9\n\"... [delegated] down to that so [delegates] down...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2462\n\nDeep Learning from the Foundations -> Lesson 9 (2019) - How to train your model\n\"... [delegate] get attribute to the other...\"\nhttps://youtu.be/AcA8HAYh7IE?t=6435\n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #9\n\"... [delegate] everything Sodor in Python...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2304\n\nDeep Learning from the Foundations -> Lesson 13 (2019) - Basics of Swift for Deep Learning\n\"... default [delegates] is probably going to...\"\nhttps://youtu.be/3TqN_M1L4ts?t=6750\n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #2\n\"... this [delegates] decorator and what the...\"\nhttps://youtu.be/yEe5ZUMLEys?t=4756\n\n\n\n\nFaceted Search\nWe can limit the search to specific playlists in a faceted like search.\n\nplaylists\n\n{'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU': 'fast.ai APL Study Group',\n 'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU': 'Practical Deep Learning for Coders 2022',\n 'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM': 'fast.ai live coding & tutorials',\n 'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM': 'Practical Deep Learning for Coders (2020)',\n 'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj': 'Deep Learning from the Foundations',\n 'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq': 'fastai v2 code walk-thrus',\n 'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn': 'Practical Deep Learning for Coders 2019',\n 'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96': 'Introduction to Machine Learning for Coders',\n 'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia': 'Cutting Edge Deep Learning for Coders 2',\n 'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM': 'Practical Deep Learning For Coders 2018'}\n\n\n\npl_lst = list(playlists.keys())\n\n\n# Search in playlist 'Practical Deep Learning for Coders 2022' or\n# 'fast.ai live coding & tutorials'\nq = f\"\"\"\n(text: fastcore OR paral*) AND \n(playlist_id: \"{pl_lst[1]}\" OR \"{pl_lst[2]}\")\n\"\"\"\nres = cur.execute(get_query(q, limit=10)).fetchall()\n\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE transcriptions_fts MATCH '\n(text: fastcore OR paral*) AND \n(playlist_id: \"PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU\" OR \"PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM\")\n' ORDER BY rank\n    LIMIT \"10\" \n    \n\nPractical Deep Learning for Coders 2022 -> Lesson 6: Practical Deep Learning for Coders 2022\n\"... but my [fastcore] library has a [parallel] sub moduleÂ \nwhich can basically do anything that you can doÂ Â ...\"\nhttps://youtu.be/AdhG64NF76E?t=3799\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1155\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1160\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... and import [fastcore] it can't find it...\"\nhttps://youtu.be/B6BQiIgiEks?t=3401\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... for [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1049\n\nfast.ai live coding & tutorials -> Live coding 15\n\"... somewhat in [parallel]...\"\nhttps://youtu.be/6JGoes9_bPs?t=5589"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Conclusions",
    "text": "Conclusions\n\nWe used youtube-transcript-api and pytube Python libraries to extract YouTube captions based on the given playlists.\nWe indexed the captions using the capabilities of the ubiquitous SQLite and FTS5.\nWe did some powerful full-text search queries and simulated a faceted search.\nWe can go exactly to the video part the search is returning.\nIn Part 2 we are going to deploy an web app to Streamlit."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "",
    "text": "Part 1. Create the service and upload the pictures"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\nWe are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 will cover:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "References",
    "text": "References\n\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.1. Create a Custom Vision Service",
    "text": "Part 1.1. Create a Custom Vision Service\nIâ€™m not going to get into the details of creating the service. And the reason is that there is a detailed tutorial covering not just that, but also the code for uploading and training a simple model. I encourage you to try it first:\nDetect Objects in Images with Custom Vision\n\nFor this tutorial I created a Custom Vision with the following settings:\n\nCustom Vision service:\n\nResource: ai102cvision\nResource Kind: Custom Vision Training\n\nProject:\n\nName: Telecom Equipment Detection\nDescription: Detect different types of antennas\nResource: ai102cvision [F0]\nProject Types: Object Detection\nDomains: General"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.2. Upload the images",
    "text": "Part 1.2. Upload the images\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nTrainingEndpoint=YOUR_TRAINING_ENDPOINT\nTrainingKey=YOUR_TRAINING_KEY\nProjectID=YOUR_PROJECT_ID\n\n\n\n\n\n\nNote\n\n\n\nIn order to protect my credentials, Iâ€™m going to store .env file in a creds folder that isnâ€™t being pushed to github.\n\n\n\nDOTENV_PATH = './.env'\n\n\n\nInstall and import libraries\nWe need to install Custom Visionâ€™s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nFunctions\n\n# Borrowed from fastai library\ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\n    #except PIL.UnidentifiedImageError:\n\nThe SDK / API allows to upload images in batches but I didnâ€™t find a way to match the local image name with the id generated by the service. Then I opted to create a function that uploads the pictures one by one.\n\nImageFileCreateEntry\nCustomVisionTrainingClient.create_images_from_files()\n\n\ndef Upload_Images_1by1(pictures: list[Path]) -> list('dict'):\n    \"\"\"Upload the pictures from a list of paths,\n    one by one to keep track of the relation between\n    local image name and Azure image id.\n    And to track the ones that python fails opening\n    \"\"\"\n    print(\"Uploading images...\")\n\n    processed_ids = []\n    processed_status = []\n    picture_names = []\n\n    for pic_path in pictures:\n\n        if verify_image(pic_path):\n            with open(pic_path, mode=\"rb\") as image_data:\n                image_entry = ImageFileCreateEntry(\n                    name=pic_path.name, contents=image_data.read()\n                )\n            \n            # Upload the list of (1) images as a batch\n            upload_result = training_client.create_images_from_files(\n                custom_vision_project.id, \n                # Creates an ImageFileCreateBatch from a list of 1 ImageFileCreateEntry\n                ImageFileCreateBatch(images=[image_entry])\n            )\n            # Check for failure\n            if not upload_result.is_batch_successful:\n                pic_status = upload_result.images[0].status\n                pic_id = None\n            else:\n                pic_status = upload_result.images[0].status\n                pic_id = upload_result.images[0].image.id\n        else:\n            pic_status = \"ReadError\" # Equivalente to SDK `ErrorSource`\n            pic_id = None\n        \n        processed_status.append(pic_status)\n        processed_ids.append(pic_id)\n        picture_names.append(pic_path.name)\n        print(pic_path.name, \"//\", pic_id, \"//\", pic_status)\n    \n    return {\"image_name\": picture_names, \n            \"image_id\": processed_ids, \n            \"image_status\": processed_status}\n\n\n\nExplore pictures\n\npics_folder = Path('./train_images')\n\npictures = sorted(list(pics_folder.iterdir()))\n\nprint(f\"There are {len(pictures)} pictures\")\n\nThere are 203 pictures\n\n\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\nfor i, ax in enumerate(axes.flat):\n    im = Image.open( pictures[i*10] )\n    ax = show_img(im, ax=ax)\n\n\n\n\nAs you can see the pictures are very varied. Different cameras, lighting conditions, focus, resolutions and sizesâ€¦\n\n\nUpload the pictures to Custom Vision Service\n\nuploaded_images_df = pd.DataFrame(columns=[\"image_name\", \"image_id\", \"image_status\"])\n\n\nupload_batch = Upload_Images_1by1(pictures)\n\n\nuploaded_images_df = pd.DataFrame(upload_batch)\nuploaded_images_df\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      198\n      torre cerro el pavon 075.jpg\n      b6dd061a-a68d-4d91-a39f-711968445571\n      OK\n    \n    \n      199\n      torre cerro el pavon 080.jpg\n      d12264cf-3d7b-469c-9445-da8dce8dabef\n      OK\n    \n    \n      200\n      torre cerro el pavon 085.jpg\n      c6d587fe-5f3a-46ea-bc04-7ff54f10b4ae\n      OK\n    \n    \n      201\n      torre cerro el pavon 086.jpg\n      ea34cbad-8d50-4b5f-aed0-91d7fe40a754\n      OK\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n203 rows Ã— 3 columns\n\n\n\n\nprint(f\"{sum(uploaded_images_df.image_status != 'OK')} \n      images failed when uploading\")\n\n0 images failed uploading\n\n\nSave a csv:\n\nuploaded_images_df.to_csv('20221012_203_Images_Uploaded.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.3. Explore Data from Custom Vision Service",
    "text": "Part 1.3. Explore Data from Custom Vision Service\n\nGet idâ€™s of uploaded images\n\nCustomVisionTrainingClient.get_images()\n\n\ntrain_images = training_client.get_images(\n    project_id=custom_vision_project.id,\n    take=250,\n    skip=0\n)\n\n\nprint(f\"There are {len(train_images)} training images in the service.\")\nprint(f\"Each image has a type of {type(train_images[0])}.\")\n\nThere are 203 training images in the service.\nEach image has a type of <class 'azure.cognitiveservices.vision.customvision.training.models._models_py3.Image'>.\n\n\nSome properties of the image class:\n\nimage = train_images[0]\nprint(f\"image.id: {image.id}\")\nprint(f\"image.width: {image.width}\")\nprint(f\"image.height: {image.height}\")\n\nimage.id: 6e274dfc-411a-4bf3-9151-51b96f662248\nimage.width: 1188\nimage.height: 900\n\n\n\nimage.original_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/o-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=ru8DNhvBrpA46oZtmzNP7CRHSkwGugumb3R%2F3IzJaUE%3D'\n\n\n\nimage.resized_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/i-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=U5UQ6tjjdLF5gZHFR6wrrWk8B0w9at4cIUeYyxylx2E%3D'\n\n\nOf course there are no tags yet:\n\nprint(f\"image.tags: {image.tags}\")\n\nimage.tags: None\n\n\n\n\nThe images are resized when uploaded\nLetâ€™s see the same image locally:\n\nuploaded_images_df[uploaded_images_df.image_id==image.id]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n\n\n\n\nlocal_image = uploaded_images_df[\n    uploaded_images_df.image_id=='6e274dfc-411a-4bf3-9151-51b96f662248'\n].image_name.item()\nlocal_image\n\n'torre cerro el pavon 087.jpg'\n\n\n\nim = Image.open(pics_folder / local_image)\nim.size\n\n(2576, 1952)\n\n\nThe local image has a size of (2576, 1952) and was resized to (1188, 900) by the service\n\n\nKeep track of original size vs.Â size in the service\nTo get the real width and height we need to consider EXIF metadata. Thatâ€™s because local images are sometimes rotated by the viewer with some app viewer.\n\nSize of local images\n\n# The image has some EXIF meta data including information about orientation (rotation)\n# https://stackoverflow.com/a/63950647\n    \nlocal_w = []\nlocal_h = []\n\nfor image in uploaded_images_df.image_name:\n    im = Image.open(pics_folder / image)\n    im = ImageOps.exif_transpose(im)\n\n    local_w.append(im.size[0])\n    local_h.append(im.size[1])\n\n\nlocal_w[:5], local_h[:5]\n\n([640, 1620, 1620, 2160, 2160], [480, 2160, 2160, 1620, 1620])\n\n\n\nuploaded_images_df['ori_w'] = local_w\nuploaded_images_df['ori_h'] = local_h\nuploaded_images_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n    \n  \n\n\n\n\n\n\nSize of images in the service\n\nservice_ids = [im.id for im in train_images]\nservice_w = [im.width for im in train_images]\nservice_h = [im.height for im in train_images]\n\n\nservice_w = {id: w for id, w in zip(service_ids, service_w)}\nservice_h = {id: h for id, h in zip(service_ids, service_h)}\n\nuploaded_images_df['train_w'] = uploaded_images_df['image_id'].map(service_w)\nuploaded_images_df['train_h'] = uploaded_images_df['image_id'].map(service_h)\n\n\n\n\nChecking consistency in the ratios\n\nori_ratio = uploaded_images_df.ori_w / uploaded_images_df.ori_h\ntrain_ratio = uploaded_images_df.train_w / uploaded_images_df.train_h\nall(abs(ori_ratio - i_ratio) > .3)\n\nFalse\n\n\nImages that has an inconsistent ratio:\n\nuploaded_images_df[abs(ori_ratio - train_ratio) > 0.1]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      480\n      640\n      640\n      480\n    \n  \n\n\n\n\n\nim = Image.open( pics_folder / 'TORRE EL TIGRITO 01.jpg' )\nshow_img(im);\n\n\n\n\n\nim.size\n\n(640, 480)\n\n\nImageOps.exif_transpose failed for this image.\nBut if you donâ€™t use it, more images would be inconsistent.\nIf seems that exif_transpose keep track of manually rotated images.\n\nim = ImageOps.exif_transpose(im)\nim.size\n\n(480, 640)\n\n\n\nfilter = uploaded_images_df.image_id == '2563fffe-d621-4799-8e81-6ad57049cdaa'\nuploaded_images_df.loc[filter, ['ori_w', 'ori_h']] = (640, 480)\nuploaded_images_df[filter]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\n\nExporting csv with size data\n\nuploaded_images_df.sample(10)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      155\n      P1100700.JPG\n      b10efb57-70a4-48d6-a846-121ded4546f8\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      7\n      CUMACA 11.jpg\n      2c55467b-5de5-4329-91d2-a2fafdedd080\n      OK\n      2592\n      1944\n      1200\n      900\n    \n    \n      49\n      IMG_1170.JPG\n      ce2177ae-d03e-4a61-9dfb-4229542572fe\n      OK\n      480\n      640\n      480\n      640\n    \n    \n      141\n      MVC-024S.JPG\n      9ba84daa-e00c-4975-a07b-3ae23ef8f884\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      136\n      Imagen008.jpg\n      c861b4de-127a-4dc0-84ea-9cb96fb380f2\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n      2576\n      1952\n      1188\n      900\n    \n    \n      145\n      P1100611.JPG\n      1148d437-fc44-4c51-af4a-4751e242b3b7\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      147\n      P1100613.JPG\n      c9dab11e-0663-42f8-8c93-4e2351b15d4c\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      171\n      PICT0386.JPG\n      0e51a561-b938-48f6-8bc6-3c3bf4c72c44\n      OK\n      2560\n      1920\n      1200\n      900\n    \n    \n      142\n      MVC-025S.JPG\n      a8c2a746-2a65-4872-b7b5-0bd5edf965c9\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\nuploaded_images_df.to_csv('20221015_203_Images_Uploaded_WxH.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Conslusions",
    "text": "Conslusions\n\nIt was straightforward to upload images to the service.\nBig images got resized, but their ratios were kept.\nexif_transpose needs to be used to get the real width and height of the image, which may be different to the original size. For example when the image is rotated manually when looking at it. But somehow it failed with one of the images."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "",
    "text": "Part 2. Label images with Smart Labeler"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\n-> We are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "References",
    "text": "References\n\nCustom Vision Documentation: Label images faster with Smart Labeler\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.1. Labeling the Images",
    "text": "Part 2.1. Labeling the Images\nSmart Labeler is a simple tool for labeling images. It can be used for classification and object detection problems. When working in this problem I missed the ability to zoom-in when labeling some small objects, but as I said, this is a straightforward tool.\nFor speeding up bigger projects it might be usefull that you can first label some pictures, then train and get suggestions for the untagged images, but I didnâ€™t use it. By default the labeler tries to give suggestions even without that first training.\nThe process is simple and you can the use the annotation to train models outside the service (as we are going to try after this series, hopefully, using fastai).\n\nInstall and import libraries\nWe need to install Custom Visionâ€™s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region, ImageRegionCreateEntry\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nDOTENV_PATH = './.env'\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nCreating Labels\nSince I already did the manual tagging, we can use those tags in this new project.\nFirst we need to create the labels/tags in the service: - CustomVisionTrainingClient.create_tag()\n\ntags = ['Grid', 'Panel', 'Radome', 'RRU', 'Shroud', 'Solid']\ndesc = ['Grid Antenna', 'Panel Cel. Antenna', 'Radome Antenna', \n        'RRU Equipment', 'Shroud Antenna', 'Solid Antenna']\n\n\nservice_tags = []\nfor i, tag in enumerate(tags):\n    service_tags.append(\n        training_client.create_tag(\n            project_id=project_id, name=tag,\n            description=desc[i]\n        )\n    )\nservice_tags\n\n[<azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>]\n\n\nNow we can see this in the service:\n\n\nCustomVisionTrainingClient.get_tags()\n\n\nservice_tags = training_client.get_tags(project_id=project_id)\n\n\nservice_tag_ids = {tag.name: tag.id for tag in service_tags}\nservice_tag_ids\n\n{'RRU': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n 'Shroud': '4e413c15-141a-419b-a958-1485008b2904',\n 'Solid': '3f13d9b0-7b4d-4679-8fb8-7855cea0a118',\n 'Radome': 'a1020654-79c5-4d8a-867c-93dfb2a4a81d',\n 'Grid': 'e016b6a4-49e6-4897-a0c7-d8fc64d032f1',\n 'Panel': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e'}\n\n\n\n\nUpload Regions from json file\nAs I pointed before, you can create all the regions with Smart Labeler. Since I did that already in a previos project, I updated the regionâ€™s image ids and tags to the ones in this project and save them as a json.\n\nCustomVisionTrainingClient.create_image_regions()\n\nWe can see from the documentation that â€œThere is a limit of 64 entries in a batch.â€\n\nwith open(\"20221016_CreateImageRegions_Body.json\") as json_file:\n    regions_dict = json.load(json_file)\n\nprint(f'We have a total of {len(regions_dict[\"regions\"]):_} regions.')\nprint()\nprint('The first two regions:')\nregions_dict['regions'][:2]\n\nWe have a total of 1_279 regions.\n\nThe first two regions:\n\n\n[{'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n  'left': 0.6395582,\n  'top': 0.0,\n  'width': 0.10740108,\n  'height': 0.14776269},\n {'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e',\n  'left': 0.772766,\n  'top': 0.16059849,\n  'width': 0.22664931,\n  'height': 0.40633526}]\n\n\n\n# Create batches of 60 regions\n\nregions = regions_dict['regions']\n\nfor i in range(int(1_279 / 60)+1):\n    \n    batch_regions = []\n    print(f'Creating Regions {i*60+1:>{5}_} to {min((i+1)*60, 1_279):>{5}_}')\n    \n    for region in regions[i*60: (i+1)*60]:\n        batch_regions.append(\n            ImageRegionCreateEntry(\n                image_id=region['imageId'],\n                tag_id=region['tagId'],\n                left=region['left'], top=region['top'],\n                width=region['width'], height=region['height']\n        ))\n\n    training_client.create_image_regions(\n        project_id=project_id, \n        regions=batch_regions\n    )\n\nCreating Regions     1 to    60\nCreating Regions    61 to   120\nCreating Regions   121 to   180\nCreating Regions   181 to   240\nCreating Regions   241 to   300\nCreating Regions   301 to   360\nCreating Regions   361 to   420\nCreating Regions   421 to   480\nCreating Regions   481 to   540\nCreating Regions   541 to   600\nCreating Regions   601 to   660\nCreating Regions   661 to   720\nCreating Regions   721 to   780\nCreating Regions   781 to   840\nCreating Regions   841 to   900\nCreating Regions   901 to   960\nCreating Regions   961 to 1_020\nCreating Regions 1_021 to 1_080\nCreating Regions 1_081 to 1_140\nCreating Regions 1_141 to 1_200\nCreating Regions 1_201 to 1_260\nCreating Regions 1_261 to 1_279\n\n\nExample image, capture from the service:\n\n\n\nVerifying the number of created Regions\n\nCustomVisionTrainingClient.get_images()\n\n\nall_tagged_images = training_client.get_images(\n    project_id=project_id,\n    tagging_status=\"Tagged\", \n    take=250   # Max 256\n)\ni = 0\nfor im in all_tagged_images: i += len(im.regions)\nprint(f\"Number of created Regions: {i:_}\")\n\nNumber of created Regions: 1_279\n\n\n\n\nDraw some regions\n\nimages_df = pd.read_csv('20221015_203_Images_Uploaded_WxH.csv')\nimages_df.index = images_df.image_id\nimages_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n    \n      image_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n      1200\n      900\n    \n    \n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n      1200\n      900\n    \n  \n\n\n\n\nCreate a dictionary to easily access all regions from an image id:\n\nimg2ann = dict()\n\nfor image in all_tagged_images:\n    img2ann[image.id] = tuple([list(), list()])\n    image_w = image.width; image_h = image.height\n    ori_w = images_df.loc[image.id].ori_w\n    ori_h = images_df.loc[image.id].ori_h\n    for region in image.regions:\n        img2ann[image.id][1].append(region.tag_name)\n        img2ann[image.id][0].append([\n            region.left*ori_w, \n            region.top*ori_h, \n            region.width*ori_w, \n            region.height*ori_h\n        ])\n\n\npics_folder = Path('./train_images')\n\n\n# https://youtu.be/Z0ssNAbe81M?t=4636\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()\n    ])\n\ndef draw_rect(ax, b):\n    patch = ax.add_patch(\n        patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=1)\n    )\n    draw_outline(patch, 4)\n\ndef draw_text(ax, xy, txt, sz=14):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_regions(index=0):\n    im = Image.open( pics_folder / images_df.iloc[index].image_name )\n    ax = show_img(im, figsize=(8,8))\n\n    reg, lab = img2ann[images_df.iloc[index].image_id]\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(index=0)\n\n[[329.19859199999996, 205.3586496, 53.42696959999999, 114.2365248], [249.3986112, 264.75866399999995, 112.4269696, 85.23652799999999]]\n\n\n\n\n\nA dragon-fly was cought in that picture!\n\ndraw_regions(index=100)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.2. Train and Test a Model",
    "text": "Part 2.2. Train and Test a Model\n\nTrain the model\n\nCustomVisionTrainingClient.train_project()\n\n\ntrain_iteration = training_client.train_project(\n    project_id=project_id,\n    training_type='Regular'\n)\n\n\ntrain_iteration.as_dict()\n\n{'id': 'd0006e20-33dd-4806-9fe9-cfc3fca82552',\n 'name': 'Iteration 1',\n 'status': 'Training',\n 'created': '2022-10-12T13:34:38.120Z',\n 'last_modified': '2022-10-22T14:56:30.406Z',\n 'project_id': 'f6cb4ba7-5bbe-46a4-8836-69654dc86f3a',\n 'exportable': False,\n 'training_type': 'Regular',\n 'reserved_budget_in_hours': 0,\n 'training_time_in_minutes': 0}\n\n\n\nCustomVisionTrainingClient.get_iteration_performance()\n\n\nperformance = training_client.get_iteration_performance(\n    project_id=project_id,\n    iteration_id=train_iteration.id\n).as_dict()\n\n   \nfor tag in performance['per_tag_performance']:\n    print('/'*20)\n    print('tag:', tag['name'])\n    print('image count:', training_client.get_tag(\n        project_id=project_id, tag_id=service_tags[tag['name']]\n    ).image_count)\n    print('recall:', tag['recall'])\n    print('average_precision:', tag['average_precision'])\n\n////////////////////\ntag: Shroud\nimage count: 140\nrecall: 0.35789475\naverage_precision: 0.7280897\n////////////////////\ntag: Panel\nimage count: 68\nrecall: 0.11392405\naverage_precision: 0.3710658\n////////////////////\ntag: Solid\nimage count: 88\nrecall: 0.21428572\naverage_precision: 0.4641156\n////////////////////\ntag: Grid\nimage count: 80\nrecall: 0.10526316\naverage_precision: 0.3784035\n////////////////////\ntag: Radome\nimage count: 20\nrecall: 0.0\naverage_precision: 0.051538005\n////////////////////\ntag: RRU\nimage count: 32\nrecall: 0.13043478\naverage_precision: 0.48053658\n\n\nSome things that I would take into account now that negatively impact the model performance: - I choose many images with small boxes. - Some tags are not represented equally, so we ended an unbalanced distribution. - And of course lets remember we only did a quick train.\nThis is a very good thread on some tips and tricks to improve object detection:\n\n\nðŸ˜¨ Training an Object Detection Model is a very challenging task and involves tweaking so many knobsHere is an exhaustive ðŸŽ tips & tricks list ðŸŽ that you could use to boost your model performance ðŸ§µ pic.twitter.com/sOvEUhCCwg\n\nâ€” AI Fast Track (@ai_fast_track) October 20, 2022\n\n\n\n\nTest the model (Quick Test)\nQuick test allows to test the model without publishing a prediction API.\n\n# Load image and get height, width and channels\nimage_file = Path(\"./test_images/las-palmas-at-60-(20).jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\nCustomVisionTrainingClient.quick_test_image()\n\n\n# Detect objects in the test image\nprint('Detecting objects in', image_file)\n\nwith open(image_file, mode=\"rb\") as image_data:\n    results = training_client.quick_test_image(\n        project_id=project_id, \n        image_data=image_data,\n        iteration_id=train_iteration.id\n    )\n\nDetecting objects in test_images/las-palmas-at-60-(20).jpg\n\n\n\ndef get_reg_lab(results):\n    reg = []; lab = []\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            left = prediction.bounding_box.left * w\n            top = prediction.bounding_box.top * h\n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n            reg.append([left, top, width, height])\n            lab.append(prediction.tag_name)\n    return reg, lab\n\n\ndef draw_regions(image):\n    \n    ax = show_img(image, figsize=(8,8))\n    reg, lab = get_reg_lab(results)\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(image)\n\n\n\n\nAs you can see, it didnâ€™t detect some antennas. But taking into account we did a regular training and the limitations mentioned in the training data, it is impressive that it got some right in such a complex problem as object detection.\n\nimage_file = Path(\"./test_images/DSC09399.jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\ndraw_regions(image)\n\n\n\n\nNot a good job in this one. But this result is with a â€œregularâ€ training (quick).\nYou can see in the Gradio demo Telecom Object Detection with Azure Custom Vision that the model trained for 1 hour (free tier limit) does a better job with this picture."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Conslusions",
    "text": "Conslusions\n\nObject detection is a complex problem. The fact that the service does a reasonable good job with unbalanced training photos and with such a limited training time talks about the great margin for improvement."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "",
    "text": "Part 3. Deploy Gradio Web App"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Introduction",
    "text": "Introduction\nAfter we train the model in Azure for 1 hour (free tier) and publishing it, when end up with a Prediction URL.\n\nWe are going to use that Prediction endpoint to do the inference.\nHere is the App already published for you to try:\nTelecom-Object-Detection\nAnd here the repository:\nhttps://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 covered:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "References",
    "text": "References\n\nCreate A ðŸ¤— Space From A Notebook\nBuild & Share Delightful Machine Learning Apps"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Part 3.1. Publishing a Gradio App",
    "text": "Part 3.1. Publishing a Gradio App\nGradio is a great tool to demo machine learning models. The model is already deployed in Azure, so our Gradio App is going to be our front end to connect to that prediction endpoint. What I mean is that the model itself is not going to be deployed in Hugging Face Spaces, which is the normal workflow.\nIf you are new to Gradio, I encourage you to start from the Quickstart.\nThe Gradio demo was created from a Jupyter Notebook with a great tool from fast.ai which is nbdev. You can start learning the basics here: Create A ðŸ¤— Space From A Notebook\nIn both tutorials you will find the instructions to setup a Gradio enabled space in Hugging Face.\nThis code is based and adapted from:\n- https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py\n- https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py\n\nInstall and import libraries\n#|export\n\nimport gradio as gr\nimport numpy as np\nimport os\nimport io\n\nimport requests, validators\n\nfrom pathlib import Path\n\n\nAzure Functions\n#| export\n\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nfrom msrest.authentication import ApiKeyCredentials\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom dotenv import load_dotenv\n\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nPredictionEndpoint=YOUR_PREDICTION_ENDPOINT\nPredictionKey=YOUR_PREDICTION_KEY\nProjectID=YOUR_PROJECT_ID\nModelName=YOUR_PUBLISHED_MODEL\nWe need to create these environment variables in the Hugging Face Spaces repository under Settings -> Repo Secrets:\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n#| export\n\ndef fig2img(fig):\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n    return img\n    \ndef custom_vision_detect_objects(image_file: Path):\n    dpi = 100\n\n    # Get Configuration Settings\n    load_dotenv()\n    prediction_endpoint = os.getenv('PredictionEndpoint')\n    prediction_key = os.getenv('PredictionKey')\n    project_id = os.getenv('ProjectID')\n    model_name = os.getenv('ModelName')\n\n    # Authenticate a client for the training API\n    credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n    prediction_client = CustomVisionPredictionClient(\n        endpoint=prediction_endpoint, credentials=credentials)\n\n    # Load image and get height, width and channels\n    #image_file = 'produce.jpg'\n    print('Detecting objects in', image_file)\n    image = Image.open(image_file)\n    h, w, ch = np.array(image).shape\n\n    # Detect objects in the test image\n    with open(image_file, mode=\"rb\") as image_data:\n        results = prediction_client.detect_image(project_id, model_name, image_data)\n    \n    # Create a figure for the results\n    fig = plt.figure(figsize=(w/dpi, h/dpi))\n    plt.axis('off')\n\n    # Display the image with boxes around each detected object\n    draw = ImageDraw.Draw(image)\n    lineWidth = int(w/800)\n    color = 'cyan'\n\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            # Box coordinates and dimensions are proportional - convert to absolutes\n            left = prediction.bounding_box.left * w \n            top = prediction.bounding_box.top * h \n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n\n            # Draw the box\n            points = ((left,top), (left+width,top), \n                      (left+width,top+height), (left,top+height), \n                      (left,top))\n            draw.line(points, fill=color, width=lineWidth)\n\n            # Add the tag name and probability\n            plt.annotate(\n                prediction.tag_name + \": {0:.0f}%\".format(prediction.probability * 100),\n                (left, top-1.372*h/dpi), \n                backgroundcolor=color,\n                fontsize=max(w/dpi, h/dpi), \n                fontfamily='monospace'\n            )\n\n    plt.imshow(image)\n    plt.tight_layout(pad=0)\n    \n    return fig2img(fig)\n\n    outputfile = 'output.jpg'\n    fig.savefig(outputfile)\n    print('Resulabsts saved in ', outputfile)\n\n\nGradio\n#| export\n\ntitle = \"\"\"<h1 id=\"title\">Telecom Object Detection with Azure Custom Vision</h1>\"\"\"\n\ncss = \"\"\"\nh1#title {\n  text-align: center;\n}\n\"\"\"\nExample images and url to be used in the App\n#| export\n\nurls = [\"https://www.dropbox.com/s/y5bk8om5ucu46d3/747.jpg?dl=1\"]\nimgs = [path.as_posix() for path in sorted(Path('images').rglob('*.jpg'))]\nimg_samples = [[path.as_posix()] for path in sorted(Path('images').rglob('*.jpg'))]\nFunctions for the Gradio App\n#| export\n\ndef set_example_url(example: list) -> dict:\n    print(gr.Textbox.update(value=example[0]))\n    return gr.Textbox.update(value=example[0])\n\ndef set_example_image(example: list) -> dict:\n    return gr.Image.update(value=example[0])\n\ndef detect_objects(url_input:str, image_input:Image):\n    print(f\"{url_input=}\")\n    if validators.url(url_input):\n        image = Image.open(requests.get(url_input, stream=True).raw)\n    elif image_input:\n        image = image_input\n        \n    print(image)\n    print(image.size)\n    w, h = image.size\n    \n    if max(w, h) > 1_200:\n        factor = 1_200 / max(w, h)\n        factor = 1\n        size = (int(w*factor), int(h*factor))\n        image = image.resize(size, resample=Image.Resampling.BILINEAR)\n    \n    resized_image_path = \"input_object_detection.jpg\"\n    image.save(resized_image_path)\n    \n    return custom_vision_detect_objects(resized_image_path)\n#| export\n\nwith gr.Blocks(css=css) as demo:\n    \n    gr.Markdown(title)\n    \n    with gr.Tabs():\n        with gr.TabItem(\"Image Upload\"):\n            with gr.Row():\n                image_input = gr.Image(type='pil')\n                image_output = gr.Image(shape=(650,650))\n                \n            with gr.Row(): \n                example_images = gr.Dataset(components=[image_input], samples=img_samples)\n            \n            image_button = gr.Button(\"Detect\")\n        \n        with gr.TabItem(\"Image URL\"):\n            with gr.Row():\n                url_input = gr.Textbox(lines=2, label='Enter valid image URL here..')\n                img_output_from_url = gr.Image(shape=(650,650))\n                \n            with gr.Row():\n                example_url = gr.Dataset(components=[url_input], samples=[[str(url)] for url in urls])\n            url_button = gr.Button(\"Detect\")\n            \n    url_button.click(detect_objects, inputs=[url_input,image_input], outputs=img_output_from_url)\n    image_button.click(detect_objects, inputs=[url_input,image_input], outputs=image_output)\n    \n    example_url.click(fn=set_example_url, inputs=[example_url], outputs=[url_input])\n    example_images.click(fn=set_example_image, inputs=[example_images], outputs=[image_input])\n\ndemo.launch()\nTo publish the script app.py from the notebook:\n\nfrom nbdev.export import nb_export\nnb_export('ObjectDetectionWithAzureCustomVision_Part_3', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Conslusions",
    "text": "Conslusions\n\nGradio is great for publishing our demo Apps."
  }
]