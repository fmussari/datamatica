[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Francisco Mussari, blogging from Caracas about data, ai, cloud computing, engineering and social sciences."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "datamatica",
    "section": "",
    "text": "BigQuery\n\n\nBigQuery ML\n\n\nRecSys\n\n\nRecommender\n\n\nPipeline\n\n\nVertex AI\n\n\nMovielens\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbt\n\n\nData-Engineer-Camp\n\n\nDatawarehousing\n\n\nKimball\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\nfastai\n\n\nminiai\n\n\n\n\n\n\n\n\n\n\n\nMar 19, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\nfastai\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nWalk with fastai\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nMar 6, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\ndataloaders\n\n\nAPI\n\n\ntransforms\n\n\nWalk with fastai\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\nFrancisco Mussari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\nStreamlit\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ndeeplearning\n\n\nfull-text search\n\n\nSQLite\n\n\npytube\n\n\nyoutube-transcript-api\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\ngradio\n\n\nhugging face\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncomputer-vision\n\n\ndeeplearning\n\n\nazure\n\n\ncustom-vision\n\n\nobject-detection\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\nFrancisco Mussari\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "",
    "text": "The first attempt for developing this idea started as the homework assignment for lesson 1: Deep Learning 2019 (v3) fast.ai course.\nFor that homework I toke my domain expertise on telecommunication towers to build an image clasifier which could hopefuly recognize different tower components.\nIn may 2022 I was coursing live the 2022 version of the course, which is done in collaboration with The University of Queensland and its now called Practical Deep Learning for Coders.\nI’m running this notebook on an old GTX-1070 NVIDIA GPU."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#import-libraries",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom fastai.vision.all import *\nimport timm\nimport plotly.express as px"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataset",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataset",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Dataset",
    "text": "Dataset\nI curated an image dataset of multiple towers I worked with in the past several years. And choosed 514 images in 8 relatively “easy” to distinguish categories (components).\nThe dataset was stored in google drive and is shared here.\n\nLocal Dataset\nFor the 2022 course, thanks to the help of many great people in fastai forums, I was able to install fastai locally and to use my local GPU on WSL2.\n\npath = Path(\"photos\")\n\n\ntrain_path = path / 'train'\nvalid_path = path / 'valid'\n\nlabels = [label.parts[-1] for label in train_path.iterdir()]\ntrain_quantity = [len(list(each.iterdir())) for each in train_path.iterdir()]\nvalid_quantity = [len(list(each.iterdir())) for each in valid_path.iterdir()]\ndf = pd.DataFrame()\n\ndf['label'] = labels * 2\ndf['set'] = ['train'] * 8 + ['valid'] * 8\ndf['quantity'] = train_quantity + valid_quantity\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      label\n      set\n      quantity\n    \n  \n  \n    \n      0\n      base_plate\n      train\n      87\n    \n    \n      1\n      grounding_bar\n      train\n      52\n    \n    \n      2\n      identification\n      train\n      30\n    \n    \n      3\n      ladder\n      train\n      37\n    \n    \n      4\n      light\n      train\n      69\n    \n    \n      5\n      lightning_rod\n      train\n      33\n    \n    \n      6\n      platform\n      train\n      57\n    \n    \n      7\n      transmission_lines\n      train\n      29\n    \n    \n      8\n      base_plate\n      valid\n      29\n    \n    \n      9\n      grounding_bar\n      valid\n      15\n    \n    \n      10\n      identification\n      valid\n      8\n    \n    \n      11\n      ladder\n      valid\n      11\n    \n    \n      12\n      light\n      valid\n      22\n    \n    \n      13\n      lightning_rod\n      valid\n      10\n    \n    \n      14\n      platform\n      valid\n      16\n    \n    \n      15\n      transmission_lines\n      valid\n      9\n    \n  \n\n\n\n\n\nis_train = df.set == 'train'\ndf[is_train].quantity.sum(), df[~is_train].quantity.sum()\n\n(394, 120)\n\n\n\nfig = px.bar(\n    df, x=\"set\", y=\"quantity\",\n    color='label', barmode='group',\n    height=400\n)\nfig.show()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#the-data",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#the-data",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "The Data",
    "text": "The Data\nThe data was hand picked from a huge tower photoset. To start, I choose these eight easy distinguishable components to be clasified:\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nThere are two folders, one for the training (train) and the other for the validation set (valid).\n\nprint(path.ls())   \nprint('*'*100)\n(path/'train').ls()\n\n[Path('photos/train'), Path('photos/valid')]\n****************************************************************************************************\n\n\n(#8) [Path('photos/train/base_plate'),Path('photos/train/grounding_bar'),Path('photos/train/identification'),Path('photos/train/ladder'),Path('photos/train/light'),Path('photos/train/lightning_rod'),Path('photos/train/platform'),Path('photos/train/transmission_lines')]\n\n\n\ntower_parts_fns = get_image_files(path)\ntower_parts_fns\n\n(#514) [Path('photos/train/base_plate/Ac102-Corozopando-(64).jpg'),Path('photos/train/base_plate/Ac102-Corozopando-(75).jpg'),Path('photos/train/base_plate/camaguan-087.jpg'),Path('photos/train/base_plate/camaguan-098.jpg'),Path('photos/train/base_plate/cantv el yoco 015.JPG'),Path('photos/train/base_plate/cantv-capanaparo-011.jpg'),Path('photos/train/base_plate/cantv-cinaruco-018.jpg'),Path('photos/train/base_plate/cantv-cinaruco-025.jpg'),Path('photos/train/base_plate/cartanal-(7).jpg'),Path('photos/train/base_plate/CHUSPITA-II-AC-72-MTS-002.jpg')...]\n\n\n\nfailed = verify_images(tower_parts_fns)\nprint(failed)\n\n[]"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataloaders",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#dataloaders",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "DataLoaders",
    "text": "DataLoaders\n\ntower_parts = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=GrandparentSplitter(train_name='train', valid_name='valid'),\n    get_y=parent_label,\n    item_tfms=Resize(224)\n)\n\n\ndls = tower_parts.dataloaders(path)\n\n\n%%time\nfor _ in dls.train: pass\n\nCPU times: user 449 ms, sys: 506 ms, total: 955 ms\nWall time: 20.4 s\n\n\n\ndls.train.show_batch(max_n=16, nrows=4, figsize=(10,10))"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#learner",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#learner",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Learner",
    "text": "Learner\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n\nfastai’s lr_find\n\n%%time\nlearn.lr_find()\n\n\n\n\n\n\n\n\nCPU times: user 24.4 s, sys: 8.72 s, total: 33.2 s\nWall time: 5min 38s\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\ndls.num_workers\n\n1\n\n\n\n\n\n\n\n\nNote\n\n\n\nI experimented by setting dls.num_workers = 4 and it didn’t make any difference in the time it takes to run lr_find(), even though that, by watching at the progress bar, it seemed that the bottleneck was in pre-processing the batch. Not in GPU.\n\n\n\nlen(dls.train), len(dls.train.get_idxs())\n\n(6, 394)\n\n\n\nlen(dls.valid), len(dls.valid.get_idxs())\n\n(2, 120)\n\n\n\ndls.drop_last\n\nTrue"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#training",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#training",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Training",
    "text": "Training\n\n%%time\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      2.751579\n      0.932106\n      0.316667\n      00:44\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.000472\n      0.463910\n      0.166667\n      00:43\n    \n    \n      1\n      0.632878\n      0.272529\n      0.091667\n      00:43\n    \n    \n      2\n      0.429861\n      0.208375\n      0.050000\n      00:43\n    \n  \n\n\n\nCPU times: user 8.31 s, sys: 4.01 s, total: 12.3 s\nWall time: 2min 55s\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(8,8))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(9)\n\n\n\n\n\n\n\n\n\n\n\n\nlearn.export('models/tower_parts_model')\nlearn.save(\"exported_model_from_fastai\", with_opt=False)\n\nPath('models/exported_model_from_fastai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_1__Tower_Parts_Classisfier_with_fastai.html#conclusions",
    "title": "Telecommunication towers component clasification - Part 1. fastai",
    "section": "Conclusions",
    "text": "Conclusions\n\nI took about 500 pictures and 3 epochs to finetune a small pre-trained model to make it recognize 8 components with an error rate of about 6%.\nlearn.lr_find() took about 5 minutes to run, more that the fine tuning.\nThere are some categories that normally appear in one picture at the same time. I solved a classification problem to simplify, but maybe the actual problem should be a multi-class classificacion."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "",
    "text": "Zach Mueller proposed us to create a Gradio app as a homework for for the course Walk with fastai, the missing pieces for success.\nI had already deployed a Tower Classificacion App as a homework for Practical Deep Learning for Coders.\nHere the link to Part 1 post:\nTelecommunication towers component clasification - Part 1. fastai\nThe new thing here is that everythong but the training is going to be done using pure PyTorch, which was teched in Zach’s course."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#import-libraries",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom pathlib import Path\nfrom miniai.datasets import show_images\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport torch\nimport torchvision.transforms as tvtfms\n\nfrom PIL import Image\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#image-data",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#image-data",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Image Data",
    "text": "Image Data\nThere are 514 images in 8 relatively “easy” to distinguish categories (components).\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nYou can download the pictures here.\nThere are two folders, one for the training (train) and the other for validation set (valid).\n\npath = Path(\"photos\")\n[folder.stem for folder in path.iterdir()]\n\n['test', 'train', 'valid']\n\n\n\ntrain_path = path / \"train\"\nvalid_path = path / \"valid\"\n\nAnd in each folder there is one folder for each label.\n\nlabels = [folder.stem for folder in train_path.iterdir()]\nnumber_of_labels = len(labels)\nprint(labels)\n\n['base_plate', 'grounding_bar', 'identification', 'ladder', 'light', 'lightning_rod', 'platform', 'transmission_lines']\n\n\n\nint_to_label = {k:v for k,v in enumerate(labels)}\nlabel_to_int = {k:v for v,k in int_to_label.items()}\nprint(label_to_int)\n\n{'base_plate': 0, 'grounding_bar': 1, 'identification': 2, 'ladder': 3, 'light': 4, 'lightning_rod': 5, 'platform': 6, 'transmission_lines': 7}\n\n\n\nVisualizing the distribution of labels\n\nlabels = [label.parts[-1] for label in train_path.iterdir()]\ntrain_quantity = [len(list(each.iterdir())) for each in train_path.iterdir()]\nvalid_quantity = [len(list(each.iterdir())) for each in valid_path.iterdir()]\ndf = pd.DataFrame()\n\ndf['label'] = labels * 2\ndf['set'] = ['train'] * 8 + ['valid'] * 8\ndf['number of pics'] = train_quantity + valid_quantity\n\n\nfig = px.bar(\n    df, x=\"set\", y=\"number of pics\",\n    color='label', barmode='group',\n    height=400\n)\nfig.show()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-pytorch-dataset",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-pytorch-dataset",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Creating PyTorch Dataset",
    "text": "Creating PyTorch Dataset\n\nDataset Class\nThis class is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code.\n\nThis example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\n\nclass TowerPartsDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential, label_to_int:dict):\n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]\n        self.to_tensor = tvtfms.ToTensor()\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return label_to_int[label]\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\n\nItem Transforms\n\nfrom fastai.vision.data import imagenet_stats\nprint(imagenet_stats)\n\nitem_tfms = nn.Sequential(\n    tvtfms.Resize((224, 224)), \n    tvtfms.Normalize(*imagenet_stats)\n)\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nitem_tfms[0](Image.open(train_path / 'base_plate/CIMG4695.jpg'))\n\n\n\n\n\n\nTrain and validation datasets\n\ntrain_dataset = TowerPartsDataset(\n    train_path,\n    item_tfms,\n    label_to_int\n)\n\nvalid_dataset = TowerPartsDataset(\n    valid_path,\n    item_tfms,\n    label_to_int\n)\n\n\nx, y = train_dataset[0]\nx.shape, y\n\n(torch.Size([3, 224, 224]), 0)\n\n\n\n\nPreviewing transformed images\nThe warning occurs because the images has been normalized.\n\nfigure = plt.figure(figsize=(10, 10))\ncols, rows = 3, 3\n\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    label = int_to_label[label]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.permute(1, 2, 0))\n    \nplt.show()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#dataloader",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#dataloader",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "DataLoader",
    "text": "DataLoader\n\nbatch_size = 64\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=True,\n    batch_size=batch_size\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size\n)\n\n\n%%time\n# This should be the PyTorch dataloaders\nfor _ in train_dataloader: pass\n\nCPU times: user 6min 31s, sys: 38 s, total: 7min 9s\nWall time: 1min 22s"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-a-pytorch-model",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#creating-a-pytorch-model",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Creating a PyTorch Model",
    "text": "Creating a PyTorch Model\nWhen loading a model to fastai learner, it is customized by changing the last two children (the Head).\nWhat we are going to do here is to change only what is essential for our problem, that is, the final linear layer in order to have 8 features (the number of labels need to classify).\nFor more detail on this I encourage you to take Walk with fastai, the missing pieces for success\n\nfrom torchvision.models import resnet18, resnet34\nmodel = resnet34(pretrained=True)\n\n\nCustomizing the last linear layer\nThese are the two layers that are changed by fastai when creating the learner:\n\nmodel_child = list(model.children())\nmodel_child[-2:]\n\n[AdaptiveAvgPool2d(output_size=(1, 1)),\n Linear(in_features=512, out_features=1000, bias=True)]\n\n\nWe can access the original final layer with model.fc\n\nmodel.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\nIt has the 1,000 features resnet was trained for. We need to change it to the 8 features (labels, classes) of the problem we are dealing here:\n\nmodel.fc = nn.Linear(512, out_features=number_of_labels, bias=True)\n\n\nmodel.fc\n\nLinear(in_features=512, out_features=8, bias=True)\n\n\n\n\nGradual Unfreezing\nOne other thing fastai does is it freeze the backbone (Body) of the model. We can achieve that as follows:\n\n\"\"\"for layer in list(model.children())[:-1]:\n    if hasattr(layer, \"requires_grad_\"):\n        layer.requires_grad_(False)\"\"\";\n\n\n\n\n\n\n\nNote\n\n\n\nZach used gradual unfreezing and fit_one_cycle in Walk with fastai, the missing pieces for success. Here I’m going to use fine_tune instead to compare with results in Part 1."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#the-optimizer",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#the-optimizer",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "The Optimizer",
    "text": "The Optimizer\nLet’s use the same optimizer as fastai’s default: AdamW\n\nfrom torch.optim import AdamW\nfrom functools import partial\nfrom fastai.optimizer import OptimWrapper\n\n\nopt_func = partial(OptimWrapper, opt=AdamW)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#bringing-in-fastai-and-training",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#bringing-in-fastai-and-training",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Bringing in fastai and Training!",
    "text": "Bringing in fastai and Training!\n\nfrom fastai.losses import CrossEntropyLossFlat\nfrom fastai.metrics import accuracy, error_rate\nfrom fastai.learner import Learner\nfrom fastai.callback.schedule import Learner # To get `fit_one_cycle`, `lr_find`, and more\n\nfrom fastai.data.core import DataLoaders\n\n\nmodel.cuda();\n\n\ndls = DataLoaders(train_dataloader, valid_dataloader)\n\n\nlearn = Learner(\n    dls, \n    model, \n    opt_func=opt_func, \n    loss_func=CrossEntropyLossFlat(), \n    metrics=error_rate\n)\n\n\n%%time\nlearn.lr_find()\n\n\n\n\n\n\n\n\nCPU times: user 1h 46min 47s, sys: 12min 46s, total: 1h 59min 33s\nWall time: 22min 55s\n\n\nSuggestedLRs(valley=5.248074739938602e-05)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Part 1 it took 5 minutes for lr_find() to run, and the suggested lr was about 0.001. Now, with PyTorch’s Dataloaders things got very different. Should keep digging in the causes.\n\n\n\nFine Tuning\n\nfine_tuning is geared towards transfer learning specifically. “fine_tune” vs. “fit_one_cycle”\n\nfine_tune does one epoch to train only the last layer (the linear layer we just modified). It means that the parameter of all the other layers are not changed, are “freezed”. But after that epoch, all the model is “unfreezed” and trained. So the parameters of all its layers are updated or optmized.\n\n%%time\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.041510\n      1.549582\n      0.283333\n      02:02\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.095129\n      1.055718\n      0.208333\n      01:59\n    \n    \n      1\n      0.171666\n      2.795403\n      0.300000\n      02:02\n    \n    \n      2\n      0.146215\n      0.879028\n      0.166667\n      02:01\n    \n  \n\n\n\nCPU times: user 35min 42s, sys: 4min 46s, total: 40min 29s\nWall time: 8min 7s\n\n\n\nlearn.fit(2, lr=5.25e-05)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.047634\n      0.423191\n      0.116667\n      02:00\n    \n    \n      1\n      0.033922\n      0.314130\n      0.100000\n      01:58\n    \n  \n\n\n\n\nfrom fastai.interpret import ClassificationInterpretation\n\ntry:\n    interp = ClassificationInterpretation.from_learner(learn)\n    interp.plot_confusion_matrix(figsize=(8,8))\nexcept Exception as inst:\n    print(inst)\n\n'DataLoader' object has no attribute 'new'\n\n\n\nlearn.save(\"exported_model_from_pytorch-fastai\", with_opt=False)\n\nPath('models/exported_model_from_pytorch-fastai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_2__Wwf_Homework_PyTorch_Tower_Classification.html#conclusions",
    "title": "Telecommunication towers component clasification. Part 2. PyTorch",
    "section": "Conclusions",
    "text": "Conclusions\n\nFor some reason when starting with a PyTorch Dataloader lr_find takes more than 20 minutes to run. 4 times what it took in Part 1 where fastai’s Dataloaders were used.\nIt takes 2 minutes to train each epoch, which in Part 1 took less than a minute.\nThe error rate wasn’t that good after the 3 epochs of fine tuning. 16% vs 6% in Part 1. It took two more epochs with the learning rate suggested by lr_find to lower the error rate to 10%. Maybe the customized fastai head plays an important role here.\nfastai’s Dataloaders created from PyTorch’s Dataloader missed some attributed needed for fastai’s ClassificationInterpretation to run."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "",
    "text": "This is the third part of a series of posts dedicated to image classification of components that are pats of telecommunication structures.\nIn Part 1 we used all the magic of fastai directly, blindly, as was introduced in the firsts lessons of Practical Deep Learning for Coders:\n\nTelecommunication towers component clasification - Part 1. fastai\n\nIn Part 2 the idea was to apply the lessons from the course Walk with fastai, the missing pieces for success. It meant to get rid of the fastai magic except for the training, and use raw PyTorch for the dataset and dataloader creation and model setup.\n\nTelecommunication towers component clasification. Part 2. PyTorch\n\nHere we are going to use miniai, which is a simple and flexible framework that is being developed in Part 2 of Practical Deep Learning for Coders 2022. To install the framework go to: https://github.com/fastai/course22p2."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#import-libraries",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#import-libraries",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nfrom pathlib import Path\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom torch import nn\nimport torch\nimport torchvision.transforms as tvtfms\n\nfrom PIL import Image\n\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#image-data",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#image-data",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Image Data",
    "text": "Image Data\nThere are 514 images (training + validation) in 8 relatively “easy” to distinguish categories (components). Plus there are 20 images for testing.\n\nBase plate\nGrounding bar\nIdentification\nLadder\nLight\nLightning rod\nPlatform\nTransmission lines\n\nYou can download the pictures here.\nThere are two folders, one for the training (train) and the other for validation set (valid).\n\npath = Path(\"photos\")\n[folder.stem for folder in path.iterdir()]\n\n['test', 'train', 'valid']\n\n\n\ntrain_path = path / \"train\"\nvalid_path = path / \"valid\"\ntest_path = path / \"test\"\n\nAnd in each folder there is one folder for each label.\n\nlabels = [folder.stem for folder in train_path.iterdir()]\nnumber_of_labels = len(labels)\nprint(labels)\n\n['base_plate', 'grounding_bar', 'identification', 'ladder', 'light', 'lightning_rod', 'platform', 'transmission_lines']\n\n\n\nint_to_label = {k:v for k,v in enumerate(labels)}\nlabel_to_int = {k:v for v,k in int_to_label.items()}\nprint(label_to_int)\n\n{'base_plate': 0, 'grounding_bar': 1, 'identification': 2, 'ladder': 3, 'light': 4, 'lightning_rod': 5, 'platform': 6, 'transmission_lines': 7}"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#creating-pytorch-dataset",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#creating-pytorch-dataset",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Creating PyTorch Dataset",
    "text": "Creating PyTorch Dataset\n\nDataset Class\nThis class is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code.\n\nThis example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\n\nclass TowerPartsDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential, label_to_int:dict):\n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]\n        self.to_tensor = tvtfms.ToTensor()\n    \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return label_to_int[label]\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\n\nItem Transforms\n\nfrom fastai.vision.data import imagenet_stats\nprint(imagenet_stats)\n\nitem_tfms = nn.Sequential(\n    tvtfms.Resize((224, 224)), \n    tvtfms.Normalize(*imagenet_stats)\n)\n\n([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n\n\nitem_tfms[0](Image.open(train_path / 'base_plate/CIMG4695.jpg'))\n\n\n\n\n\n\nTrain and validation datasets\n\ntrain_dataset = TowerPartsDataset(\n    train_path,\n    item_tfms,\n    label_to_int\n)\n\nvalid_dataset = TowerPartsDataset(\n    valid_path,\n    item_tfms,\n    label_to_int\n)\n\n\nx, y = train_dataset[0]\nx.shape, y\n\n(torch.Size([3, 224, 224]), 0)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#dataloader",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#dataloader",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "DataLoader",
    "text": "DataLoader\n\nbatch_size = 64\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=True,\n    batch_size=batch_size\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=batch_size * 2\n)\n\n\nminiai Dataloaders\n\nfrom miniai.datasets import DataLoaders, show_images\n\n\ndls = DataLoaders(train_dataloader, valid_dataloader)\n\n\ndt = dls.train\nxb, yb = next(iter(dt))\nxb.shape, yb[:10]\n\n(torch.Size([64, 3, 224, 224]), tensor([6, 7, 0, 0, 2, 6, 6, 6, 3, 5]))\n\n\n\n%%time\nfor _ in train_dataloader: pass\n\nCPU times: user 6min 35s, sys: 49.7 s, total: 7min 25s\nWall time: 1min 25s\n\n\n\nfrom operator import itemgetter\n\n# To avoid warning of clipping input data, a sigmoid is applied\nxbt = xb[:16].sigmoid()\nybt = yb[:16]\ntitles = itemgetter(*ybt.tolist())(int_to_label)\nshow_images(xbt, imsize=2.25, titles=titles)\n\n\n\n\nAs we can see, there are some pictures that have at least two components that could be classified as for our labels. So maybe the correct approach would be a multi-label classification."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#customizing-a-pytorch-model",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#customizing-a-pytorch-model",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Customizing a PyTorch Model",
    "text": "Customizing a PyTorch Model\nWhen loading a model to fastai learner, it is customized by changing the last two children (the Head).\nWhat we are going to do here is to change only what is essential for our problem, that is, the final linear layer in order to have 8 features (the number of labels need to classify).\nFor more detail on this I encourage you to take Walk with fastai, the missing pieces for success.\nUnlike Part 1 and Part 2, here we are going to try with resnet18.\n\nfrom torchvision.models import resnet18\nmodel = resnet18(pretrained=True)\n\n\nCustomizing the last linear layer\nWe need an output size equal to the number of labels we are trying to predict.\n\nmodel_child = list(model.children())\nmodel_child[-2:]\n\n[AdaptiveAvgPool2d(output_size=(1, 1)),\n Linear(in_features=512, out_features=1000, bias=True)]\n\n\nWe can access the original final layer with model.fc\n\nmodel.fc\n\nLinear(in_features=512, out_features=1000, bias=True)\n\n\nIt has the 1,000 features ResNet was trained for. We need to change it to the 8 features (labels, classes) of the problem we are dealing here:\n\nmodel.fc = nn.Linear(512, out_features=number_of_labels, bias=True)\n\n\nmodel.fc\n\nLinear(in_features=512, out_features=8, bias=True)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#callbacks",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#callbacks",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Callbacks",
    "text": "Callbacks\nFor more info about miniai and its callback system:\n\nFastai Course Part 2 2022: Understanding CallBacks by Francesco Pochetti\nRedesign your Training Loop with CallBacks by Dien-Hoa Truong\n\n\nfrom miniai.learner import MetricsCB, DeviceCB, ProgressCB, TrainLearner\nfrom torcheval.metrics import MulticlassAccuracy\n\nmetrics = MetricsCB(accuracy=MulticlassAccuracy())\ncbs = [DeviceCB(), metrics, ProgressCB(plot=True)]"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#the-optimizer",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#the-optimizer",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "The Optimizer",
    "text": "The Optimizer\nLet’s use the same optimizer as fastai’s default: AdamW\n\nfrom miniai.activations import set_seed\nfrom miniai.sgd import BatchSchedCB, RecorderCB\n\nfrom torch.optim import AdamW\nfrom functools import partial\nfrom torch.optim import lr_scheduler\nimport torch.nn.functional as F\n\n\nRecord the scheduler’s parameters\n\ndef _lr(cb): return cb.pg['lr']\ndef _beta1(cb): return cb.pg['betas'][0]\n\nrec = RecorderCB(lr=_lr, mom=_beta1)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#training-with-miniai",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#training-with-miniai",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Training with miniai",
    "text": "Training with miniai\n\nset_seed(42)\nlr, epochs = 0.001, 4\n\n\nCustom training loop\nminiai callbacks allows us to easily modify the training loop. In this case we are going to store the validation loss for each sample of the validation dataset (without reducing it to a single value). Since the validation set has 120 pictures, and the validation batch size is 64 x 2, by storing the last loss as valid_loss is enough to then use those values and plot the top losses as we do with fastai.\nPredictions are stored in learn.preds as set in miniai TrainLearner.\n\nclass TrainValidLossTrack(TrainLearner):\n    def get_loss(self): \n        self.loss = self.loss_func(self.preds, self.batch[1])\n        # Store loss without reduction in the Learner\n        self.valid_loss = self.loss_func(self.preds, self.batch[1], reduction=\"none\")\n\n\ntmax = epochs * len(dls.train)\nsched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n\nxtra = [BatchSchedCB(sched), rec]\nlearn = TrainValidLossTrack(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=AdamW)\n\n\nlearn.fit(epochs)\n\n\n\n\n\n\n\n  \n    \n      accuracy\n      loss\n      epoch\n      train\n    \n  \n  \n    \n      0.401\n      1.696\n      0\n      train\n    \n    \n      0.850\n      0.442\n      0\n      eval\n    \n    \n      0.977\n      0.103\n      1\n      train\n    \n    \n      0.842\n      0.638\n      1\n      eval\n    \n    \n      1.000\n      0.013\n      2\n      train\n    \n    \n      0.875\n      0.621\n      2\n      eval\n    \n    \n      1.000\n      0.007\n      3\n      train\n    \n    \n      0.925\n      0.326\n      3\n      eval\n    \n  \n\n\n\n\n\n\nGood news, it trained better than (I) expected! It looks like it overfit a little, although we only trained for 4 epochs and a smaller model (resnet18) vs Part 1 & 2 (resnet34).\n\n\nHow the the parameters were changed by the scheduler\n\nimport matplotlib.pyplot as plt\nrec.plot()"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#classification-interpretation",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#classification-interpretation",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Classification Interpretation",
    "text": "Classification Interpretation\n\nlearn.preds.shape, learn.valid_loss.shape\n\n(torch.Size([120, 8]), torch.Size([120]))\n\n\n\nactuals = [label_to_int[l.parts[-2]] for l in valid_dataset.paths]\n\n\nConfusion Matrix\n\nconfusion_matrix = [[0]*number_of_labels for n in range(number_of_labels)]\nact_pred = list(zip(actuals, learn.preds.argmax(1).tolist()))\n\nfor act, pred in act_pred:\n    confusion_matrix[act][pred] += 1\n\n\nx = y = [x for x in labels]\n\nfig = px.imshow(confusion_matrix, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"# Pics\"),\n                text_auto=True, x=x, y=x,\n                color_continuous_scale='blues', title='Confusion Matrix')\nfig.show()\n\n\n                                                \n\n\n\n\nPlot Top Losses\n\nsorted_losses_idcs = learn.valid_loss.argsort(descending=True).cpu()\n\n\ndv = dls.valid\nxb, yb = next(iter(dv))\nxbv = xb[sorted_losses_idcs[:12]].sigmoid()\n\n\nact_pred_plot = torch.tensor(act_pred)[sorted_losses_idcs[:12]].tolist()\n\n\ntitles = [f\"{int_to_label[act]} / \\n {int_to_label[pred]}\" for act, pred in act_pred_plot][:12]\n\n\nshow_images(xbv, imsize=3.0, title='Actual / Predicted', titles=titles)"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#save-the-model",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#save-the-model",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Save the model",
    "text": "Save the model\n\nmdl_path = Path('models')\ntorch.save(learn.model,  mdl_path/'exported_resnet18_pytorch-miniai.pth')"
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#gpu-inference",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#gpu-inference",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "GPU Inference",
    "text": "GPU Inference\n\nloaded_model = torch.load(mdl_path/'exported_resnet18_pytorch-miniai.pth')\n\n\nmodel evaluation\n\nRemember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference.\n\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-and-loading-models\n\nloaded_model.eval();\n\n\n\nTest dataset\n\ntest_files = list(test_path.iterdir())\nprint(test_files)\n\n[Path('photos/test/archipielago-los-roques-078.jpg'), Path('photos/test/archipielago-los-roques-313.jpg'), Path('photos/test/cantv-mata-palo-350.jpg'), Path('photos/test/CIMG8119.jpg'), Path('photos/test/DSC00191.jpg'), Path('photos/test/DSC01399.jpg'), Path('photos/test/DSC01537.jpg'), Path('photos/test/DSC01628.jpg'), Path('photos/test/DSC01657.jpg'), Path('photos/test/DSC01723.jpg'), Path('photos/test/DSC01734.jpg'), Path('photos/test/DSC01955.jpg'), Path('photos/test/DSC01956.jpg'), Path('photos/test/DSC04892.jpg'), Path('photos/test/DSC05105.jpg'), Path('photos/test/DSC05130.jpg'), Path('photos/test/DSC09446.jpg'), Path('photos/test/DSC09524.jpg'), Path('photos/test/DSC09685.jpg'), Path('photos/test/DSC09908.jpg')]\n\n\n\nshow_images([item_tfms[0](Image.open(f)) for f in test_files], imsize=2.5)\n\n\n\n\n\nto_tensor = tvtfms.ToTensor()\n\ndef get_predictions(im_files:list, model:\"torchvision.models\"):\n    tensor_images = []\n    for file in im_files:\n        tensor_images.append(item_tfms(to_tensor(Image.open(file)).cuda()))\n        \n    return model(torch.stack(tensor_images)).argmax(1)\n\nprint([int_to_label[pred.item()] for pred in get_predictions(test_files, loaded_model)])\n\n['grounding_bar', 'ladder', 'light', 'platform', 'base_plate', 'light', 'platform', 'grounding_bar', 'base_plate', 'base_plate', 'ladder', 'light', 'lightning_rod', 'ladder', 'identification', 'ladder', 'lightning_rod', 'ladder', 'light', 'identification']\n\n\nAlmost all the images in the test set are predicted correctly. Thats ok for the purpuse of this post. But as said before, probably the approach would be to do a multi-label classification."
  },
  {
    "objectID": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#conclusions",
    "href": "posts/2023-03_Wwf_homework/Part_3__Tower_Parts_Classisfier_with_miniai.html#conclusions",
    "title": "Telecommunication towers component clasification. Part 3. PyTorch and miniai",
    "section": "Conclusions",
    "text": "Conclusions\n\nIt was great to being able to train with more than 90% accuracy using PyTorch and miniai, starting with a pretrained resnet18 model.\nWe implemented Confusion Matrix and Plot Top Losses from scratch, techniques available in fastai’s Classification Interpretation.\nWe also did batch inference in the GPU."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "",
    "text": "The exploration I’m documenting here about dbt (Data Build Tool) is almost a recreation of the awsome tutorial Building a Kimball dimensional model with dbt but with some twists.\nI encourage you to read that tutorial for a discussion of what a dimensional model is, its benefits, and the differences with other modeling techniques such as Third Normal Form (3NF) and One Big Table (OBT).\nWhat I did different from that tutorial was the following:\n\nWorked with the AdventureWorks data already loaded into PostgreSQL, and not seeded from .csv files. For that I managed to create a backup that you can restore to your Postgres database. You can download it here.\nExperimented with three flavors of Postgres:\n\nLocal PostgreSQL.\nSupabase PostgreSQL as-a-service (free).\nAWS Aurora Serverless for PostgreSQL.\n\nCreated custom schema names Deploy to custom schemas & override dbt defaults.\nCreated two date dimension tables, one with calogical dbt-date extension, which is a wraper around dbt_utils.date_spine, and the other with date_spine itself. I managed to create the calendar dynamically ranging from the first day in the fact table to the last one."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#setting-up-the-project",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#setting-up-the-project",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Setting up the project",
    "text": "Setting up the project\nIt is not in the scope of this document to explain how to get started with dbt. I had never used dbt and it was straight forward with these two tutorials:\n\nBuilding a Kimball dimensional model with dbt\nIntro to Data Build Tool (dbt) // Create your first project!\n\nYou can explore the whole experiment I did in this repository: fmussari/dbt-dimensional-modeling-experiment"
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#defining-the-databases-in-profiles.yml",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#defining-the-databases-in-profiles.yml",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Defining the databases in profiles.yml",
    "text": "Defining the databases in profiles.yml\ndbt is a transformation tool, so it only works with one database at a time, meaning that the same database is the source and the target. In this case we have the normalized AdventureWorks database and we are creating the transformations to create a model with dimensions and fact tables. I found interesting that multiple databases can be defined in a project, so you can test, for example, in your local database, and then create the models into the cloud database.\nFor storing the credentials I used conda, based on setting environment variables.\nThe profiles.yml ended looking like this:\n\n\nprofiles.yml\n\ndbt_dimensional_demo:\n\n  outputs:\n\n    postgresAdventure:\n      type: postgres\n      threads: 4\n      host: localhost\n      port: 5432\n      user: postgres\n      pass: '10042076'\n      database: Adventureworks\n      schema: target  # this is override in each model's sql\n\n    supabaseAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('SUPABASE_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('SUPABASE_USER') }}\"\n      pass: \"{{ env_var('SUPABASE_PASS') }}\"\n      database: Adventureworks\n      schema: t\n    \n    auroraAdventure:\n      type: postgres\n      threads: 4\n      host: \"{{ env_var('AURORA_HOST') }}\"\n      port: 5432\n      user: \"{{ env_var('AURORA_USER') }}\"\n      pass: \"{{ env_var('AURORA_PASS') }}\"\n      database: Adventureworks\n      schema: t\n\n  target: supabaseAdventure\n\nAs you can see your experiment can run in any database specified in the file, by only changing the target."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#creating-the-dimensional-model",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#creating-the-dimensional-model",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Creating the Dimensional Model",
    "text": "Creating the Dimensional Model\nThis part is almost a copy from the aforementioned tutorial. The only difference being that based on the video Deploy to custom schemas & override dbt defaults I added some lines to configure the Schema in the transformations file.\nAnother difference was that I specified my sources in in the file schema.yml that looks like this:\n\n\nschema.yml\n\n# Copied from: https://docs.getdbt.com/docs/build/sources\n\nversion: 2\n\nsources:\n  - name: pg_production\n    database: Adventureworks\n    description: 'Adventureworks, production schema'\n    schema: production\n    tables:\n      - name: product\n      - name: productcategory\n      - name: productsubcategory\n\n  - name: pg_person\n    database: Adventureworks\n    description: 'Adventureworks, person schema'\n    schema: person\n    tables:\n      - name: address\n      - name: stateprovince\n      - name: countryregion\n      - name: person\n\n  - name: pg_sales\n    database: Adventureworks\n    description: 'Adventureworks, sales schema, customer table'\n    schema: sales\n    tables:\n      - name: customer\n      - name: store\n      - name: creditcard\n      - name: salesorderheader\n      - name: salesorderdetail\n\nSo dim_address.sql had the lines on materialization and schema configuration, pointed to the sources defined in the previous .yml, and ended looking like this:\n\n\ndim_address.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\nwith stg_address as (\n    select *\n    from {{ source('pg_person', 'address') }}\n),\n\nstg_stateprovince as (\n    select *\n    from {{ source('pg_person', 'stateprovince') }}\n),\n\nstg_countryregion as (\n    select *\n    from {{ source('pg_person', 'countryregion') }}\n)\n\nselect\n    {{ dbt_utils.generate_surrogate_key(['stg_address.addressid']) }} as address_key,\n    stg_address.addressid,\n    stg_address.city as city_name,\n    stg_stateprovince.name as state_name,\n    stg_countryregion.name as country_name\nfrom stg_address\nleft join stg_stateprovince on stg_address.stateprovinceid = stg_stateprovince.stateprovinceid\nleft join stg_countryregion on stg_stateprovince.countryregioncode = stg_countryregion.countryregioncode\n\nIn the same way as the tutorial, there were created the other dimensions:\n\ndim_credit_card.sql\ndim_customer.sql\ndim_order_status.sql\ndim_product.sql\n\n\nFact table:\n\nfact_sales.sql"
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#date-dimension",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#date-dimension",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Date Dimension",
    "text": "Date Dimension\nThis is where I did most of my own experiments. I wanted to create a Date Domension or Calendar table ranging from the earliest to the latest date in the fact table, that comes from salesorderheader. I had little to no experience at all with Jinja templates, which is what dbt relies on for transformations, so maybe there are better ways to achieve what I intented to do, but I must say it did worked fine.\n\ndbt_utils.date_spine\nThe first version of the date dimension was created using dbt_utils, based on the tutorial As of Date Tables. I simply changed start_date and end_date statements to get the aforementioned earliest and latest dates from salesorderheader. The transformation ended looking like this:\n\n\ndim_dates.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\n{%- set datepart = \"day\" -%}\n{%- set start_date = \"(select MIN(cast(orderdate as date)) from sales.salesorderheader)\" -%}\n{%- set end_date = \"(select MAX(cast(orderdate as date)) + (interval '1 day') from sales.salesorderheader)\" -%}\n\nwith as_of_date AS (\n    {{ dbt_utils.date_spine(\n        datepart=datepart, \n        start_date=start_date,\n        end_date=end_date\n       ) \n    }}\n)\n\nSELECT * FROM as_of_date\n\nThis isn’t a full calendar but a sequence of dates from earliest to latest. For a full calendar we then would need to create all the columns as year, month, day, quarter…\n\n\nStatement Blocks\nI then made some experiments with statement blocks and created one called get_dates to generate a view with the earliest and latest dates from salesorderheader:\n\n\ndim_view_date_range.sql\n\n{%- call statement('get_dates', fetch_result=True) -%}\n\n      SELECT  MIN(cast(orderdate as date)), MAX(cast(orderdate as date)) \n      FROM {{ source('pg_sales', 'salesorderheader') }}\n\n{%- endcall -%}\n\n{%- set earliest = load_result('get_dates')['data'][0][0] -%}\n{%- set latest = load_result('get_dates')['data'][0][1] -%}\n\nSELECT cast('{{ earliest }}' as date) AS earliest, cast('{{ latest }}' as date) AS latest\n\nIt worked, now we know how to create a statement that gets dates from salesorderheader. Lets use it for our final step in this experiment which is creating the Calendar with calogical.\n\n\ndbt_date.get_date_dimension\nThis wraper creates a full calendar dimensio, the transformation\n\n\ndim_dates_calogical.sql\n\n{{ config(materialized='table') }}\n{{ config(schema='Dimensions') }}\n\n{%- call statement('get_dates', fetch_result=True) -%}\n\n      SELECT  MIN(cast(orderdate as date)), MAX(cast(orderdate as date)) + (interval '1 day')\n      FROM {{ source('pg_sales', 'salesorderheader') }}\n\n{%- endcall -%}\n\n{%- set earliest = load_result('get_dates')['data'][0][0] -%}\n{%- set latest = load_result('get_dates')['data'][0][1] -%}\n\n{{ dbt_date.get_date_dimension(\n        start_date=earliest, \n        end_date=latest\n    ) \n}}\n\nThis script creates the following columns: date_day, prior_date_day, next_date_day, prior_year_date_day, prior_year_over_year_date_day, day_of_week, day_of_week_name, day_of_week_name_short, day_of_month, day_of_year, week_start_date, week_end_date, prior_year_week_start_date, prior_year_week_end_date, week_of_year, iso_week_start_date, iso_week_end_date, prior_year_iso_week_start_date, prior_year_iso_week_end_date, iso_week_of_year, prior_year_week_of_year, prior_year_iso_week_of_year, month_of_year, month_name, month_name_short, month_start_date, month_end_date, prior_year_month_start_date, prior_year_month_end_date, quarter_of_year, quarter_start_date, quarter_end_date, year_number, year_start_date and year_end_date."
  },
  {
    "objectID": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#wrap-up",
    "href": "posts/2023-05_dbt_dimensional_model/dimensional_model_with_dbt_postgres.html#wrap-up",
    "title": "Dimensional Modelling with dbt and different flavors of PostgreSQL",
    "section": "Wrap Up",
    "text": "Wrap Up\ndbt is one of most representative tool of the called Modern Data Stack and the Analytics Engineering, claiming the incorporation of Software Engineering practices to the data stack. What I found interesting is the fact that (at least in theory) we can create the tranformations collaboratively, version control them and then deploy those models to any supported datawarehouse.\nTo get deepen in the data landscape, Modern Data Stack and dbt I highly recommend:\n\nHot Takes on the Modern Data Stack\nMAD 2023, PART III: TRENDS IN DATA INFRASTRUCTURE\nThe Next Layer of the Modern Data Stack | dbt’s Tristan Handy"
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "",
    "text": "Loading MovieLens data into BigQuery\nSetup services and accounts for the Vertex AI Pipeline to run\nCreate and run the Pipeline and its Components\nMake sure no reservation or slot assignment remains active"
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#overview",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#overview",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Overview",
    "text": "Overview\nThis post is about training a Matrix Factorization model with BigQuery ML and deploying it as Docker container. The end-to-end process is orchestrated through a Vertex AI pipeline.\nThe post is strongly based on this tutorial: - YouTube: Recommendation Engine Pipeline with BigQuery ML and Vertex AI Pipelines using Matrix Factorization\nBut there are key differences:\n\nThis post documents the whole process, from loading the date to BigQuery to how to make recomendations in different ways.\nOn July 5th there was a Transition to BigQuery editions which resulted in some changes being made to adapt the scripts showed in the video.\nWhen trying to replicate the video tutorial I had to solve some issues with the pipeline failing to run. Most of the issues were very hard to debug, with misleading error messages and layers over layer of abstraction between python libraries and component definitions. At the end most of the errors were about the Default Service Account not having the requiered permission. So,\nIn this post we can see and run each step and command to create a Service Account and grant this account granular permissions to the Google Cloud resources needed for the end to end process to run. This is what the Google documentation recommends.\nThere are also the commands to enable each service API needed for the project."
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#additional-resources",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#additional-resources",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nYouTube: Lesson 7: Practical Deep Learning for Coders 2022 - Collaborative filtering deep dive\nTutorial: Use BigQuery ML to make recommendations from Google analytics data"
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#before-you-begin",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#before-you-begin",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Before you begin",
    "text": "Before you begin\n\nSelect or create a Google Cloud project.\nMake sure that billing is enabled for your project.\nYou can run the code locally or in Colab. If you are locally you need to install the gcloud CLI."
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#loading-data-into-bigquery",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#loading-data-into-bigquery",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Loading data into BigQuery",
    "text": "Loading data into BigQuery\nReference: Load the Movielens dataset into BigQuery\n\nAuthenticate your Google Cloud account\n\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    ! gcloud auth login\n\n\n\nSet your project ID\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n\n\n\nDownload MovieLens 1M movier ratings dataset\n\n! curl -O 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n! unzip -o ml-1m.zip\n\nChange :: delimiter to comma , and save as .csv files:\n\n! sed 's/::/,/g' ml-1m/ratings.dat > ratings.csv\n! sed 's/::/@/g' ml-1m/movies.dat > movie_titles.csv\n\n\n\nCreate BigQuery datasets and populate the tables\n\nMODEL_DATASET = \"[bq-model-dataset]\"  # @param {type:\"string\"}\nMOVIELENS_DATASET = \"[bq-data-dataset]\"  # @param {type:\"string\"}\n\n\n# To store the model\n! bq mk --location=US --dataset {PROJECT_ID}:{MODEL_DATASET}\n\n# To store movies and reviews tables\n! bq mk --location=US --dataset {PROJECT_ID}:{MOVIELENS_DATASET}\n\nPopulate the tables\n\n# Reviews table\n! bq load --project_id={PROJECT_ID} --source_format=CSV {PROJECT_ID}:{MOVIELENS_DATASET}.movielens_1m ratings.csv user_id:INT64,item_id:INT64,rating:FLOAT64,timestamp:TIMESTAMP\n\n# Movies table\n! bq load --project_id={PROJECT_ID} --source_format=CSV --field_delimiter=@ {PROJECT_ID}:{MOVIELENS_DATASET}.movie_titles movie_titles.csv movie_id:INT64,movie_title:STRING,genre:STRING"
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#vertex-ai-pipeline.-install-libraries-and-setup-services-and-accounts",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#vertex-ai-pipeline.-install-libraries-and-setup-services-and-accounts",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Vertex AI Pipeline. Install Libraries and Setup Services and Accounts",
    "text": "Vertex AI Pipeline. Install Libraries and Setup Services and Accounts\n\nInstall Libraries\n\n%%capture\n! pip install google-cloud-aiplatform==1.21.0 --upgrade\n! pip install kfp==2.0.1 --upgrade\n! pip install google-cloud-pipeline-components==2.0.0 --upgrade\n\n\n\nRestart the Kernel\n\n# Automatically restart kernel after installs so that your environment can access the new packages\nimport IPython\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n\n\n\nSet Project Variables\n\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"              # @param {type: \"string\"}\n\nBUCKET_NAME = \"[pipeline-bucket]\" # @param {type: \"string\"}\n\nPIPELINE_ROOT = f\"gs://{BUCKET_NAME}/\"   \n\nMODEL_DIR = PIPELINE_ROOT + \"recommender_model\"    # @param {type: \"string\"}\n\n\n\nAuthenticate your Google Cloud account (again)\nSince the kernel was restarted, authenticate again.\n\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    ! gcloud auth login\n\n\n! gcloud config set project {PROJECT_ID}\n\n\n\nEnable Service APIs\nIt is great to enabling the APIs with commands because everything stays documented. We need the following services: - Identity and Access Management (IAM) API - Vertex AI API - Cloud Build API - BigQuery API (Although this should be enabled already if the datasets were created and the tables populated) - BigQuery Reservation API - Cloud Run Admin API - Cloud Storage API\n\n! gcloud services enable iam.googleapis.com --project={PROJECT_ID}\n! gcloud services enable aiplatform.googleapis.com --project={PROJECT_ID}\n! gcloud services enable cloudbuild.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigquery.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigqueryreservation.googleapis.com --project={PROJECT_ID}\n! gcloud services enable run.googleapis.com --project={PROJECT_ID}\n! gcloud services enable storage-component.googleapis.com --project={PROJECT_ID}\n\n\n\nCreate the Service Account for the pipeline to run\n\nSERVICE_ACCOUNT_ID = \"[your-service-account-id]\"  # @param {type:\"string\"}\n\nIf the following cell returns an error in Colab, run it in your local machine. Or create the account directly in the Google Cloud Console UI.\n\n! gcloud iam service-accounts create {SERVICE_ACCOUNT_ID} --description=\"Vertex AI Pipeline Service Account\" --display-name=\"vertex_service_account\"\n\n\nSERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_ID}@{PROJECT_ID}.iam.gserviceaccount.com\"\n\n\nIf not, use the Default Service Account\n\nshell_output = ! gcloud projects describe {PROJECT_ID}\nPROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\nDEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n\nimport sys\n\nIS_COLAB = \"google.colab\" in sys.modules\n\nif (\n    SERVICE_ACCOUNT_ID == \"\"\n    or SERVICE_ACCOUNT_ID is None\n    or SERVICE_ACCOUNT_ID == \"[your-service-account-id]\"\n):\n    # Get your service account from gcloud\n    if not IS_COLAB:\n        shell_output = !gcloud auth list 2>/dev/null\n        DEFAULT_SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n\n    if IS_COLAB:\n        shell_output = ! gcloud projects describe {PROJECT_ID}\n        PROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n        DEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n    print(\"Service Account:\", DEFAULT_SERVICE_ACCOUNT)\n    SERVICE_ACCOUNT = DEFAULT_SERVICE_ACCOUNT\n\n\n\n\nGrant the Service Account granular permissions to GCP resources\nThe most challenging part of the project was figuring out how to give the service account the right granular permissions. I didn’t want to give the service account the Editor or Owner rol. It took me a while to find the right roles. As I mentioned some errors in the pipeline were because lack of permissions, but it was hard to troubleshoot as there wasn’t any mention to the actual rol needed.\n\naiplatform.user rol\n\nservice_arg = f\"serviceAccount:{SERVICE_ACCOUNT}\"\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/aiplatform.user\"\n\n\n\nCreate the GCS bucket and assign storage.objectAdmin rol\nI first assign the rols of storage.objectCreator and objectViewer. Took me hours to find out that the trained model couldn’t be exported without the rol of storage.objectAdmin which is needed to modify existing data in the bucket.\n\n! gsutil mb -p {PROJECT_ID} -l {REGION} {PIPELINE_ROOT}\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectAdmin {PIPELINE_ROOT}\n\n\n\nCloud Run’s run.developer rol\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/run.developer\"\n\n\n\nCloud Build’s cloudbuild.builds.editor rol\nI thought that Cloud Run developer rol was enough for the model to be deployed. Took me a while to find out the service account needed a permission from Cloud Build service.\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/cloudbuild.builds.editor\"\n\n\n\nAssign roles to Cloud Build’s service account\nWhen Cloud Build’s API is enabled, its service account is automatically created.\n\nBy default – for security reasons – the Cloud Build Service Account does not have the permissions to manage Cloud Run. Google Cloud Build + Google Cloud Run\n\nFirst lets grab the default service accounts.\n\n# Cloud Build default service account\ncloud_build_sa = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\nservice_build_arg = f\"serviceAccount:{cloud_build_sa}\"\n# Compute Engine default service account\ncompute_engine_sa = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_build_arg} --role=\"roles/run.admin\"\n\nAnd the last part is “Grant the IAM Service Account User role to the Cloud Build service account on the Cloud Run runtime service account”.\nMeaning that the Cloud Build service account is going to be able to impersonate the Cloud Run runtime service account, which is Compute Engine default service account. More on impersonation here: Youtube: Service Account Impersonation in Google Cloud - IAM in GCP\n\n! gcloud iam service-accounts add-iam-policy-binding {compute_engine_sa} --member={service_build_arg} --role=\"roles/iam.serviceAccountUser\"\n\nAnd thats it, all services are enabled and the service accounts has the granular permission for the pipeline to run."
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#vertex-ai-pipeline---create-and-run-the-pipeline",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#vertex-ai-pipeline---create-and-run-the-pipeline",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Vertex AI Pipeline - Create and Run the Pipeline",
    "text": "Vertex AI Pipeline - Create and Run the Pipeline\n\nImport Libraries\n\nimport kfp\nfrom typing import NamedTuple\nfrom kfp.dsl import (\n    pipeline, component, OutputPath, InputPath, Model, \n    Input, Artifact, Output, Metrics\n)\nfrom kfp import compiler\n\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\n\n\n\nInitialize Vertex AI SDK for Python\n\naiplatform.init(project=PROJECT_ID, location=REGION)\n\n\n\nDeclare Pipeline Components\n\nCreate Reservation Component\n\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef create_reservation(\n    project: str,\n    location: str,\n    commitment_slots: int,\n    reservation_id: str\n) -> NamedTuple(\"outputs\", [(\"reservation_name\", str), (\"assignment_name\", str)]):\n    \"\"\"\n    Create BigQuery Reservation and Assignment\n    \"\"\"\n    print('project', project, 'location', location, 'commitment_slots', commitment_slots, 'reservation_id', reservation_id)\n\n    # import libraries\n    import time\n    from google.cloud.bigquery_reservation_v1 import (\n        CapacityCommitment, Reservation, Assignment, ReservationServiceClient\n    )\n\n    reservation_client = ReservationServiceClient()\n    parent_arg = f\"projects/{project}/locations/{location}\"\n\n    # Reservation (autoscaling)\n    autosc = Reservation.Autoscale(current_slots=0, max_slots=commitment_slots) # NEW Jul-05 Update\n    #reservation_slots = commitment_slots\n    slot_capacity = 0\n    reservation_config = Reservation(\n        #slot_capacity=reservation_slots,\n        slot_capacity=slot_capacity, autoscale=autosc,  # NEW Jul-05 Update\n        edition='ENTERPRISE',   # NEW Jul-05 Update (Could be \"STANDARD\" ?)\n        ignore_idle_slots=False\n    )\n    reservation = reservation_client.create_reservation(\n        parent=parent_arg, reservation_id=reservation_id, reservation=reservation_config\n    )\n    reservation_name = reservation.name\n    print('reservation_name', reservation_name)\n\n    # Assignment\n    print(\"Creating Assignment...\")\n    assignment_config = Assignment(\n        job_type='QUERY', assignee='projects/{}'.format(project)\n    )\n    assignment = reservation_client.create_assignment(\n        parent=reservation_name, assignment=assignment_config\n    )\n    assignment_name = assignment.name\n    print('assignment_name', assignment_name)\n\n    # it can take a lot for the slots to be available\n    print(\"Waiting for 300 seconds...\")\n    time.sleep(300)\n\n    return reservation_name, assignment_name\n\n\n\n\n\n\n\nThis is Important\n\n\n\nAfter the reservation is created, the account is going to be billed by time and number of slots. It can cost about $3 for each pipeline run, but you need to be very cautious about the pipeline failing before the reservation is deleted. To be sure the reservation is deleted you can go to https://console.cloud.google.com/bigquery/admin/reservations and select the project of interest. Check that SLOT RESERVATIONS and SLOT COMMITMENTS are empty and look like this:\nSLOT RESERVATIONS\n\nSLOT COMMITMENTS\n\nIf not empty, delete them in the UI. Below you can find some scripts you can run to delete the reservations in case the pipeline fails before doing it.\n\n\n\n\nDelete Reservation Component\n\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef delete_reservation(\n    reservation_name: str,\n    assignment_name: str\n):\n    # import libraries\n    from google.cloud.bigquery_reservation_v1 import ReservationServiceClient\n\n    # Delete Assignment, Reservation and Capacity\n    reservation_client = ReservationServiceClient()\n    reservation_client.delete_assignment(name=assignment_name)\n    reservation_client.delete_reservation(name=reservation_name)\n\n    print('reservation_name', reservation_name, 'deleted')\n    print('assignment_name', assignment_name, 'deleted')\n\n\n\nLog Eval Metrics Component\n\n@component()\ndef log_eval_metrics(\n    eval_metrics: Input[Artifact], metrics: Output[Metrics]\n) -> dict:\n    # import libraries\n    import math\n\n    metadata = eval_metrics.metadata\n    for r in metadata[\"rows\"]:\n        rows = r[\"f\"]\n        schema = metadata[\"schema\"][\"fields\"]\n        output = {}\n        for metric, value in zip(schema, rows):\n            metric_name = metric[\"name\"]\n            val = float(value[\"v\"])\n            output[metric_name] = val\n            metrics.log_metric(metric_name, val)\n\n    print(output)\n\n\n\nDeploy Model Component\nThe names in this function are hard coded. It assumes you keep recommender_model as the model dir. If it was changed previously, change it here accordingly. Also edit [PIPELINE_ROOT] with your chosen name.\n\n@component(packages_to_install=[\n    \"google-cloud-build==3.18.0\",\n    \"google-api-python-client==2.93.0\"])\ndef deploy_recommendations_model(\n    artifact_uri: str,\n    project: str\n):\n    # import libraries\n    from google.cloud.devtools import cloudbuild\n    from googleapiclient.discovery import build\n\n    # Deploy Model\n    client = cloudbuild.CloudBuildClient()\n    build = cloudbuild.Build()\n\n    # Fill [PIPELINE_ROOT] with your selected parameter\n    # If you didn't keep `recommender_model` change it accordingly\n    build.steps = [\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/recommender_model', \".\"]  },\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/dockerfile/Dockerfile/', \"Dockerfile\"] },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"build\", \"-t\", f\"gcr.io/{project}/recommender_model\", \".\" ]   },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"push\", f\"gcr.io/{project}/recommender_model\"]    },\n        {   \"name\": \"gcr.io/cloud-builders/gcloud\",\n            \"args\": [\n                \"run\", \"deploy\",\n                \"recommender-model\", \"--image\", f\"gcr.io/{project}/recommender_model\",\n                \"--region\", \"us-central1\", \"--platform\", \"managed\", \"--memory\", \"500Mi\",\n                \"--allow-unauthenticated\", \"--max-instances\", \"5\", \"--port\", \"8501\"\n            ]\n        }\n    ]\n\n    operation = client.create_build(project_id=project, build=build)\n    # Print the in-progress operation\n    print(\"IN PROGRESS:\")\n    print(operation.metadata)\n\n    result = operation.result()\n    # Print the completed status\n    print(\"RESULT:\", result.status)\n\n\n\nCreate the Docker file and copy it to GCS\nKeep recommender_model or change it accordingly.\n\n%%writefile Dockerfile\nFROM tensorflow/serving:2.4.4\nENTRYPOINT [\"/usr/bin/env\"]\nENV MODEL_NAME=recommender_model\nENV PORT=8501\nCOPY recommender_model /models/recommender_model/1\nCMD tensorflow_model_server --port=8500 --rest_api_port=$PORT --model_base_path=/models/recommender_model --model_name=$MODEL_NAME\n\n\ndocker_path = PIPELINE_ROOT + 'dockerfile/'\n! gsutil cp Dockerfile {docker_path}\n\n\n\n\nPipeline and Job\n\nDeclare the Pipeline\n\n@pipeline(\n    name=\"bigquery-recommender-pipeline\",\n    pipeline_root=PIPELINE_ROOT + \"bigquery-recommender-pipeline\"\n)\ndef recommendation_pipeline(\n    artifact_uri: str,\n    display_name: str\n):\n\n    # import libraries\n    from google_cloud_pipeline_components.v1.bigquery import (\n        BigqueryCreateModelJobOp,\n        BigqueryEvaluateModelJobOp,\n        BigqueryExportModelJobOp\n    )\n\n    # NODE create-reservation\n    project = PROJECT_ID\n    location = \"us\"\n    slots = 100\n    reservation_id = \"matrix-factorization-reservation\"\n\n    create_reservation_task = create_reservation(\n        project=project, location=location, commitment_slots=slots, reservation_id=reservation_id\n    )\n\n    # NODE create-model-task\n    # Query for training\n    num_factors = 60\n    q = f\"\"\"\n    CREATE OR REPLACE MODEL {MODEL_DATASET}.model_00\n    OPTIONS\n        (model_type='matrix_factorization',\n        user_col='user_id',\n        item_col='item_id',\n        RATING_COL='rating',\n        feedback_type='EXPLICIT',\n        l2_reg=9.83,\n        num_factors={num_factors}) AS\n    SELECT user_id, item_id, rating FROM {MOVIELENS_DATASET}.movielens_1m\n    \"\"\"\n    create_model_task = BigqueryCreateModelJobOp(\n        project=project,\n        location=location,\n        query=q,\n    ).after(create_reservation_task)\n\n    # NODE evaluate-model-task\n    bq_evaluate_task = BigqueryEvaluateModelJobOp(\n        project=project, location=location, model=create_model_task.outputs[\"model\"]\n    ).after(create_model_task)\n\n    # NODE delete-reservation\n    delete_reservation_task = delete_reservation(\n        reservation_name=create_reservation_task.outputs[\"reservation_name\"],\n        assignment_name=create_reservation_task.outputs[\"assignment_name\"]\n    ).after(bq_evaluate_task)\n\n    # NODE log-eval-metrics\n    bqml_eval_metrics_raw = bq_evaluate_task.outputs[\"evaluation_metrics\"]\n    log_eval_metrics_task = log_eval_metrics(\n        eval_metrics=bqml_eval_metrics_raw\n    )\n\n    # NODE export-bq-model-to-gcs\n    bq_export_task = BigqueryExportModelJobOp(\n        project=project,\n        location=location,\n        model=create_model_task.outputs[\"model\"],\n        model_destination_path=artifact_uri,\n    ).after(create_model_task)\n\n    # NODE deploy-model-cloud-run\n    deploy_recommendations_model_task = deploy_recommendations_model(\n        artifact_uri=artifact_uri,\n        project=project\n    ).after(bq_export_task)\n\n\n\nCompile the Pipeline\n\ncompiler.Compiler().compile(\n    pipeline_func=recommendation_pipeline,\n    package_path=\"recommendation_pipeline.json\"\n)\n\n\n\nCreate and Run Job\n\njob = pipeline_jobs.PipelineJob(\n    enable_caching=False,\n    display_name=\"recommendation-pipeline\",\n    template_path=\"recommendation_pipeline.json\",\n    parameter_values={\n        \"artifact_uri\": MODEL_DIR,\n        \"display_name\": \"recommendation\",\n    }\n)\n\n\njob.run(\n    service_account=SERVICE_ACCOUNT,\n    sync=False\n)\n\nYou can monitor the pipeline here: - GCP COnsole: Vertex AI Pipelines\nAnd if all went fine, you must see this beautiful picture:"
  },
  {
    "objectID": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#make-sure-no-reservation-or-slot-assignment-is-active",
    "href": "posts/2023-07_recsys_pipeline_bigquery_vertexai/recsys_pipeline_gcp_part1.html#make-sure-no-reservation-or-slot-assignment-is-active",
    "title": "Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1",
    "section": "Make sure no reservation or slot assignment is active",
    "text": "Make sure no reservation or slot assignment is active\nIf the pipeline fails before deleting the reservation, this command lists the reservations, if any.\n\n! bq ls --reservation --project_id={PROJECT_ID} --location=us\n\nAnd this command shows the assignment.\n\n! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n\n\nDelete any reservation and assignment\n\nshell_output = ! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n\nassignment_id = shell_output[2].split(' ')[2].split('.')[-1]\nreservation_id = 'matrix-factorization-reservation' # As declared in the pipeline\n\n# remove assignment\n! bq rm --project_id={PROJECT_ID} --location=us --reservation_assignment {reservation_id}.{assignment_id}\n# remove reservation\n! bq rm --project_id={PROJECT_ID} --location=us --reservation {reservation_id}"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "",
    "text": "In the first lesson of the course Walk with fastai, the missing pieces for success, Zachary Mueller explained to us the three levels he sees in fastai APIs. This is different to Jeremy Howard’s consideration, who sees two levels.\nBy the second lesson we had made use of those levels with different datasets.\nIn this post we are going to explore the three levels with MNIST dataset, and in the process we will also go step by step in some fastai pieces that were not obvious for me in my beginnings.\nAs an extra we are going to create the Datasets and Dataloader in raw PyTorch."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#importing-fastai-library-and-data",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Importing fastai library and data",
    "text": "Importing fastai library and data\n\nfrom fastai.vision.all import *\n\n\nImageDataLoaders??\n\n\npath = untar_data(URLs.MNIST); path, path.ls()\n\n(Path('/home/fmussari/.fastai/data/mnist_png'),\n (#2) [Path('/home/fmussari/.fastai/data/mnist_png/training'),Path('/home/fmussari/.fastai/data/mnist_png/testing')])\n\n\n\n(path/'training').ls()\n\n(#10) [Path('/home/fmussari/.fastai/data/mnist_png/training/4'),Path('/home/fmussari/.fastai/data/mnist_png/training/7'),Path('/home/fmussari/.fastai/data/mnist_png/training/9'),Path('/home/fmussari/.fastai/data/mnist_png/training/5'),Path('/home/fmussari/.fastai/data/mnist_png/training/8'),Path('/home/fmussari/.fastai/data/mnist_png/training/0'),Path('/home/fmussari/.fastai/data/mnist_png/training/2'),Path('/home/fmussari/.fastai/data/mnist_png/training/1'),Path('/home/fmussari/.fastai/data/mnist_png/training/6'),Path('/home/fmussari/.fastai/data/mnist_png/training/3')]\n\n\nWe can see that the data is stored in training and testing folders.\nEach image is then stored in folders that represent its labels: 0, 1, 2, … 9."
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#file-names-and-transforms",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "File names and transforms",
    "text": "File names and transforms\n\nfnames = get_image_files(path)\nfnames\n\n(#70000) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/49105.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/746.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/13451.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/54187.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30554.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/30886.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/52580.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/38515.png')...]\n\n\n\nItem and Batch transforms\n\nitem_tfms = [CropPad(34), RandomCrop(size=28), ToTensor()]\nbatch_tfms = [IntToFloatTensor(), Normalize()]\n\n\nTransforms in action\nHere we are going to explore what each transform does.\n\nim = PILImageBW.create(fnames[0])\nim\n\n\n\n\n\nCropPad(50)(im)\n\n\n\n\n\nRandomCrop(size=28)(CropPad(50)(im))\n\n\n\n\nHave you ever tried reduce? Let’s the first two transforms sequentially with it:\n\nfrom functools import reduce\nreduce(lambda t,f: f(t), item_tfms[:2], im)\n\n\n\n\nWhich is equivalent to apply the transforms with a for loop:\n\nim0 = im\nfor tfm in item_tfms[:2]:\n    im0 = tfm(im0)\nim0\n\n\n\n\nWhat about applying the three transforms, also including ToTensor()\n\nreduce(lambda t,f: f(t), item_tfms, im).shape\n\ntorch.Size([1, 28, 28])\n\n\n\nToTensor()(RandomCrop(size=28)(CropPad(34)(im))).shape\n\ntorch.Size([1, 28, 28])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#high-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "High Level API",
    "text": "High Level API\nFor this level we are going to use ImageDataLoaders, let’s explore some of its methods:\n\nprint([f for f in dir(ImageDataLoaders) if f[:4]=='from'])\n\n['from_csv', 'from_dblock', 'from_df', 'from_dsets', 'from_folder', 'from_lists', 'from_name_func', 'from_name_re', 'from_path_func', 'from_path_re']\n\n\nFor this dataset from_folder method is the way to go.\n\nhelp(ImageDataLoaders.from_folder)\n\nHelp on method from_folder in module fastai.vision.data:\n\nfrom_folder(path, train='train', valid='valid', valid_pct=None, seed=None, vocab=None, item_tfms=None, batch_tfms=None, img_cls=<class 'fastai.vision.core.PILImage'>, *, bs: 'int' = 64, val_bs: 'int' = None, shuffle: 'bool' = True, device=None) method of builtins.type instance\n    Create from imagenet style dataset in `path` with `train` and `valid` subfolders (or provide `valid_pct`)\n\n\n\n\nData Loaders\n\ndls = ImageDataLoaders.from_folder(\n    path, train='training', valid='testing',\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dls)\n\nfastai.data.core.DataLoaders\n\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs #default batch size\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...],\n (#10000) [(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4)),(PILImage mode=RGB size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#mid-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Mid Level API",
    "text": "Mid Level API\nWith the mid level API we define blocks according to the problem. In this case we have image inputs and categories to predict.\n\nblocks\n\nblocks = (ImageBlock(cls=PILImageBW), CategoryBlock)\n\n\nGrandparentSplitter and parent_label\nIt was transparent to us in the high level API, but what from_folder method used to split the dataset into train and valid was GrandparentSplitter. Lets see how it works:\n\nsplitter = GrandparentSplitter(train_name='training', valid_name='testing')\n\nLet’s create a tiny subset of fnames, having training and testing samples, and see how splitter works:\n\nsub_fnames = fnames[:2] + fnames[-2:]\nsub_fnames\n\n(#4) [Path('/home/fmussari/.fastai/data/mnist_png/training/4/47823.png'),Path('/home/fmussari/.fastai/data/mnist_png/training/4/45709.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/7829.png'),Path('/home/fmussari/.fastai/data/mnist_png/testing/3/2300.png')]\n\n\n\nsplitter(sub_fnames)\n\n([0, 1], [2, 3])\n\n\nWe can see it is returnin a tuple with two list of indices, one for training, and one for validations, according to the folder the images are located in.\n\nt, v = splitter(fnames)\nlen(t), len(v)\n\n(60000, 10000)\n\n\nThe high level API also used the parent_label function under the hood. It returns the label from the parent folder of each image.\n\nparent_label(sub_fnames[0]), parent_label(sub_fnames[2])\n\n('4', '3')\n\n\n\nPILImageBW.create(sub_fnames[2])\n\n\n\n\n\n\n\nDataBlock\n\ndblock = DataBlock(\n    blocks=blocks,\n    get_items=get_image_files, \n    splitter=splitter,\n    get_y=parent_label,\n    item_tfms=item_tfms,\n    batch_tfms=batch_tfms\n)\n\n\ntype(dblock)\n\nfastai.data.block.DataBlock\n\n\n\n\nData Loaders\n\ndls = dblock.dataloaders(path, bs=64)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n64\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#low-level-api",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Low Level API",
    "text": "Low Level API\nWe already created a splitter, lets now split the data.\n\nsplits=splitter(fnames)\nlen(splits[0]), len(splits[1])\n\n(60000, 10000)\n\n\n\nDatasets\n\ndsrc = Datasets(\n    items=fnames,\n    tfms=[[PILImageBW.create], [parent_label, Categorize]],\n    splits=splits\n)\n\n\ntype(dsrc)\n\nfastai.data.core.Datasets\n\n\n\nshow_at(dsrc.train, 3);\n\n\n\n\n\nExploring what each tfms does\n\nPILImageBW.create(fnames[0])\n\n\n\n\n\nl = parent_label(fnames[0])\nl\n\n'4'\n\n\n\nCategorize(l)\n\nCategorize -- {'vocab': ['4'], 'sort': True, 'add_na': False}:\nencodes: (Tabular,object) -> encodes\n(object,object) -> encodes\ndecodes: (Tabular,object) -> decodes\n(object,object) -> decodes\n\n\n\n\n\nData Loaders\n\ndls = dsrc.dataloaders(\n    bs=128,\n    after_item=item_tfms,\n    after_batch=batch_tfms\n)\n\n\ndls.show_batch(figsize=(4,4))\n\n\n\n\n\ndls.bs\n\n128\n\n\n\nprint(dls.vocab.o2i)\n\n{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\ndls.train_ds, dls.valid_ds\n\n((#60000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...],\n (#10000) [(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4)),(PILImageBW mode=L size=28x28, TensorCategory(4))...])"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#raw-pytorch",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Raw PyTorch",
    "text": "Raw PyTorch\nJust as the previous fastai examples, this is based in the amazing Walk with fastai course whose Using Raw PyTorch lesson has the following note in the Dataset code. > This example is highly based on the work of Sylvain Gugger for the Accelerate notebook example which can be found here: https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb\n\nfrom torchvision.transforms import ToTensor, Normalize, Pad, RandomCrop\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\nTrain and validation Datasets\n\nclass MNISTDataset(Dataset):\n    def __init__(self, path:Path, transforms:nn.Sequential=None):       \n        self.transforms = transforms\n        self.paths = [f for folder in path.iterdir() for f in folder.iterdir()]     \n        self.to_tensor = ToTensor()\n        \n    def __len__(self):\n        return len(self.paths)\n    \n    def apply_x_transforms(self, filepath):\n        image = Image.open(filepath)#.convert(\"RGB\")\n        tensor_image = self.to_tensor(image)\n        return self.transforms(tensor_image)\n    \n    def apply_y_transforms(self, filepath):\n        label = filepath.parent.name\n        return int(label)\n    \n    def __getitem__(self, index):\n        filepath = self.paths[index]\n        x = self.apply_x_transforms(filepath)\n        y = self.apply_y_transforms(filepath)\n        return (x, y)\n\n\ntrain_transforms = nn.Sequential(\n    Pad(6),\n    RandomCrop(28)\n)\n\n\ntrain_dataset = MNISTDataset(path/'training', train_transforms)\nvalid_dataset = MNISTDataset(path/'testing')\nlen(train_dataset), len(valid_dataset)\n\n(60000, 10000)\n\n\nOne of the items in the dataset:\n\nx,y = train_dataset[0]\nx.shape, y\n\n(torch.Size([1, 28, 28]), 4)\n\n\n\n\nPyTorch Dataloader\n\nbs = 128\n\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    shuffle=True,\n    drop_last=False,\n    batch_size=bs\n)\n\n\nvalid_dataloader = DataLoader(\n    valid_dataset,\n    batch_size=bs*2\n)\n\n\nnext(iter(train_dataloader))[0].shape, next(iter(train_dataloader))[1].shape\n\n(torch.Size([128, 1, 28, 28]), torch.Size([128]))\n\n\n\nfigure = plt.figure(figsize=(4, 4))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n    img, label = train_dataset[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(label)\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\nThe next cell is just a curiosity leaned from twitter:\n\na = [1,2,3]\nb = [4,5,6]\n[i for i in (*a,*b)]\n\n[1, 2, 3, 4, 5, 6]"
  },
  {
    "objectID": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "href": "posts/fastai-api-levels-MNIST/API-levels-in-fastai-dataloaders.html#conclusion",
    "title": "Exploring fastai API levels to create DataLoaders",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe ended with the same fastai’s DataLoaders in three different ways, and then in an extra raw PyTorch way.\nAs Zachary puts it, the higher the level of the API, the lesser the flexibility, but there is also less complexity.\nHe also pointed out that the mid level API is made with the building blocks of the framework."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 2. Deploying the web app to Streamlit"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to take the SQLite database with the searchable index we created in Part 1 and use it as the search engine for a web app we are going to deploy to Streamlit.\nSo we are going to be able to search over the entire fastai channel.\nThe Python file was created with a Jupyter Notebook using nbdev.\nThe web app is just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)\nCreate A 🤗 Space From A Notebook"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5-Part2.html#creating-app.py",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Creating app.py",
    "text": "Creating app.py\n#|export\nimport streamlit as st\nimport sqlite3\n#| export\ndb_path = '/mnt/m/datamatica/posts/full-text-search-fastai-youtube-channel/'\n\ntry:\n    db = sqlite3.connect(db_path + 'fastai_yt.db')\nexcept:\n    db = sqlite3.connect('fastai_yt.db')\n\ncur = db.cursor()\n#| export\nplaylist = cur.execute('SELECT playlist_id, playlist_name FROM playlist').fetchall()\nvideo = cur.execute('SELECT video_id, video_name FROM video').fetchall()\nplaylist = {p: n for p, n in playlist}\nvideo = {p: n for p, n in video}\npl_sel = list(playlist.values())\npl_to_id = {v:k for k,v in playlist.items()}\n#| export\nst.title('Full-Text Search fastai Youtube Playlists')\n\n# https://discuss.streamlit.io/t/select-all-on-a-streamlit-multiselect/9799/2\n\nall_options = st.checkbox(\"Select all Playlists\",\n    key='sel_all', value=True)\n\ncontainer = st.container()\nif all_options:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, disabled=True)\nelse:\n    sel_options = container.multiselect(\"Select one or more Playlist(s):\", \n                                        pl_sel, pl_sel)\n\nif all_options: options = list(playlist.values())\nelse: options = sel_options\n\nst.write('Selected playlist(s):', options)\n#| export\ndef get_query(q, limit):\n    \n    search_in = 'text'\n    \n    if not( len(options)==len(pl_sel) or len(options)==0 ):\n        search_in = 'transcriptions_fts'\n        q_pl = '(playlist_id: '\n        for pl in options:\n            end = ' OR ' if pl != options[-1] else ')'\n            q_pl = q_pl + f'\"{pl_to_id[pl]}\"' + end\n        \n        q = f\"(text: {q}) AND \" + q_pl\n\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    return query\n\nwith st.form(\"Input\"):\n    queryText = st.text_area(\"Search query: \\ne.g. «fastc*», «fastcore OR paral*»\", height=1, max_chars=None)\n    limit_val = st.slider(\"Number of results:\", min_value=5, max_value=20)\n    btnResult = st.form_submit_button('Search')\n    \nif btnResult:\n    if not queryText:\n        st.text('Please enter a search query.')\n    else:\n        try:\n            st.text('Search query generated:')\n            # run query\n            st.write(get_query(queryText, limit_val).replace('*', '\\*'))\n            res = cur.execute(get_query(q=queryText, limit=limit_val)).fetchall()\n            st.text('Search results (click to go to YouTube):')\n\n            res_md = ('  \\n  '.join(['  \\n  '.join([\n                f\"{i}.- Playlist: `{playlist[each[0]]}`, Video: `{video[each[1]]}`\", \n                f\"Caption: '...[{each[4].replace('[','**'\n                ).replace(']','**')}](https://youtu.be/{each[1]}?t={str(int(each[2]))})...'\", \n                '\\n'])\n                for i, each in enumerate(res)\n            ]))\n\n            st.markdown(res_md)\n        except:\n            st.text('Invalid search query.')\n#| hide\nfrom nbdev.export import nb_export\nnb_export('_Deploy_Search_Engine_Streamlit.ipynb', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "",
    "text": "Part 1. Extracting transcriptions and creating SQLite’s searchable index"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#introduction",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Introduction",
    "text": "Introduction\nIn this post we are going to get the transcriptions of YouTube videos from one or more given Playlists. Here we are going to do it for fastai channel, but it can be done for any given list of playlists (if the videos have transcriptions).\nAfter we get the transcriptions, we are going to build a search engine with SQLite’s full-text search functionality provided by its FTS5 extension.\nIn Part 2 we are going to build and share the search engine as a Streamlit web app, just like this one: Full-Text Search Engine for fastai Youtube Chanel\n\nReferences\nIf you want to get deeper, I encourage you to read these articles:\n\nUseful Full Text Search (FTS) queries with sqlite in Python 3.7\nFast Autocomplete Search for Your Website\nYouTube Transcript/Subtitle API (including automatically generated subtitles and subtitle translations)"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#get-youtube-transcriptions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Get YouTube Transcriptions",
    "text": "Get YouTube Transcriptions\n\nInstall and Import Libraries\nWe need to first install the libraries we need (pytube and youtube-transcript-api).\nWe can use pip:\n$ pip install pytube\n$ pip install youtube_transcript_api\nOr conda:\n$ conda install -c conda-forge pytube\n$ conda install -c conda-forge youtube-transcript-api\n\nfrom youtube_transcript_api import YouTubeTranscriptApi\nfrom pytube import YouTube, Playlist\n\nimport sqlite3\n\n\n\nYouTube Playlists\nLet’s create a list of YouTube playlist ids. We can get them browsing YouTube playlist. The id is in the url which has the following format:\nhttps://www.youtube.com/playlist?list={PLAYLIST_ID}\n\nbase_pl = 'https://www.youtube.com/playlist?list='\nbase_yt = 'https://youtu.be/'\n\nyt_pl_ids = [\n    'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', # fast.ai APL Study Group #2022\n    'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU', # Practical Deep Learning for Coders 2022\n    'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM', # fast.ai live coding & tutorials #2022\n    'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM', # Practical Deep Learning for Coders (2020)\n    'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj', # Deep Learning from the Foundations #2019\n    'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq', # fastai v2 code walk-thrus #2019\n    'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn', # Practical Deep Learning for Coders 2019\n    'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96', # Introduction to Machine Learning for Coders\n    'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia', # Cutting Edge Deep Learning for Coders 2 #2018\n    'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM', # Practical Deep Learning For Coders 2018\n]\n\n\n\nGet Transcriptions\nLet’s explore the methods:\n\nplaylist = Playlist('https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU')\nprint(playlist.title)\nvideo = YouTube(playlist[0])\nprint(video.title)\nprint(playlist[0])\nvideo_id = playlist[0].split('=')[1]\nscript = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\nprint(script[0])\n\nPractical Deep Learning for Coders 2022\nLesson 1: Practical Deep Learning for Coders 2022\nhttps://www.youtube.com/watch?v=8SF_h3xF3cE\n{'text': 'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0', 'start': 2.0, 'duration': 8.0}\n\n\n\nDownload all transcriptions\nNow we are going to download all the transcriptions. Let’s create three dictionaries to store the data: - playlists to store each playlist as {playlist_id: playlist_name} - videos to store videos as {video_id: video_name} - database to store all captions as {playlist_id: {video_id: {'start': caption}}.\n\nplaylists = dict()\nvideos = dict()\ndatabase = dict()\n\nfor pl_id in yt_pl_ids:\n    playlist = Playlist(base_pl + pl_id)\n    print(playlist.title)\n    playlists[pl_id] = playlist.title\n    database[pl_id] = dict()\n\n    for video in playlist:\n        video_id = video.split(\"=\")[1]\n        videos[video_id] = YouTube(video).title\n        database[pl_id][video_id] = dict()\n        # Manually created transcripts are returned first\n        script = YouTubeTranscriptApi.get_transcript(video_id, languages=('en',))\n\n        for txt in script:\n            database[pl_id][video_id][txt['start']] = txt['text']\n\nfast.ai APL Study Group\nPractical Deep Learning for Coders 2022\nfast.ai live coding & tutorials\nPractical Deep Learning for Coders (2020)\nDeep Learning from the Foundations\nfastai v2 code walk-thrus\nPractical Deep Learning for Coders 2019\nIntroduction to Machine Learning for Coders\nCutting Edge Deep Learning for Coders 2\nPractical Deep Learning For Coders 2018"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#building-the-search-engine",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Building the Search Engine",
    "text": "Building the Search Engine\n\nFormatting the data to facilitate insertion into SQLite\n\n# https://stackoverflow.com/a/60932565/10013187\nrecords = [\n    (level1, level2, level3, leaf)\n    for level1, level2_dict in database.items()\n    for level2, level3_dict in level2_dict.items()\n    for level3, leaf in level3_dict.items()\n]\nprint(\"(playlist_id, video_id, start, text)\")\nprint(records[100])\n\n(playlist_id, video_id, start, text)\n('PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU', 'CGpR2ILao5M', 294.18, 'gonna go watch them or anything all')\n\n\n\n\nCreating the database\n\ndb = sqlite3.connect('fastai_yt.db')\ncur = db.cursor()\n\n\n# virtual table configured to allow full-text search\ncur.execute('DROP TABLE IF EXISTS transcriptions_fts;') \ncur.execute('CREATE VIRTUAL TABLE transcriptions_fts USING fts5(playlist_id, video_id, start, text, tokenize=\"porter unicode61\");')\n\n# dimension like tables\ncur.execute('DROP TABLE IF EXISTS playlist;')\ncur.execute('CREATE TABLE playlist (playlist_id, playlist_name);')\ncur.execute('DROP TABLE IF EXISTS video;')\ncur.execute('CREATE TABLE video (video_id, video_name);')\n\n<sqlite3.Cursor>\n\n\n\n# bulk index records\ncur.executemany('INSERT INTO transcriptions_fts (playlist_id, video_id, start, text) values (?,?,?,?);', records)\ncur.executemany('INSERT INTO playlist (playlist_id, playlist_name) values (?,?);', playlists.items())\ncur.executemany('INSERT INTO video (video_id, video_name) values (?,?);', videos.items())\ndb.commit()\n\nExample of a simple query:\n\ncur.execute('SELECT start, text FROM transcriptions_fts WHERE video_id=\"8SF_h3xF3cE\" LIMIT 5').fetchall()\n\n[(2.0,\n  'Welcome to Practical Deep Learning for coders,\\xa0\\nlesson one. This is version five of this course,\\xa0\\xa0'),\n (11.44,\n  \"and it's the first new one we've done\\xa0\\nin two years. So, we've got a lot of\\xa0\\xa0\"),\n (15.12,\n  \"cool things to cover! It's amazing how much has\\xa0\\nchanged. Here is an xkcd from the end of 2015.\\xa0\\xa0\"),\n (28.0,\n  'Who here has seen xkcd comics before?\\xa0\\n…Pretty much everybody. Not surprising.\\xa0\\xa0'),\n (35.36,\n  \"So the basic joke here is… I'll let you\\xa0\\nread it, and then I'll come back to it.\")]\n\n\nfastai_yt.db. Once we have the database populated, we can use it in any application we want without the need to get the transcriptions from YouTube."
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#search-queries",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Search queries",
    "text": "Search queries\n\ndef print_search_results(res):\n    for each in res:\n        print()\n        print(playlists[each[0]], \"->\", videos[each[1]])\n        print(f'\"... {each[4]}...\"')\n        print('https://youtu.be/' + each[1] + \"?t=\" + str(int(each[2])))\n\ndef get_query(q, limit):\n    search_in = 'text'\n    if 'text:' in q: search_in = 'transcriptions_fts'\n    query = f\"\"\"\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE {search_in} MATCH '{q}' ORDER BY rank\n    LIMIT \"{limit}\" \n    \"\"\"\n    print(query)\n    return query\n\n\nSearch for a word\n\nq = 'fastc*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'fastc*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfast.ai live coding & tutorials -> Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -> Live coding 2\n\"... fastgen so [fastchan] is a channel that...\"\nhttps://youtu.be/0pWjZByJ3Lk?t=3720\n\n\n\nq = 'deleg*'\nres = cur.execute(get_query(q, limit=5)).fetchall()\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE text MATCH 'deleg*' ORDER BY rank\n    LIMIT \"5\" \n    \n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #9\n\"... [delegated] down to that so [delegates] down...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2462\n\nDeep Learning from the Foundations -> Lesson 9 (2019) - How to train your model\n\"... [delegate] get attribute to the other...\"\nhttps://youtu.be/AcA8HAYh7IE?t=6435\n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #9\n\"... [delegate] everything Sodor in Python...\"\nhttps://youtu.be/bBqFVBpOZoY?t=2304\n\nDeep Learning from the Foundations -> Lesson 13 (2019) - Basics of Swift for Deep Learning\n\"... default [delegates] is probably going to...\"\nhttps://youtu.be/3TqN_M1L4ts?t=6750\n\nfastai v2 code walk-thrus -> fastai v2 walk-thru #2\n\"... this [delegates] decorator and what the...\"\nhttps://youtu.be/yEe5ZUMLEys?t=4756\n\n\n\n\nFaceted Search\nWe can limit the search to specific playlists in a faceted like search.\n\nplaylists\n\n{'PLfYUBJiXbdtSgU6S_3l6pX-4hQYKNJZFU': 'fast.ai APL Study Group',\n 'PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU': 'Practical Deep Learning for Coders 2022',\n 'PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM': 'fast.ai live coding & tutorials',\n 'PLfYUBJiXbdtRL3FMB3GoWHRI8ieU6FhfM': 'Practical Deep Learning for Coders (2020)',\n 'PLfYUBJiXbdtTIdtE1U8qgyxo4Jy2Y91uj': 'Deep Learning from the Foundations',\n 'PLfYUBJiXbdtSWRCYUHh-ThVCC39bp5yiq': 'fastai v2 code walk-thrus',\n 'PLfYUBJiXbdtSIJb-Qd3pw0cqCbkGeS0xn': 'Practical Deep Learning for Coders 2019',\n 'PLfYUBJiXbdtSyktd8A_x0JNd6lxDcZE96': 'Introduction to Machine Learning for Coders',\n 'PLfYUBJiXbdtTttBGq-u2zeY1OTjs5e-Ia': 'Cutting Edge Deep Learning for Coders 2',\n 'PLfYUBJiXbdtS2UQRzyrxmyVHoGW0gmLSM': 'Practical Deep Learning For Coders 2018'}\n\n\n\npl_lst = list(playlists.keys())\n\n\n# Search in playlist 'Practical Deep Learning for Coders 2022' or\n# 'fast.ai live coding & tutorials'\nq = f\"\"\"\n(text: fastcore OR paral*) AND \n(playlist_id: \"{pl_lst[1]}\" OR \"{pl_lst[2]}\")\n\"\"\"\nres = cur.execute(get_query(q, limit=10)).fetchall()\n\nprint_search_results(res)\n\n\n    SELECT *, HIGHLIGHT(transcriptions_fts, 3, '[', ']')\n    FROM transcriptions_fts WHERE transcriptions_fts MATCH '\n(text: fastcore OR paral*) AND \n(playlist_id: \"PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU\" OR \"PLfYUBJiXbdtSLBPJ1GMx-sQWf6iNhb8mM\")\n' ORDER BY rank\n    LIMIT \"10\" \n    \n\nPractical Deep Learning for Coders 2022 -> Lesson 6: Practical Deep Learning for Coders 2022\n\"... but my [fastcore] library has a [parallel] sub module \nwhich can basically do anything that you can do  ...\"\nhttps://youtu.be/AdhG64NF76E?t=3799\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... going to install python and [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=820\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... but for a library like [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2818\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... use the latest version of [fastcore]...\"\nhttps://youtu.be/B6BQiIgiEks?t=2975\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... no module named [fastcore] is actually...\"\nhttps://youtu.be/B6BQiIgiEks?t=3617\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1155\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1160\n\nfast.ai live coding & tutorials -> Live coding 3\n\"... and import [fastcore] it can't find it...\"\nhttps://youtu.be/B6BQiIgiEks?t=3401\n\nfast.ai live coding & tutorials -> Live coding 8\n\"... for [parallel]...\"\nhttps://youtu.be/-Scs4gbwWXg?t=1049\n\nfast.ai live coding & tutorials -> Live coding 15\n\"... somewhat in [parallel]...\"\nhttps://youtu.be/6JGoes9_bPs?t=5589"
  },
  {
    "objectID": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "href": "posts/full-text-search-fastai-youtube-channel/Full-Text-Search-fastai-Youtube-Playlists-SQLite-FTS5.html#conclusions",
    "title": "Building a Full-Text Search Engine for fastai YouTube channel",
    "section": "Conclusions",
    "text": "Conclusions\n\nWe used youtube-transcript-api and pytube Python libraries to extract YouTube captions based on the given playlists.\nWe indexed the captions using the capabilities of the ubiquitous SQLite and FTS5.\nWe did some powerful full-text search queries and simulated a faceted search.\nWe can go exactly to the video part the search is returning.\nIn Part 2 we are going to deploy an web app to Streamlit."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "",
    "text": "Part 1. Create the service and upload the pictures"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\nWe are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 will cover:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "References",
    "text": "References\n\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.1.-create-a-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.1. Create a Custom Vision Service",
    "text": "Part 1.1. Create a Custom Vision Service\nI’m not going to get into the details of creating the service. And the reason is that there is a detailed tutorial covering not just that, but also the code for uploading and training a simple model. I encourage you to try it first:\nDetect Objects in Images with Custom Vision\n\nFor this tutorial I created a Custom Vision with the following settings:\n\nCustom Vision service:\n\nResource: ai102cvision\nResource Kind: Custom Vision Training\n\nProject:\n\nName: Telecom Equipment Detection\nDescription: Detect different types of antennas\nResource: ai102cvision [F0]\nProject Types: Object Detection\nDomains: General"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.2.-upload-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.2. Upload the images",
    "text": "Part 1.2. Upload the images\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nTrainingEndpoint=YOUR_TRAINING_ENDPOINT\nTrainingKey=YOUR_TRAINING_KEY\nProjectID=YOUR_PROJECT_ID\n\n\n\n\n\n\nNote\n\n\n\nIn order to protect my credentials, I’m going to store .env file in a creds folder that isn’t being pushed to github.\n\n\n\nDOTENV_PATH = './.env'\n\n\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nFunctions\n\n# Borrowed from fastai library\ndef verify_image(fn):\n    \"Confirm that `fn` can be opened\"\n    try:\n        im = Image.open(fn)\n        im.draft(im.mode, (32,32))\n        im.load()\n        return True\n    except: return False\n    #except PIL.UnidentifiedImageError:\n\nThe SDK / API allows to upload images in batches but I didn’t find a way to match the local image name with the id generated by the service. Then I opted to create a function that uploads the pictures one by one.\n\nImageFileCreateEntry\nCustomVisionTrainingClient.create_images_from_files()\n\n\ndef Upload_Images_1by1(pictures: list[Path]) -> list('dict'):\n    \"\"\"Upload the pictures from a list of paths,\n    one by one to keep track of the relation between\n    local image name and Azure image id.\n    And to track the ones that python fails opening\n    \"\"\"\n    print(\"Uploading images...\")\n\n    processed_ids = []\n    processed_status = []\n    picture_names = []\n\n    for pic_path in pictures:\n\n        if verify_image(pic_path):\n            with open(pic_path, mode=\"rb\") as image_data:\n                image_entry = ImageFileCreateEntry(\n                    name=pic_path.name, contents=image_data.read()\n                )\n            \n            # Upload the list of (1) images as a batch\n            upload_result = training_client.create_images_from_files(\n                custom_vision_project.id, \n                # Creates an ImageFileCreateBatch from a list of 1 ImageFileCreateEntry\n                ImageFileCreateBatch(images=[image_entry])\n            )\n            # Check for failure\n            if not upload_result.is_batch_successful:\n                pic_status = upload_result.images[0].status\n                pic_id = None\n            else:\n                pic_status = upload_result.images[0].status\n                pic_id = upload_result.images[0].image.id\n        else:\n            pic_status = \"ReadError\" # Equivalente to SDK `ErrorSource`\n            pic_id = None\n        \n        processed_status.append(pic_status)\n        processed_ids.append(pic_id)\n        picture_names.append(pic_path.name)\n        print(pic_path.name, \"//\", pic_id, \"//\", pic_status)\n    \n    return {\"image_name\": picture_names, \n            \"image_id\": processed_ids, \n            \"image_status\": processed_status}\n\n\n\nExplore pictures\n\npics_folder = Path('./train_images')\n\npictures = sorted(list(pics_folder.iterdir()))\n\nprint(f\"There are {len(pictures)} pictures\")\n\nThere are 203 pictures\n\n\n\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\nfor i, ax in enumerate(axes.flat):\n    im = Image.open( pictures[i*10] )\n    ax = show_img(im, ax=ax)\n\n\n\n\nAs you can see the pictures are very varied. Different cameras, lighting conditions, focus, resolutions and sizes…\n\n\nUpload the pictures to Custom Vision Service\n\nuploaded_images_df = pd.DataFrame(columns=[\"image_name\", \"image_id\", \"image_status\"])\n\n\nupload_batch = Upload_Images_1by1(pictures)\n\n\nuploaded_images_df = pd.DataFrame(upload_batch)\nuploaded_images_df\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      198\n      torre cerro el pavon 075.jpg\n      b6dd061a-a68d-4d91-a39f-711968445571\n      OK\n    \n    \n      199\n      torre cerro el pavon 080.jpg\n      d12264cf-3d7b-469c-9445-da8dce8dabef\n      OK\n    \n    \n      200\n      torre cerro el pavon 085.jpg\n      c6d587fe-5f3a-46ea-bc04-7ff54f10b4ae\n      OK\n    \n    \n      201\n      torre cerro el pavon 086.jpg\n      ea34cbad-8d50-4b5f-aed0-91d7fe40a754\n      OK\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n203 rows × 3 columns\n\n\n\n\nprint(f\"{sum(uploaded_images_df.image_status != 'OK')} \n      images failed when uploading\")\n\n0 images failed uploading\n\n\nSave a csv:\n\nuploaded_images_df.to_csv('20221012_203_Images_Uploaded.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#part-1.3.-explore-data-from-custom-vision-service",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Part 1.3. Explore Data from Custom Vision Service",
    "text": "Part 1.3. Explore Data from Custom Vision Service\n\nGet id’s of uploaded images\n\nCustomVisionTrainingClient.get_images()\n\n\ntrain_images = training_client.get_images(\n    project_id=custom_vision_project.id,\n    take=250,\n    skip=0\n)\n\n\nprint(f\"There are {len(train_images)} training images in the service.\")\nprint(f\"Each image has a type of {type(train_images[0])}.\")\n\nThere are 203 training images in the service.\nEach image has a type of <class 'azure.cognitiveservices.vision.customvision.training.models._models_py3.Image'>.\n\n\nSome properties of the image class:\n\nimage = train_images[0]\nprint(f\"image.id: {image.id}\")\nprint(f\"image.width: {image.width}\")\nprint(f\"image.height: {image.height}\")\n\nimage.id: 6e274dfc-411a-4bf3-9151-51b96f662248\nimage.width: 1188\nimage.height: 900\n\n\n\nimage.original_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/o-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=ru8DNhvBrpA46oZtmzNP7CRHSkwGugumb3R%2F3IzJaUE%3D'\n\n\n\nimage.resized_image_uri\n\n'https://irisprodeutraining.blob.core.windows.net:443/i-f6cb4ba75bbe46a4883669654dc86f3a/i-6e274dfc411a4bf3915151b96f662248?sv=2020-04-08&se=2022-10-16T22%3A23%3A43Z&sr=b&sp=r&sig=U5UQ6tjjdLF5gZHFR6wrrWk8B0w9at4cIUeYyxylx2E%3D'\n\n\nOf course there are no tags yet:\n\nprint(f\"image.tags: {image.tags}\")\n\nimage.tags: None\n\n\n\n\nThe images are resized when uploaded\nLet’s see the same image locally:\n\nuploaded_images_df[uploaded_images_df.image_id==image.id]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n    \n  \n  \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n    \n  \n\n\n\n\n\nlocal_image = uploaded_images_df[\n    uploaded_images_df.image_id=='6e274dfc-411a-4bf3-9151-51b96f662248'\n].image_name.item()\nlocal_image\n\n'torre cerro el pavon 087.jpg'\n\n\n\nim = Image.open(pics_folder / local_image)\nim.size\n\n(2576, 1952)\n\n\nThe local image has a size of (2576, 1952) and was resized to (1188, 900) by the service\n\n\nKeep track of original size vs. size in the service\nTo get the real width and height we need to consider EXIF metadata. That’s because local images are sometimes rotated by the viewer with some app viewer.\n\nSize of local images\n\n# The image has some EXIF meta data including information about orientation (rotation)\n# https://stackoverflow.com/a/63950647\n    \nlocal_w = []\nlocal_h = []\n\nfor image in uploaded_images_df.image_name:\n    im = Image.open(pics_folder / image)\n    im = ImageOps.exif_transpose(im)\n\n    local_w.append(im.size[0])\n    local_h.append(im.size[1])\n\n\nlocal_w[:5], local_h[:5]\n\n([640, 1620, 1620, 2160, 2160], [480, 2160, 2160, 1620, 1620])\n\n\n\nuploaded_images_df['ori_w'] = local_w\nuploaded_images_df['ori_h'] = local_h\nuploaded_images_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n    \n  \n  \n    \n      0\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n    \n    \n      1\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n    \n    \n      2\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n    \n    \n      3\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n    \n    \n      4\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n    \n  \n\n\n\n\n\n\nSize of images in the service\n\nservice_ids = [im.id for im in train_images]\nservice_w = [im.width for im in train_images]\nservice_h = [im.height for im in train_images]\n\n\nservice_w = {id: w for id, w in zip(service_ids, service_w)}\nservice_h = {id: h for id, h in zip(service_ids, service_h)}\n\nuploaded_images_df['train_w'] = uploaded_images_df['image_id'].map(service_w)\nuploaded_images_df['train_h'] = uploaded_images_df['image_id'].map(service_h)\n\n\n\n\nChecking consistency in the ratios\n\nori_ratio = uploaded_images_df.ori_w / uploaded_images_df.ori_h\ntrain_ratio = uploaded_images_df.train_w / uploaded_images_df.train_h\nall(abs(ori_ratio - i_ratio) > .3)\n\nFalse\n\n\nImages that has an inconsistent ratio:\n\nuploaded_images_df[abs(ori_ratio - train_ratio) > 0.1]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      480\n      640\n      640\n      480\n    \n  \n\n\n\n\n\nim = Image.open( pics_folder / 'TORRE EL TIGRITO 01.jpg' )\nshow_img(im);\n\n\n\n\n\nim.size\n\n(640, 480)\n\n\nImageOps.exif_transpose failed for this image.\nBut if you don’t use it, more images would be inconsistent.\nIf seems that exif_transpose keep track of manually rotated images.\n\nim = ImageOps.exif_transpose(im)\nim.size\n\n(480, 640)\n\n\n\nfilter = uploaded_images_df.image_id == '2563fffe-d621-4799-8e81-6ad57049cdaa'\nuploaded_images_df.loc[filter, ['ori_w', 'ori_h']] = (640, 480)\nuploaded_images_df[filter]\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      179\n      TORRE EL TIGRITO 01.jpg\n      2563fffe-d621-4799-8e81-6ad57049cdaa\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\n\nExporting csv with size data\n\nuploaded_images_df.sample(10)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n  \n  \n    \n      155\n      P1100700.JPG\n      b10efb57-70a4-48d6-a846-121ded4546f8\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      7\n      CUMACA 11.jpg\n      2c55467b-5de5-4329-91d2-a2fafdedd080\n      OK\n      2592\n      1944\n      1200\n      900\n    \n    \n      49\n      IMG_1170.JPG\n      ce2177ae-d03e-4a61-9dfb-4229542572fe\n      OK\n      480\n      640\n      480\n      640\n    \n    \n      141\n      MVC-024S.JPG\n      9ba84daa-e00c-4975-a07b-3ae23ef8f884\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      136\n      Imagen008.jpg\n      c861b4de-127a-4dc0-84ea-9cb96fb380f2\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      202\n      torre cerro el pavon 087.jpg\n      6e274dfc-411a-4bf3-9151-51b96f662248\n      OK\n      2576\n      1952\n      1188\n      900\n    \n    \n      145\n      P1100611.JPG\n      1148d437-fc44-4c51-af4a-4751e242b3b7\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      147\n      P1100613.JPG\n      c9dab11e-0663-42f8-8c93-4e2351b15d4c\n      OK\n      2048\n      1360\n      1355\n      900\n    \n    \n      171\n      PICT0386.JPG\n      0e51a561-b938-48f6-8bc6-3c3bf4c72c44\n      OK\n      2560\n      1920\n      1200\n      900\n    \n    \n      142\n      MVC-025S.JPG\n      a8c2a746-2a65-4872-b7b5-0bd5edf965c9\n      OK\n      640\n      480\n      640\n      480\n    \n  \n\n\n\n\n\nuploaded_images_df.to_csv('20221015_203_Images_Uploaded_WxH.csv', index=False)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_1.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 1",
    "section": "Conslusions",
    "text": "Conslusions\n\nIt was straightforward to upload images to the service.\nBig images got resized, but their ratios were kept.\nexif_transpose needs to be used to get the real width and height of the image, which may be different to the original size. For example when the image is rotated manually when looking at it. But somehow it failed with one of the images."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "",
    "text": "Part 2. Label images with Smart Labeler"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Introduction",
    "text": "Introduction\nIn this series of posts we are going to follow along the process and code required to train an object detection model using Azure Custom Vision (in its free tier).\n-> We are going to use real world pictures compiled from work I have done over the years in Venezuela. In this kind of supervised learning problem we need tagged images. So we will use Smart Labeler to do that.\nAfter the model is published in Azure service, we can use the API to build and share a demo with Gradio and Huggingface.\nHere is the one that is already published for you to try:\nTelecom-Object-Detection\n\nThe model will be trained to detect the following objects:\n\nGrid Antenna\nPanel antenna\nRadome antenna\nRRU\nShroud antenna\nSolid antenna\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrid\n\n\nPanel\n\n\nRadome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRU\n\n\nShroud\n\n\nSolid"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3 will cover:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "References",
    "text": "References\n\nCustom Vision Documentation: Label images faster with Smart Labeler\nMicrosoft Learn Excersice: Detect Objects in Images with Custom Vision\nCustom Vision Documentation: Quickstart: Create an object detection project with the Custom Vision client library\nREST API Endpoint: Custom Vision REST API reference - Azure Cognitive Services\nAPIs Documentation: Custom_Vision_Training_3.3\nAzure SDK for Python: Custom Vision Client Library\nSource Code: Azure/azure-sdk-for-python"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.1.-labeling-the-images",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.1. Labeling the Images",
    "text": "Part 2.1. Labeling the Images\nSmart Labeler is a simple tool for labeling images. It can be used for classification and object detection problems. When working in this problem I missed the ability to zoom-in when labeling some small objects, but as I said, this is a straightforward tool.\nFor speeding up bigger projects it might be usefull that you can first label some pictures, then train and get suggestions for the untagged images, but I didn’t use it. By default the labeler tries to give suggestions even without that first training.\nThe process is simple and you can the use the annotation to train models outside the service (as we are going to try after this series, hopefully, using fastai).\n\nInstall and import libraries\nWe need to install Custom Vision’s Python SDK and python-dotenv:\n! pip install azure-cognitiveservices-vision-customvision==3.1.0\n! pip install python-dotenv\n\nfrom azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\nfrom azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region, ImageRegionCreateEntry\nfrom msrest.authentication import ApiKeyCredentials\nimport time\nimport json\nimport os\n\nimport pandas as pd\nimport numpy as np\nfrom dotenv import load_dotenv\nfrom pathlib import Path\n\nfrom PIL import Image, ImageOps\nfrom PIL import UnidentifiedImageError\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, patheffects\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n\nDOTENV_PATH = './.env'\n\n\nload_dotenv(DOTENV_PATH)\ntraining_endpoint = os.getenv('TrainingEndpoint')\ntraining_key = os.getenv('TrainingKey')\nproject_id = os.getenv('ProjectID')\n\ncredentials = ApiKeyCredentials(in_headers={\"Training-key\": training_key})\ntraining_client = CustomVisionTrainingClient(training_endpoint, credentials)\ncustom_vision_project = training_client.get_project(project_id)\n\n\n\nCreating Labels\nSince I already did the manual tagging, we can use those tags in this new project.\nFirst we need to create the labels/tags in the service: - CustomVisionTrainingClient.create_tag()\n\ntags = ['Grid', 'Panel', 'Radome', 'RRU', 'Shroud', 'Solid']\ndesc = ['Grid Antenna', 'Panel Cel. Antenna', 'Radome Antenna', \n        'RRU Equipment', 'Shroud Antenna', 'Solid Antenna']\n\n\nservice_tags = []\nfor i, tag in enumerate(tags):\n    service_tags.append(\n        training_client.create_tag(\n            project_id=project_id, name=tag,\n            description=desc[i]\n        )\n    )\nservice_tags\n\n[<azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>,\n <azure.cognitiveservices.vision.customvision.training.models._models_py3.Tag>]\n\n\nNow we can see this in the service:\n\n\nCustomVisionTrainingClient.get_tags()\n\n\nservice_tags = training_client.get_tags(project_id=project_id)\n\n\nservice_tag_ids = {tag.name: tag.id for tag in service_tags}\nservice_tag_ids\n\n{'RRU': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n 'Shroud': '4e413c15-141a-419b-a958-1485008b2904',\n 'Solid': '3f13d9b0-7b4d-4679-8fb8-7855cea0a118',\n 'Radome': 'a1020654-79c5-4d8a-867c-93dfb2a4a81d',\n 'Grid': 'e016b6a4-49e6-4897-a0c7-d8fc64d032f1',\n 'Panel': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e'}\n\n\n\n\nUpload Regions from json file\nAs I pointed before, you can create all the regions with Smart Labeler. Since I did that already in a previos project, I updated the region’s image ids and tags to the ones in this project and save them as a json.\n\nCustomVisionTrainingClient.create_image_regions()\n\nWe can see from the documentation that “There is a limit of 64 entries in a batch.”\n\nwith open(\"20221016_CreateImageRegions_Body.json\") as json_file:\n    regions_dict = json.load(json_file)\n\nprint(f'We have a total of {len(regions_dict[\"regions\"]):_} regions.')\nprint()\nprint('The first two regions:')\nregions_dict['regions'][:2]\n\nWe have a total of 1_279 regions.\n\nThe first two regions:\n\n\n[{'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': '91ffb5b0-fe25-4d72-9c65-14793183a3b9',\n  'left': 0.6395582,\n  'top': 0.0,\n  'width': 0.10740108,\n  'height': 0.14776269},\n {'imageId': '6e274dfc-411a-4bf3-9151-51b96f662248',\n  'tagId': 'c9b15b62-6823-44a4-8fee-fa9d84e65a7e',\n  'left': 0.772766,\n  'top': 0.16059849,\n  'width': 0.22664931,\n  'height': 0.40633526}]\n\n\n\n# Create batches of 60 regions\n\nregions = regions_dict['regions']\n\nfor i in range(int(1_279 / 60)+1):\n    \n    batch_regions = []\n    print(f'Creating Regions {i*60+1:>{5}_} to {min((i+1)*60, 1_279):>{5}_}')\n    \n    for region in regions[i*60: (i+1)*60]:\n        batch_regions.append(\n            ImageRegionCreateEntry(\n                image_id=region['imageId'],\n                tag_id=region['tagId'],\n                left=region['left'], top=region['top'],\n                width=region['width'], height=region['height']\n        ))\n\n    training_client.create_image_regions(\n        project_id=project_id, \n        regions=batch_regions\n    )\n\nCreating Regions     1 to    60\nCreating Regions    61 to   120\nCreating Regions   121 to   180\nCreating Regions   181 to   240\nCreating Regions   241 to   300\nCreating Regions   301 to   360\nCreating Regions   361 to   420\nCreating Regions   421 to   480\nCreating Regions   481 to   540\nCreating Regions   541 to   600\nCreating Regions   601 to   660\nCreating Regions   661 to   720\nCreating Regions   721 to   780\nCreating Regions   781 to   840\nCreating Regions   841 to   900\nCreating Regions   901 to   960\nCreating Regions   961 to 1_020\nCreating Regions 1_021 to 1_080\nCreating Regions 1_081 to 1_140\nCreating Regions 1_141 to 1_200\nCreating Regions 1_201 to 1_260\nCreating Regions 1_261 to 1_279\n\n\nExample image, capture from the service:\n\n\n\nVerifying the number of created Regions\n\nCustomVisionTrainingClient.get_images()\n\n\nall_tagged_images = training_client.get_images(\n    project_id=project_id,\n    tagging_status=\"Tagged\", \n    take=250   # Max 256\n)\ni = 0\nfor im in all_tagged_images: i += len(im.regions)\nprint(f\"Number of created Regions: {i:_}\")\n\nNumber of created Regions: 1_279\n\n\n\n\nDraw some regions\n\nimages_df = pd.read_csv('20221015_203_Images_Uploaded_WxH.csv')\nimages_df.index = images_df.image_id\nimages_df.head(5)\n\n\n\n\n\n  \n    \n      \n      image_name\n      image_id\n      image_status\n      ori_w\n      ori_h\n      train_w\n      train_h\n    \n    \n      image_id\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      41.JPG\n      452a0b58-0dc5-41ff-83d1-8d1ae7bd5d1c\n      OK\n      640\n      480\n      640\n      480\n    \n    \n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      CIMG0030.JPG\n      96b7774e-f5ad-4591-aa71-99ad5c71135e\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      CIMG0031.JPG\n      3027bc7e-6e21-4b13-a7d7-bb7e08ce6824\n      OK\n      1620\n      2160\n      900\n      1200\n    \n    \n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      CIMG0056.JPG\n      1320ab2e-3405-4853-bd7e-b0ef0f915d4b\n      OK\n      2160\n      1620\n      1200\n      900\n    \n    \n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      CIMG0059.JPG\n      aa67eceb-3db0-4026-bf16-0842c006e6ac\n      OK\n      2160\n      1620\n      1200\n      900\n    \n  \n\n\n\n\nCreate a dictionary to easily access all regions from an image id:\n\nimg2ann = dict()\n\nfor image in all_tagged_images:\n    img2ann[image.id] = tuple([list(), list()])\n    image_w = image.width; image_h = image.height\n    ori_w = images_df.loc[image.id].ori_w\n    ori_h = images_df.loc[image.id].ori_h\n    for region in image.regions:\n        img2ann[image.id][1].append(region.tag_name)\n        img2ann[image.id][0].append([\n            region.left*ori_w, \n            region.top*ori_h, \n            region.width*ori_w, \n            region.height*ori_h\n        ])\n\n\npics_folder = Path('./train_images')\n\n\n# https://youtu.be/Z0ssNAbe81M?t=4636\ndef show_img(im, figsize=None, ax=None):\n    if not ax: fig, ax = plt.subplots(figsize=figsize)\n    ax.imshow(im)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    return ax\n\ndef draw_outline(o, lw):\n    o.set_path_effects([patheffects.Stroke(\n        linewidth=lw, foreground='black'), patheffects.Normal()\n    ])\n\ndef draw_rect(ax, b):\n    patch = ax.add_patch(\n        patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=1)\n    )\n    draw_outline(patch, 4)\n\ndef draw_text(ax, xy, txt, sz=14):\n    text = ax.text(*xy, txt,\n        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n    draw_outline(text, 1)\n\n\ndef draw_regions(index=0):\n    im = Image.open( pics_folder / images_df.iloc[index].image_name )\n    ax = show_img(im, figsize=(8,8))\n\n    reg, lab = img2ann[images_df.iloc[index].image_id]\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(index=0)\n\n[[329.19859199999996, 205.3586496, 53.42696959999999, 114.2365248], [249.3986112, 264.75866399999995, 112.4269696, 85.23652799999999]]\n\n\n\n\n\nA dragon-fly was cought in that picture!\n\ndraw_regions(index=100)"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#part-2.2.-train-and-test-a-model",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Part 2.2. Train and Test a Model",
    "text": "Part 2.2. Train and Test a Model\n\nTrain the model\n\nCustomVisionTrainingClient.train_project()\n\n\ntrain_iteration = training_client.train_project(\n    project_id=project_id,\n    training_type='Regular'\n)\n\n\ntrain_iteration.as_dict()\n\n{'id': 'd0006e20-33dd-4806-9fe9-cfc3fca82552',\n 'name': 'Iteration 1',\n 'status': 'Training',\n 'created': '2022-10-12T13:34:38.120Z',\n 'last_modified': '2022-10-22T14:56:30.406Z',\n 'project_id': 'f6cb4ba7-5bbe-46a4-8836-69654dc86f3a',\n 'exportable': False,\n 'training_type': 'Regular',\n 'reserved_budget_in_hours': 0,\n 'training_time_in_minutes': 0}\n\n\n\nCustomVisionTrainingClient.get_iteration_performance()\n\n\nperformance = training_client.get_iteration_performance(\n    project_id=project_id,\n    iteration_id=train_iteration.id\n).as_dict()\n\n   \nfor tag in performance['per_tag_performance']:\n    print('/'*20)\n    print('tag:', tag['name'])\n    print('image count:', training_client.get_tag(\n        project_id=project_id, tag_id=service_tags[tag['name']]\n    ).image_count)\n    print('recall:', tag['recall'])\n    print('average_precision:', tag['average_precision'])\n\n////////////////////\ntag: Shroud\nimage count: 140\nrecall: 0.35789475\naverage_precision: 0.7280897\n////////////////////\ntag: Panel\nimage count: 68\nrecall: 0.11392405\naverage_precision: 0.3710658\n////////////////////\ntag: Solid\nimage count: 88\nrecall: 0.21428572\naverage_precision: 0.4641156\n////////////////////\ntag: Grid\nimage count: 80\nrecall: 0.10526316\naverage_precision: 0.3784035\n////////////////////\ntag: Radome\nimage count: 20\nrecall: 0.0\naverage_precision: 0.051538005\n////////////////////\ntag: RRU\nimage count: 32\nrecall: 0.13043478\naverage_precision: 0.48053658\n\n\nSome things that I would take into account now that negatively impact the model performance: - I choose many images with small boxes. - Some tags are not represented equally, so we ended an unbalanced distribution. - And of course lets remember we only did a quick train.\nThis is a very good thread on some tips and tricks to improve object detection:\n\n\n😨 Training an Object Detection Model is a very challenging task and involves tweaking so many knobsHere is an exhaustive 🎁 tips & tricks list 🎁 that you could use to boost your model performance 🧵 pic.twitter.com/sOvEUhCCwg\n\n— AI Fast Track (@ai_fast_track) October 20, 2022\n\n\n\n\nTest the model (Quick Test)\nQuick test allows to test the model without publishing a prediction API.\n\n# Load image and get height, width and channels\nimage_file = Path(\"./test_images/las-palmas-at-60-(20).jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\nCustomVisionTrainingClient.quick_test_image()\n\n\n# Detect objects in the test image\nprint('Detecting objects in', image_file)\n\nwith open(image_file, mode=\"rb\") as image_data:\n    results = training_client.quick_test_image(\n        project_id=project_id, \n        image_data=image_data,\n        iteration_id=train_iteration.id\n    )\n\nDetecting objects in test_images/las-palmas-at-60-(20).jpg\n\n\n\ndef get_reg_lab(results):\n    reg = []; lab = []\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            left = prediction.bounding_box.left * w\n            top = prediction.bounding_box.top * h\n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n            reg.append([left, top, width, height])\n            lab.append(prediction.tag_name)\n    return reg, lab\n\n\ndef draw_regions(image):\n    \n    ax = show_img(image, figsize=(8,8))\n    reg, lab = get_reg_lab(results)\n    for idx, region in enumerate(reg):\n        draw_rect(ax, np.array(region))\n        tag = lab[idx]\n        draw_text(ax, region[:2], tag)\n\n\ndraw_regions(image)\n\n\n\n\nAs you can see, it didn’t detect some antennas. But taking into account we did a regular training and the limitations mentioned in the training data, it is impressive that it got some right in such a complex problem as object detection.\n\nimage_file = Path(\"./test_images/DSC09399.jpg\")\nimage = Image.open(image_file)\nh, w, ch = np.array(image).shape\nshow_img(image, figsize=(8,8));\n\n\n\n\n\ndraw_regions(image)\n\n\n\n\nNot a good job in this one. But this result is with a “regular” training (quick).\nYou can see in the Gradio demo Telecom Object Detection with Azure Custom Vision that the model trained for 1 hour (free tier limit) does a better job with this picture."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_2.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 2",
    "section": "Conslusions",
    "text": "Conslusions\n\nObject detection is a complex problem. The fact that the service does a reasonable good job with unbalanced training photos and with such a limited training time talks about the great margin for improvement."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "",
    "text": "Part 3. Deploy Gradio Web App"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#introduction",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Introduction",
    "text": "Introduction\nAfter we train the model in Azure for 1 hour (free tier) and publishing it, when end up with a Prediction URL.\n\nWe are going to use that Prediction endpoint to do the inference.\nHere is the App already published for you to try:\nTelecom-Object-Detection\nAnd here the repository:\nhttps://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#tutorial-parts",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Tutorial Parts",
    "text": "Tutorial Parts\n\nPart 1 covered:\n\nCreating a free Azure Custom Vision Service.\nUploading the images to the service.\n\nPart 2 covered:\n\nAnalyzing what happens to the images after uploading.\nHow to label the images using Smart Labeler\nTraining and testing the model.\n\nPart 3:\n\nCreate a Huggingface Gradio Demo."
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#references",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "References",
    "text": "References\n\nCreate A 🤗 Space From A Notebook\nBuild & Share Delightful Machine Learning Apps"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#part-3.1.-publishing-a-gradio-app",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Part 3.1. Publishing a Gradio App",
    "text": "Part 3.1. Publishing a Gradio App\nGradio is a great tool to demo machine learning models. The model is already deployed in Azure, so our Gradio App is going to be our front end to connect to that prediction endpoint. What I mean is that the model itself is not going to be deployed in Hugging Face Spaces, which is the normal workflow.\nIf you are new to Gradio, I encourage you to start from the Quickstart.\nThe Gradio demo was created from a Jupyter Notebook with a great tool from fast.ai which is nbdev. You can start learning the basics here: Create A 🤗 Space From A Notebook\nIn both tutorials you will find the instructions to setup a Gradio enabled space in Hugging Face.\nThis code is based and adapted from:\n- https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py\n- https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py\n\nInstall and import libraries\n#|export\n\nimport gradio as gr\nimport numpy as np\nimport os\nimport io\n\nimport requests, validators\n\nfrom pathlib import Path\n\n\nAzure Functions\n#| export\n\nfrom azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\nfrom msrest.authentication import ApiKeyCredentials\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom dotenv import load_dotenv\n\n\nEnvironment variables\nUpdate the configuration variables in the .env file that contains:\nPredictionEndpoint=YOUR_PREDICTION_ENDPOINT\nPredictionKey=YOUR_PREDICTION_KEY\nProjectID=YOUR_PROJECT_ID\nModelName=YOUR_PUBLISHED_MODEL\nWe need to create these environment variables in the Hugging Face Spaces repository under Settings -> Repo Secrets:\n\n\n\nCredentials and services\n\nApiKeyCredentials\nCustomVisionTrainingClient\nCustomVisionTrainingClient.get_project()\n\n#| export\n\ndef fig2img(fig):\n    buf = io.BytesIO()\n    fig.savefig(buf)\n    buf.seek(0)\n    img = Image.open(buf)\n    return img\n    \ndef custom_vision_detect_objects(image_file: Path):\n    dpi = 100\n\n    # Get Configuration Settings\n    load_dotenv()\n    prediction_endpoint = os.getenv('PredictionEndpoint')\n    prediction_key = os.getenv('PredictionKey')\n    project_id = os.getenv('ProjectID')\n    model_name = os.getenv('ModelName')\n\n    # Authenticate a client for the training API\n    credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n    prediction_client = CustomVisionPredictionClient(\n        endpoint=prediction_endpoint, credentials=credentials)\n\n    # Load image and get height, width and channels\n    #image_file = 'produce.jpg'\n    print('Detecting objects in', image_file)\n    image = Image.open(image_file)\n    h, w, ch = np.array(image).shape\n\n    # Detect objects in the test image\n    with open(image_file, mode=\"rb\") as image_data:\n        results = prediction_client.detect_image(project_id, model_name, image_data)\n    \n    # Create a figure for the results\n    fig = plt.figure(figsize=(w/dpi, h/dpi))\n    plt.axis('off')\n\n    # Display the image with boxes around each detected object\n    draw = ImageDraw.Draw(image)\n    lineWidth = int(w/800)\n    color = 'cyan'\n\n    for prediction in results.predictions:\n        # Only show objects with a > 50% probability\n        if (prediction.probability*100) > 50:\n            # Box coordinates and dimensions are proportional - convert to absolutes\n            left = prediction.bounding_box.left * w \n            top = prediction.bounding_box.top * h \n            height = prediction.bounding_box.height * h\n            width =  prediction.bounding_box.width * w\n\n            # Draw the box\n            points = ((left,top), (left+width,top), \n                      (left+width,top+height), (left,top+height), \n                      (left,top))\n            draw.line(points, fill=color, width=lineWidth)\n\n            # Add the tag name and probability\n            plt.annotate(\n                prediction.tag_name + \": {0:.0f}%\".format(prediction.probability * 100),\n                (left, top-1.372*h/dpi), \n                backgroundcolor=color,\n                fontsize=max(w/dpi, h/dpi), \n                fontfamily='monospace'\n            )\n\n    plt.imshow(image)\n    plt.tight_layout(pad=0)\n    \n    return fig2img(fig)\n\n    outputfile = 'output.jpg'\n    fig.savefig(outputfile)\n    print('Resulabsts saved in ', outputfile)\n\n\nGradio\n#| export\n\ntitle = \"\"\"<h1 id=\"title\">Telecom Object Detection with Azure Custom Vision</h1>\"\"\"\n\ncss = \"\"\"\nh1#title {\n  text-align: center;\n}\n\"\"\"\nExample images and url to be used in the App\n#| export\n\nurls = [\"https://www.dropbox.com/s/y5bk8om5ucu46d3/747.jpg?dl=1\"]\nimgs = [path.as_posix() for path in sorted(Path('images').rglob('*.jpg'))]\nimg_samples = [[path.as_posix()] for path in sorted(Path('images').rglob('*.jpg'))]\nFunctions for the Gradio App\n#| export\n\ndef set_example_url(example: list) -> dict:\n    print(gr.Textbox.update(value=example[0]))\n    return gr.Textbox.update(value=example[0])\n\ndef set_example_image(example: list) -> dict:\n    return gr.Image.update(value=example[0])\n\ndef detect_objects(url_input:str, image_input:Image):\n    print(f\"{url_input=}\")\n    if validators.url(url_input):\n        image = Image.open(requests.get(url_input, stream=True).raw)\n    elif image_input:\n        image = image_input\n        \n    print(image)\n    print(image.size)\n    w, h = image.size\n    \n    if max(w, h) > 1_200:\n        factor = 1_200 / max(w, h)\n        factor = 1\n        size = (int(w*factor), int(h*factor))\n        image = image.resize(size, resample=Image.Resampling.BILINEAR)\n    \n    resized_image_path = \"input_object_detection.jpg\"\n    image.save(resized_image_path)\n    \n    return custom_vision_detect_objects(resized_image_path)\n#| export\n\nwith gr.Blocks(css=css) as demo:\n    \n    gr.Markdown(title)\n    \n    with gr.Tabs():\n        with gr.TabItem(\"Image Upload\"):\n            with gr.Row():\n                image_input = gr.Image(type='pil')\n                image_output = gr.Image(shape=(650,650))\n                \n            with gr.Row(): \n                example_images = gr.Dataset(components=[image_input], samples=img_samples)\n            \n            image_button = gr.Button(\"Detect\")\n        \n        with gr.TabItem(\"Image URL\"):\n            with gr.Row():\n                url_input = gr.Textbox(lines=2, label='Enter valid image URL here..')\n                img_output_from_url = gr.Image(shape=(650,650))\n                \n            with gr.Row():\n                example_url = gr.Dataset(components=[url_input], samples=[[str(url)] for url in urls])\n            url_button = gr.Button(\"Detect\")\n            \n    url_button.click(detect_objects, inputs=[url_input,image_input], outputs=img_output_from_url)\n    image_button.click(detect_objects, inputs=[url_input,image_input], outputs=image_output)\n    \n    example_url.click(fn=set_example_url, inputs=[example_url], outputs=[url_input])\n    example_images.click(fn=set_example_image, inputs=[example_images], outputs=[image_input])\n\ndemo.launch()\nTo publish the script app.py from the notebook:\n\nfrom nbdev.export import nb_export\nnb_export('ObjectDetectionWithAzureCustomVision_Part_3', lib_path='.', name='app')"
  },
  {
    "objectID": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "href": "posts/object-detection-with-azure-custom-vision/ObjectDetectionWithAzureCustomVision_Part_3.html#conslusions",
    "title": "Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3",
    "section": "Conslusions",
    "text": "Conslusions\n\nGradio is great for publishing our demo Apps."
  }
]