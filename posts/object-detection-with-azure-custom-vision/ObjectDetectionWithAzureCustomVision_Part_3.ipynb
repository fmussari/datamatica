{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50da3568",
   "metadata": {},
   "source": [
    "---\n",
    "title: Telecom Equipment Detection with Azure Custom Vision (Free) - Part 3  \n",
    "author: \"Francisco Mussari\"  \n",
    "date: 2022-11-15  \n",
    "image: \"Part-3.PNG\"  \n",
    "categories: [computer-vision, deeplearning, azure, custom-vision, object-detection, gradio, hugging face]  \n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    \n",
    "---\n",
    "\n",
    "Part 3. Deploy Gradio Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1dcce",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1a86b",
   "metadata": {},
   "source": [
    "After we train the model in Azure for 1 hour (free tier) and publishing it, when end up with a Prediction URL.\n",
    "\n",
    "![](./blog-pictures/Part-3-Iteration.PNG)\n",
    "\n",
    "We are going to use that Prediction endpoint to do the inference.\n",
    "\n",
    "Here is the App already published for you to try:  \n",
    "[Telecom-Object-Detection](https://huggingface.co/spaces/fmussari/Telecom-Object-Detection)\n",
    "\n",
    "And here the repository:  \n",
    "[https://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main](https://huggingface.co/spaces/fmussari/Telecom-Object-Detection/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812f5e3",
   "metadata": {},
   "source": [
    "## Tutorial Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ff1f4",
   "metadata": {},
   "source": [
    "- [Part 1](./ObjectDetectionWithAzureCustomVision_Part_1.ipynb) covered:\n",
    "    - Creating a [free](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/custom-vision-service/#pricing) Azure [Custom Vision](https://www.customvision.ai/) Service.\n",
    "    - Uploading the images to the service.\n",
    "  \n",
    "- [Part 2](./ObjectDetectionWithAzureCustomVision_Part_2.ipynb) covered:\n",
    "    - Analyzing what happens to the images after uploading.\n",
    "    - How to label the images using [Smart Labeler](https://learn.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/suggested-tags)\n",
    "    - Training and testing the model.\n",
    "   \n",
    "- **Part 3**:\n",
    "    - Create a Huggingface Gradio Demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691db76",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Create A ðŸ¤— Space From A Notebook](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces/)\n",
    "- [Build & Share Delightful Machine Learning Apps](https://gradio.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472512bb",
   "metadata": {},
   "source": [
    "## Part 3.1. Publishing a Gradio App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad817222",
   "metadata": {},
   "source": [
    "Gradio is a great tool to demo machine learning models. The model is already deployed in Azure, so our Gradio App is going to be our front end to connect to that prediction endpoint. What I mean is that the model itself is not going to be deployed in Hugging Face Spaces, which is the normal workflow.\n",
    "\n",
    "If you are new to Gradio, I encourage you to start from the [Quickstart](https://gradio.app/getting_started/).\n",
    "\n",
    "The Gradio demo was created from a Jupyter Notebook with a great tool from [fast.ai](https://www.fast.ai/) which is [nbdev](https://nbdev.fast.ai/). You can start learning the basics here: [Create A ðŸ¤— Space From A Notebook](https://nbdev.fast.ai/blog/posts/2022-11-07-spaces/)\n",
    "\n",
    "In both tutorials you will find the instructions to setup a Gradio enabled space in Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5f25b",
   "metadata": {},
   "source": [
    "This code is based and adapted from:  \n",
    "- [https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py](https://github.com/MicrosoftLearning/AI-102-AIEngineer/blob/master/18-object-detection/Python/test-detector/test-detector.py)  \n",
    "- [https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py](https://huggingface.co/spaces/Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS/blob/main/app.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae63df3",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8927ec",
   "metadata": {},
   "source": [
    "```python\n",
    "#|export\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "\n",
    "import requests, validators\n",
    "\n",
    "from pathlib import Path\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787edc7a",
   "metadata": {},
   "source": [
    "### Azure Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21d5e3",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from dotenv import load_dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654639e",
   "metadata": {},
   "source": [
    "### Environment variables\n",
    "\n",
    "Update the configuration variables in the [.env](.env) file that contains:  \n",
    "\n",
    "\n",
    "```\n",
    "PredictionEndpoint=YOUR_PREDICTION_ENDPOINT\n",
    "PredictionKey=YOUR_PREDICTION_KEY\n",
    "ProjectID=YOUR_PROJECT_ID\n",
    "ModelName=YOUR_PUBLISHED_MODEL\n",
    "```\n",
    "\n",
    "We need to create these environment variables in the Hugging Face Spaces repository under Settings -> Repo Secrets:\n",
    "\n",
    "![](./blog-pictures/Part-3-Secrets.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b92b9b",
   "metadata": {},
   "source": [
    "### Credentials and services\n",
    "- [`ApiKeyCredentials`](https://learn.microsoft.com/en-us/python/api/msrest/msrest.authentication.apikeycredentials?view=azure-python)\n",
    "- [`CustomVisionTrainingClient`](https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.training.customvisiontrainingclient?view=azure-python)\n",
    "- [`CustomVisionTrainingClient.get_project()`](https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-vision-customvision/azure.cognitiveservices.vision.customvision.training.operations.customvisiontrainingclientoperationsmixin?view=azure-python#azure-cognitiveservices-vision-customvision-training-operations-customvisiontrainingclientoperationsmixin-get-project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531fdb9",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "def fig2img(fig):\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img\n",
    "    \n",
    "def custom_vision_detect_objects(image_file: Path):\n",
    "    dpi = 100\n",
    "\n",
    "    # Get Configuration Settings\n",
    "    load_dotenv()\n",
    "    prediction_endpoint = os.getenv('PredictionEndpoint')\n",
    "    prediction_key = os.getenv('PredictionKey')\n",
    "    project_id = os.getenv('ProjectID')\n",
    "    model_name = os.getenv('ModelName')\n",
    "\n",
    "    # Authenticate a client for the training API\n",
    "    credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "    prediction_client = CustomVisionPredictionClient(\n",
    "        endpoint=prediction_endpoint, credentials=credentials)\n",
    "\n",
    "    # Load image and get height, width and channels\n",
    "    #image_file = 'produce.jpg'\n",
    "    print('Detecting objects in', image_file)\n",
    "    image = Image.open(image_file)\n",
    "    h, w, ch = np.array(image).shape\n",
    "\n",
    "    # Detect objects in the test image\n",
    "    with open(image_file, mode=\"rb\") as image_data:\n",
    "        results = prediction_client.detect_image(project_id, model_name, image_data)\n",
    "    \n",
    "    # Create a figure for the results\n",
    "    fig = plt.figure(figsize=(w/dpi, h/dpi))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the image with boxes around each detected object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    lineWidth = int(w/800)\n",
    "    color = 'cyan'\n",
    "\n",
    "    for prediction in results.predictions:\n",
    "        # Only show objects with a > 50% probability\n",
    "        if (prediction.probability*100) > 50:\n",
    "            # Box coordinates and dimensions are proportional - convert to absolutes\n",
    "            left = prediction.bounding_box.left * w \n",
    "            top = prediction.bounding_box.top * h \n",
    "            height = prediction.bounding_box.height * h\n",
    "            width =  prediction.bounding_box.width * w\n",
    "\n",
    "            # Draw the box\n",
    "            points = ((left,top), (left+width,top), \n",
    "                      (left+width,top+height), (left,top+height), \n",
    "                      (left,top))\n",
    "            draw.line(points, fill=color, width=lineWidth)\n",
    "\n",
    "            # Add the tag name and probability\n",
    "            plt.annotate(\n",
    "                prediction.tag_name + \": {0:.0f}%\".format(prediction.probability * 100),\n",
    "                (left, top-1.372*h/dpi), \n",
    "                backgroundcolor=color,\n",
    "                fontsize=max(w/dpi, h/dpi), \n",
    "                fontfamily='monospace'\n",
    "            )\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    return fig2img(fig)\n",
    "\n",
    "    outputfile = 'output.jpg'\n",
    "    fig.savefig(outputfile)\n",
    "    print('Resulabsts saved in ', outputfile)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4e9f4",
   "metadata": {},
   "source": [
    "### Gradio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84c3cf",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "title = \"\"\"<h1 id=\"title\">Telecom Object Detection with Azure Custom Vision</h1>\"\"\"\n",
    "\n",
    "css = \"\"\"\n",
    "h1#title {\n",
    "  text-align: center;\n",
    "}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3406c4",
   "metadata": {},
   "source": [
    "Example images and url to be used in the App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70f8da",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "urls = [\"https://www.dropbox.com/s/y5bk8om5ucu46d3/747.jpg?dl=1\"]\n",
    "imgs = [path.as_posix() for path in sorted(Path('images').rglob('*.jpg'))]\n",
    "img_samples = [[path.as_posix()] for path in sorted(Path('images').rglob('*.jpg'))]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531f451",
   "metadata": {},
   "source": [
    "Functions for the Gradio App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0eb181",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "def set_example_url(example: list) -> dict:\n",
    "    print(gr.Textbox.update(value=example[0]))\n",
    "    return gr.Textbox.update(value=example[0])\n",
    "\n",
    "def set_example_image(example: list) -> dict:\n",
    "    return gr.Image.update(value=example[0])\n",
    "\n",
    "def detect_objects(url_input:str, image_input:Image):\n",
    "    print(f\"{url_input=}\")\n",
    "    if validators.url(url_input):\n",
    "        image = Image.open(requests.get(url_input, stream=True).raw)\n",
    "    elif image_input:\n",
    "        image = image_input\n",
    "        \n",
    "    print(image)\n",
    "    print(image.size)\n",
    "    w, h = image.size\n",
    "    \n",
    "    if max(w, h) > 1_200:\n",
    "        factor = 1_200 / max(w, h)\n",
    "        factor = 1\n",
    "        size = (int(w*factor), int(h*factor))\n",
    "        image = image.resize(size, resample=Image.Resampling.BILINEAR)\n",
    "    \n",
    "    resized_image_path = \"input_object_detection.jpg\"\n",
    "    image.save(resized_image_path)\n",
    "    \n",
    "    return custom_vision_detect_objects(resized_image_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2e62f",
   "metadata": {},
   "source": [
    "```python\n",
    "#| export\n",
    "\n",
    "with gr.Blocks(css=css) as demo:\n",
    "    \n",
    "    gr.Markdown(title)\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"Image Upload\"):\n",
    "            with gr.Row():\n",
    "                image_input = gr.Image(type='pil')\n",
    "                image_output = gr.Image(shape=(650,650))\n",
    "                \n",
    "            with gr.Row(): \n",
    "                example_images = gr.Dataset(components=[image_input], samples=img_samples)\n",
    "            \n",
    "            image_button = gr.Button(\"Detect\")\n",
    "        \n",
    "        with gr.TabItem(\"Image URL\"):\n",
    "            with gr.Row():\n",
    "                url_input = gr.Textbox(lines=2, label='Enter valid image URL here..')\n",
    "                img_output_from_url = gr.Image(shape=(650,650))\n",
    "                \n",
    "            with gr.Row():\n",
    "                example_url = gr.Dataset(components=[url_input], samples=[[str(url)] for url in urls])\n",
    "            url_button = gr.Button(\"Detect\")\n",
    "            \n",
    "    url_button.click(detect_objects, inputs=[url_input,image_input], outputs=img_output_from_url)\n",
    "    image_button.click(detect_objects, inputs=[url_input,image_input], outputs=image_output)\n",
    "    \n",
    "    example_url.click(fn=set_example_url, inputs=[example_url], outputs=[url_input])\n",
    "    example_images.click(fn=set_example_image, inputs=[example_images], outputs=[image_input])\n",
    "\n",
    "demo.launch()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed57546",
   "metadata": {},
   "source": [
    "To publish the script `app.py` from the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "nb_export('ObjectDetectionWithAzureCustomVision_Part_3', lib_path='.', name='app')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b321ffe",
   "metadata": {},
   "source": [
    "## Conslusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee87b6",
   "metadata": {},
   "source": [
    "- Gradio is great for publishing our demo Apps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
