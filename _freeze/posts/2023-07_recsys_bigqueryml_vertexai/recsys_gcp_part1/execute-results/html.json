{
  "hash": "bd09ce4f09bc0622c1ecdaff64753aed",
  "result": {
    "markdown": "---\ntitle: Train and Deploy a RecSys using BigQuery ML and Vertex AI Pipeline. Part 1\nauthor: \"Francisco Mussari\"  \ndate: 2023-07-17  \nimage: \"\"\ncategories: [BigQuery, BigQuery ML, RecSys, Recommender, Pipeline, Vertex AI, Movielens]  \nformat:\n  html:\n    toc: true\n    toc-depth: 3\n---\n\n### Part 1. Train and Deploy\n- Loading MovieLens data into BigQuery\n- Setup services and accounts for the Vertex AI Pipeline to run\n- Create and run the Pipeline and its Components\n- Make sure no reservation or slot assignment remains active\n  \n### Part 2. Inference\n- Use the Cloud Run endpoint to get recommendations\n- Get recommendations from BigQuery ML\n- Get Weights and Bias (Embeddings) and interpret them\n- Cosine similarity with NumPy\n  \n![](pipeline.PNG)\n\n\n## Overview\n\nThis post is about training a Matrix Factorization model with BigQuery ML and deploying it as Docker container. The end-to-end process is orchestrated through a Vertex AI pipeline.  \n  \nThe post is strongly based on this tutorial:\n- [YouTube: Recommendation Engine Pipeline with BigQuery ML and Vertex AI Pipelines using Matrix Factorization](https://youtu.be/S0wNWOR4WoE)\n  \nBut there are key differences:  \n\n1. This post documents the whole process, from loading the date to BigQuery to how to make recomendations in different ways.\n2. On July 5th there was a [Transition to BigQuery editions](https://cloud.google.com/bigquery/docs/editions-transition) which resulted in some changes being made to adapt the scripts showed in the video.\n3. When trying to replicate the video tutorial I had to solve some issues with the pipeline failing to run. Most of the issues were very hard to debug, with misleading error messages and layers over layer of abstraction between python libraries and component definitions. At the end most of the errors were about the Default Service Account not having the requiered permission. So,\n4. In this post we can see and run each step and command to create a Service Account and grant this account granular permissions to the Google Cloud resources needed for the end to end process to run. This is what the Google documentation recommends.\n5. There are also the commands to enable each service API needed for the project.\n\n## Additional Resources\n- [YouTube: Lesson 7: Practical Deep Learning for Coders 2022 - Collaborative filtering deep dive](https://youtu.be/p4ZZq0736Po?t=3720)\n- [Tutorial: Use BigQuery ML to make recommendations from Google analytics data](https://cloud.google.com/bigquery/docs/bigqueryml-mf-implicit-tutorial)\n\n## Before you begin\n\n1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). \n2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n3. You can run the code locally or in Colab. If you are locally you need to install the [gcloud CLI](https://cloud.google.com/sdk/docs/install).\n\n## Loading data into BigQuery\n\nReference: [Load the Movielens dataset into BigQuery](https://cloud.google.com/bigquery/docs/bigqueryml-mf-explicit-tutorial#step_two_load_the_movielens_dataset_into)\n\n### Authenticate your Google Cloud account\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    ! gcloud auth login\n```\n:::\n\n\n### Set your project ID\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n```\n:::\n\n\n### Download MovieLens 1M movier ratings dataset\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n! curl -O 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n! unzip -o ml-1m.zip\n```\n:::\n\n\nChange `::` delimiter to comma `,` and save as `.csv` files:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n! sed 's/::/,/g' ml-1m/ratings.dat > ratings.csv\n! sed 's/::/@/g' ml-1m/movies.dat > movie_titles.csv\n```\n:::\n\n\n### Create BigQuery datasets and populate the tables\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nMODEL_DATASET = \"[bq-model-dataset]\"  # @param {type:\"string\"}\nMOVIELENS_DATASET = \"[bq-data-dataset]\"  # @param {type:\"string\"}\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# To store the model\n! bq mk --location=US --dataset {PROJECT_ID}:{MODEL_DATASET}\n\n# To store movies and reviews tables\n! bq mk --location=US --dataset {PROJECT_ID}:{MOVIELENS_DATASET}\n```\n:::\n\n\nPopulate the tables\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Reviews table\n! bq load --project_id={PROJECT_ID} --source_format=CSV {PROJECT_ID}:{MOVIELENS_DATASET}.movielens_1m ratings.csv user_id:INT64,item_id:INT64,rating:FLOAT64,timestamp:TIMESTAMP\n\n# Movies table\n! bq load --project_id={PROJECT_ID} --source_format=CSV --field_delimiter=@ {PROJECT_ID}:{MOVIELENS_DATASET}.movie_titles movie_titles.csv movie_id:INT64,movie_title:STRING,genre:STRING\n```\n:::\n\n\n## Vertex AI Pipeline. Install Libraries and Setup Services and Accounts\n\n### Install Libraries\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n%%capture\n! pip install google-cloud-aiplatform==1.21.0 --upgrade\n! pip install kfp==2.0.1 --upgrade\n! pip install google-cloud-pipeline-components==2.0.0 --upgrade\n```\n:::\n\n\n### Restart the Kernel\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Automatically restart kernel after installs so that your environment can access the new packages\nimport IPython\napp = IPython.Application.instance()\napp.kernel.do_shutdown(True)\n```\n:::\n\n\n### Set Project Variables\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nPROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\nREGION = \"us-central1\"              # @param {type: \"string\"}\n\nBUCKET_NAME = \"[pipeline-bucket]\" # @param {type: \"string\"}\n\nPIPELINE_ROOT = f\"gs://{BUCKET_NAME}/\"   \n\nMODEL_DIR = PIPELINE_ROOT + \"recommender_model\"    # @param {type: \"string\"}\n```\n:::\n\n\n### Authenticate your Google Cloud account (again)\n\nSince the kernel was restarted, authenticate again.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport sys\nif \"google.colab\" in sys.modules:\n    IS_COLAB = True\n    from google.colab import auth as google_auth\n    google_auth.authenticate_user()\nelse:\n    ! gcloud auth login\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n! gcloud config set project {PROJECT_ID}\n```\n:::\n\n\n### Enable Service APIs\n\nIt is great to enabling the APIs with commands because everything stays documented. We need the following services:\n- Identity and Access Management (IAM) API\n- Vertex AI API\n- Cloud Build API\n- BigQuery API (Although this should be enabled already if the datasets were created and the tables populated)\n- BigQuery Reservation API\n- Cloud Run Admin API\n- Cloud Storage API\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n! gcloud services enable iam.googleapis.com --project={PROJECT_ID}\n! gcloud services enable aiplatform.googleapis.com --project={PROJECT_ID}\n! gcloud services enable cloudbuild.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigquery.googleapis.com --project={PROJECT_ID}\n! gcloud services enable bigqueryreservation.googleapis.com --project={PROJECT_ID}\n! gcloud services enable run.googleapis.com --project={PROJECT_ID}\n! gcloud services enable storage-component.googleapis.com --project={PROJECT_ID}\n```\n:::\n\n\n### Create the Service Account for the pipeline to run\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nSERVICE_ACCOUNT_ID = \"[your-service-account-id]\"  # @param {type:\"string\"}\n```\n:::\n\n\nIf the following cell returns an error in Colab, run it in your local machine. Or create the account directly in the Google Cloud Console UI.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n! gcloud iam service-accounts create {SERVICE_ACCOUNT_ID} --description=\"Vertex AI Pipeline Service Account\" --display-name=\"vertex_service_account\"\n```\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nSERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_ID}@{PROJECT_ID}.iam.gserviceaccount.com\"\n```\n:::\n\n\n#### If not, use the Default Service Account\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nshell_output = ! gcloud projects describe {PROJECT_ID}\nPROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\nDEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nimport sys\n\nIS_COLAB = \"google.colab\" in sys.modules\n\nif (\n    SERVICE_ACCOUNT_ID == \"\"\n    or SERVICE_ACCOUNT_ID is None\n    or SERVICE_ACCOUNT_ID == \"[your-service-account-id]\"\n):\n    # Get your service account from gcloud\n    if not IS_COLAB:\n        shell_output = !gcloud auth list 2>/dev/null\n        DEFAULT_SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n\n    if IS_COLAB:\n        shell_output = ! gcloud projects describe {PROJECT_ID}\n        PROJECT_NUMBER = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n        DEFAULT_SERVICE_ACCOUNT = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n\n    print(\"Service Account:\", DEFAULT_SERVICE_ACCOUNT)\n    SERVICE_ACCOUNT = DEFAULT_SERVICE_ACCOUNT\n```\n:::\n\n\n### Grant the Service Account granular permissions to GCP resources\n\nThe most challenging part of the project was figuring out how to give the service account the right granular permissions. I didn't want to give the service account the Editor or Owner rol. It took me a while to find the right roles. As I mentioned some errors in the pipeline were because lack of permissions, but it was hard to troubleshoot as there wasn't any mention to the actual rol needed.\n\n#### `aiplatform.user` rol\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nservice_arg = f\"serviceAccount:{SERVICE_ACCOUNT}\"\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/aiplatform.user\"\n```\n:::\n\n\n#### Create the GCS bucket and assign `storage.objectAdmin` rol\n\nI first assign the rols of `storage.objectCreator` and `objectViewer`. Took me hours to find out that the trained model couldn't be exported without the rol of `storage.objectAdmin` which is needed to modify existing data in the bucket. \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n! gsutil mb -p {PROJECT_ID} -l {REGION} {PIPELINE_ROOT}\n! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectAdmin {PIPELINE_ROOT}\n```\n:::\n\n\n#### Cloud Run's `run.developer` rol\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/run.developer\"\n```\n:::\n\n\n#### Cloud Build's `cloudbuild.builds.editor` rol\n\nI thought that Cloud Run developer rol was enough for the model to be deployed. Took me a while to find out the service account needed a permission from Cloud Build service.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_arg} --role=\"roles/cloudbuild.builds.editor\"\n```\n:::\n\n\n#### Assign roles to Cloud Build's service account\n\nWhen Cloud Build's API is enabled, its service account is automatically created. \n\n> By default – for security reasons – the Cloud Build Service Account does not have the permissions to manage Cloud Run. [Google Cloud Build + Google Cloud Run](https://www.bram.us/2020/02/13/google-cloud-build-google-cloud-run-fixing-error-gcloud-run-deploy-permission_denied-the-caller-does-not-have-permission/)\n\nFirst lets grab the default service accounts.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# Cloud Build default service account\ncloud_build_sa = f\"{PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\nservice_build_arg = f\"serviceAccount:{cloud_build_sa}\"\n# Compute Engine default service account\ncompute_engine_sa = f\"{PROJECT_NUMBER}-compute@developer.gserviceaccount.com\"\n```\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n! gcloud projects add-iam-policy-binding {PROJECT_ID} --member={service_build_arg} --role=\"roles/run.admin\"\n```\n:::\n\n\nAnd the last part is \"[Grant the IAM Service Account User role to the Cloud Build service account on the Cloud Run runtime service account](https://www.bram.us/2020/02/13/google-cloud-build-google-cloud-run-fixing-error-gcloud-run-deploy-permission_denied-the-caller-does-not-have-permission/)\".  \n  \nMeaning that the Cloud Build service account is going to be able to impersonate the Cloud Run runtime service account, which is Compute Engine default service account. More on impersonation here: [Youtube: Service Account Impersonation in Google Cloud - IAM in GCP](https://youtu.be/pHdQ13HSCq8)\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n! gcloud iam service-accounts add-iam-policy-binding {compute_engine_sa} --member={service_build_arg} --role=\"roles/iam.serviceAccountUser\"\n```\n:::\n\n\nAnd thats it, all services are enabled and the service accounts has the granular permission for the pipeline to run.\n\n## Vertex AI Pipeline - Create and Run the Pipeline\n\n### Import Libraries\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nimport kfp\nfrom typing import NamedTuple\nfrom kfp.dsl import (\n    pipeline, component, OutputPath, InputPath, Model, \n    Input, Artifact, Output, Metrics\n)\nfrom kfp import compiler\n\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform import pipeline_jobs\n```\n:::\n\n\n### Initialize Vertex AI SDK for Python\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\naiplatform.init(project=PROJECT_ID, location=REGION)\n```\n:::\n\n\n### Declare Pipeline Components\n\n#### Create Reservation Component\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef create_reservation(\n    project: str,\n    location: str,\n    commitment_slots: int,\n    reservation_id: str\n) -> NamedTuple(\"outputs\", [(\"reservation_name\", str), (\"assignment_name\", str)]):\n    \"\"\"\n    Create BigQuery Reservation and Assignment\n    \"\"\"\n    print('project', project, 'location', location, 'commitment_slots', commitment_slots, 'reservation_id', reservation_id)\n\n    # import libraries\n    import time\n    from google.cloud.bigquery_reservation_v1 import (\n        CapacityCommitment, Reservation, Assignment, ReservationServiceClient\n    )\n\n    reservation_client = ReservationServiceClient()\n    parent_arg = f\"projects/{project}/locations/{location}\"\n\n    # Reservation (autoscaling)\n    autosc = Reservation.Autoscale(current_slots=0, max_slots=commitment_slots) # NEW Jul-05 Update\n    #reservation_slots = commitment_slots\n    slot_capacity = 0\n    reservation_config = Reservation(\n        #slot_capacity=reservation_slots,\n        slot_capacity=slot_capacity, autoscale=autosc,  # NEW Jul-05 Update\n        edition='ENTERPRISE',   # NEW Jul-05 Update (Could be \"STANDARD\" ?)\n        ignore_idle_slots=False\n    )\n    reservation = reservation_client.create_reservation(\n        parent=parent_arg, reservation_id=reservation_id, reservation=reservation_config\n    )\n    reservation_name = reservation.name\n    print('reservation_name', reservation_name)\n\n    # Assignment\n    print(\"Creating Assignment...\")\n    assignment_config = Assignment(\n        job_type='QUERY', assignee='projects/{}'.format(project)\n    )\n    assignment = reservation_client.create_assignment(\n        parent=reservation_name, assignment=assignment_config\n    )\n    assignment_name = assignment.name\n    print('assignment_name', assignment_name)\n\n    # it can take a lot for the slots to be available\n    print(\"Waiting for 300 seconds...\")\n    time.sleep(300)\n\n    return reservation_name, assignment_name\n```\n:::\n\n\n::: {.callout-important}\n## This is Important\n\nAfter the reservation is created, the account is going to be billed by time and number of slots. It can cost about $3 for each pipeline run, **but you need to be very cautious about the pipeline failing before the reservation is deleted**. To be sure the reservation is deleted you can go to [https://console.cloud.google.com/bigquery/admin/reservations](https://console.cloud.google.com/bigquery/admin/reservations) and select the project of interest. Check that **SLOT RESERVATIONS** and **SLOT COMMITMENTS** are empty and look like this:\n\n  \n**SLOT RESERVATIONS**  \n![](reservations.PNG)\n\n**SLOT COMMITMENTS**  \n![](commitments.PNG)\n\nIf not empty, delete them in the UI. Below you can find some scripts you can run to delete the reservations in case the pipeline fails before doing it.\n:::\n\n#### Delete Reservation Component\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n@component(packages_to_install=[\"google-cloud-bigquery-reservation==1.11.2\"])\ndef delete_reservation(\n    reservation_name: str,\n    assignment_name: str\n):\n    # import libraries\n    from google.cloud.bigquery_reservation_v1 import ReservationServiceClient\n\n    # Delete Assignment, Reservation and Capacity\n    reservation_client = ReservationServiceClient()\n    reservation_client.delete_assignment(name=assignment_name)\n    reservation_client.delete_reservation(name=reservation_name)\n\n    print('reservation_name', reservation_name, 'deleted')\n    print('assignment_name', assignment_name, 'deleted')\n```\n:::\n\n\n#### Log Eval Metrics Component\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n@component()\ndef log_eval_metrics(\n    eval_metrics: Input[Artifact], metrics: Output[Metrics]\n) -> dict:\n    # import libraries\n    import math\n\n    metadata = eval_metrics.metadata\n    for r in metadata[\"rows\"]:\n        rows = r[\"f\"]\n        schema = metadata[\"schema\"][\"fields\"]\n        output = {}\n        for metric, value in zip(schema, rows):\n            metric_name = metric[\"name\"]\n            val = float(value[\"v\"])\n            output[metric_name] = val\n            metrics.log_metric(metric_name, val)\n\n    print(output)\n```\n:::\n\n\n#### Deploy Model Component\n\nThe names in this function are hard coded. It assumes you keep `recommender_model` as the model dir. If it was changed previously, change it here accordingly. Also edit [PIPELINE_ROOT] with your chosen name.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n@component(packages_to_install=[\n    \"google-cloud-build==3.18.0\",\n    \"google-api-python-client==2.93.0\"])\ndef deploy_recommendations_model(\n    artifact_uri: str,\n    project: str\n):\n    # import libraries\n    from google.cloud.devtools import cloudbuild\n    from googleapiclient.discovery import build\n\n    # Deploy Model\n    client = cloudbuild.CloudBuildClient()\n    build = cloudbuild.Build()\n\n    # Fill [PIPELINE_ROOT] with your selected parameter\n    # If you didn't keep `recommender_model` change it accordingly\n    build.steps = [\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/recommender_model', \".\"]  },\n        {   \"name\": \"gcr.io/cloud-builders/gsutil\",\n            \"args\": [\"cp\", \"-r\", '[PIPELINE_ROOT]/dockerfile/Dockerfile/', \"Dockerfile\"] },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"build\", \"-t\", f\"gcr.io/{project}/recommender_model\", \".\" ]   },\n        {   \"name\": \"gcr.io/cloud-builders/docker\",\n            \"args\": [\"push\", f\"gcr.io/{project}/recommender_model\"]    },\n        {   \"name\": \"gcr.io/cloud-builders/gcloud\",\n            \"args\": [\n                \"run\", \"deploy\",\n                \"recommender-model\", \"--image\", f\"gcr.io/{project}/recommender_model\",\n                \"--region\", \"us-central1\", \"--platform\", \"managed\", \"--memory\", \"500Mi\",\n                \"--allow-unauthenticated\", \"--max-instances\", \"5\", \"--port\", \"8501\"\n            ]\n        }\n    ]\n\n    operation = client.create_build(project_id=project, build=build)\n    # Print the in-progress operation\n    print(\"IN PROGRESS:\")\n    print(operation.metadata)\n\n    result = operation.result()\n    # Print the completed status\n    print(\"RESULT:\", result.status)\n```\n:::\n\n\n#### Create the Docker file and copy it to GCS\n\nKeep `recommender_model` or change it accordingly.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n%%writefile Dockerfile\nFROM tensorflow/serving:2.4.4\nENTRYPOINT [\"/usr/bin/env\"]\nENV MODEL_NAME=recommender_model\nENV PORT=8501\nCOPY recommender_model /models/recommender_model/1\nCMD tensorflow_model_server --port=8500 --rest_api_port=$PORT --model_base_path=/models/recommender_model --model_name=$MODEL_NAME\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ndocker_path = PIPELINE_ROOT + 'dockerfile/'\n! gsutil cp Dockerfile {docker_path}\n```\n:::\n\n\n### Pipeline and Job\n\n#### Declare the Pipeline\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n@pipeline(\n    name=\"bigquery-recommender-pipeline\",\n    pipeline_root=PIPELINE_ROOT + \"bigquery-recommender-pipeline\"\n)\ndef recommendation_pipeline(\n    artifact_uri: str,\n    display_name: str\n):\n\n    # import libraries\n    from google_cloud_pipeline_components.v1.bigquery import (\n        BigqueryCreateModelJobOp,\n        BigqueryEvaluateModelJobOp,\n        BigqueryExportModelJobOp\n    )\n\n    # NODE create-reservation\n    project = PROJECT_ID\n    location = \"us\"\n    slots = 100\n    reservation_id = \"matrix-factorization-reservation\"\n\n    create_reservation_task = create_reservation(\n        project=project, location=location, commitment_slots=slots, reservation_id=reservation_id\n    )\n\n    # NODE create-model-task\n    # Query for training\n    num_factors = 60\n    q = f\"\"\"\n    CREATE OR REPLACE MODEL {MODEL_DATASET}.model_00\n    OPTIONS\n        (model_type='matrix_factorization',\n        user_col='user_id',\n        item_col='item_id',\n        RATING_COL='rating',\n        feedback_type='EXPLICIT',\n        l2_reg=9.83,\n        num_factors={num_factors}) AS\n    SELECT user_id, item_id, rating FROM {MOVIELENS_DATASET}.movielens_1m\n    \"\"\"\n    create_model_task = BigqueryCreateModelJobOp(\n        project=project,\n        location=location,\n        query=q,\n    ).after(create_reservation_task)\n\n    # NODE evaluate-model-task\n    bq_evaluate_task = BigqueryEvaluateModelJobOp(\n        project=project, location=location, model=create_model_task.outputs[\"model\"]\n    ).after(create_model_task)\n\n    # NODE delete-reservation\n    delete_reservation_task = delete_reservation(\n        reservation_name=create_reservation_task.outputs[\"reservation_name\"],\n        assignment_name=create_reservation_task.outputs[\"assignment_name\"]\n    ).after(bq_evaluate_task)\n\n    # NODE log-eval-metrics\n    bqml_eval_metrics_raw = bq_evaluate_task.outputs[\"evaluation_metrics\"]\n    log_eval_metrics_task = log_eval_metrics(\n        eval_metrics=bqml_eval_metrics_raw\n    )\n\n    # NODE export-bq-model-to-gcs\n    bq_export_task = BigqueryExportModelJobOp(\n        project=project,\n        location=location,\n        model=create_model_task.outputs[\"model\"],\n        model_destination_path=artifact_uri,\n    ).after(create_model_task)\n\n    # NODE deploy-model-cloud-run\n    deploy_recommendations_model_task = deploy_recommendations_model(\n        artifact_uri=artifact_uri,\n        project=project\n    ).after(bq_export_task)\n```\n:::\n\n\n#### Compile the Pipeline\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ncompiler.Compiler().compile(\n    pipeline_func=recommendation_pipeline,\n    package_path=\"recommendation_pipeline.json\"\n)\n```\n:::\n\n\n#### Create and Run Job\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\njob = pipeline_jobs.PipelineJob(\n    enable_caching=False,\n    display_name=\"recommendation-pipeline\",\n    template_path=\"recommendation_pipeline.json\",\n    parameter_values={\n        \"artifact_uri\": MODEL_DIR,\n        \"display_name\": \"recommendation\",\n    }\n)\n```\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\njob.run(\n    service_account=SERVICE_ACCOUNT,\n    sync=False\n)\n```\n:::\n\n\nYou can monitor the pipeline here:\n- [GCP COnsole: Vertex AI Pipelines](https://console.cloud.google.com/vertex-ai/pipelines)\n  \nAnd if all went fine, you must see this beautiful picture:\n\n![](pipeline.PNG)\n\n## Make sure no reservation or slot assignment is active\n\nIf the pipeline fails before deleting the reservation, this command lists the reservations, if any. \n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n! bq ls --reservation --project_id={PROJECT_ID} --location=us\n```\n:::\n\n\nAnd this command shows the assignment. \n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\n! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n```\n:::\n\n\n### Delete any reservation and assignment\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nshell_output = ! bq show --project_id={PROJECT_ID} --location=us --reservation_assignment --job_type=QUERY --assignee_id={PROJECT_ID} --assignee_type=PROJECT\n\nassignment_id = shell_output[2].split(' ')[2].split('.')[-1]\nreservation_id = 'matrix-factorization-reservation' # As declared in the pipeline\n\n# remove assignment\n! bq rm --project_id={PROJECT_ID} --location=us --reservation_assignment {reservation_id}.{assignment_id}\n# remove reservation\n! bq rm --project_id={PROJECT_ID} --location=us --reservation {reservation_id}\n```\n:::\n\n\n",
    "supporting": [
      "recsys_gcp_part1_files"
    ],
    "filters": [],
    "includes": {}
  }
}